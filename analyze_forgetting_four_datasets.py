#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åœ¨å››ä¸ªæ•°æ®é›†ä¸Šåˆ†æForgetting Score
ä½¿ç”¨å®é™…ç­”é¢˜é—´éš”ï¼ˆæœ€åä¸€æ¬¡ - å€’æ•°ç¬¬äºŒæ¬¡ï¼‰
"""

import pandas as pd
import numpy as np
from collections import defaultdict
import random
import os

# è®¾ç½®éšæœºç§å­
random.seed(42)
np.random.seed(42)

# æ•°æ®é›†é…ç½®
DATASETS = {
    'assist2017': {
        'path': '/mnt/localssd/pykt-toolkit/data/assist2017/test_sequences.csv',
        'name': 'ASSISTments2017'
    },
    'ednet': {
        'path': '/mnt/localssd/pykt-toolkit/data/ednet/test_sequences.csv',
        'name': 'EdNet'
    },
    'algebra2005': {
        'path': '/mnt/localssd/pykt-toolkit/data/algebra2005/test_sequences.csv',
        'name': 'Algebra2005'
    },
    'bridge2006': {
        'path': '/mnt/localssd/pykt-toolkit/data/bridge2algebra2006/test_sequences.csv',
        'name': 'Bridge2Algebra2006'
    }
}

print("="*120)
print("å››ä¸ªæ•°æ®é›†çš„Forgetting Scoreåˆ†æ")
print("ä½¿ç”¨å®é™…ç­”é¢˜é—´éš”: Î”t_c = æœ€åä¸€æ¬¡ç­”é¢˜æ—¶é—´ - å€’æ•°ç¬¬äºŒæ¬¡ç­”é¢˜æ—¶é—´")
print("="*120)

# è¾…åŠ©å‡½æ•°
def parse_field(field_str):
    """è§£æCSVå­—æ®µ"""
    if pd.isna(field_str) or field_str == '' or str(field_str) == '-1':
        return []
    return [int(x) for x in str(field_str).split(',') if x.strip() != '-1' and x.strip() != '']

def calculate_forgetting_score(s_tc, delta_t_minutes, tau):
    """è®¡ç®—forgetting score"""
    if delta_t_minutes <= 0:
        return 0.0
    time_factor = delta_t_minutes / (delta_t_minutes + tau)
    return (1 - s_tc) * time_factor

def analyze_interval_distribution(df):
    """åˆ†ææ•°æ®é›†çš„ç­”é¢˜é—´éš”åˆ†å¸ƒ"""
    all_intervals = []
    
    for _, row in df.iterrows():
        timestamps = parse_field(row['timestamps'])
        concepts = parse_field(row['concepts'])
        
        if len(timestamps) < 2:
            continue
        
        # æŒ‰conceptåˆ†ç»„
        concept_timestamps = defaultdict(list)
        for i, cid in enumerate(concepts):
            concept_timestamps[cid].append(timestamps[i])
        
        # è®¡ç®—æ¯ä¸ªconceptçš„æœ€åä¸¤æ¬¡é—´éš”
        for cid, ts_list in concept_timestamps.items():
            if len(ts_list) >= 2:
                ts_list_sorted = sorted(ts_list)
                interval_ms = ts_list_sorted[-1] - ts_list_sorted[-2]
                interval_minutes = interval_ms / (1000 * 60)
                all_intervals.append(interval_minutes)
    
    return np.array(all_intervals)

def analyze_student_with_tau(student_row, tau):
    """ä½¿ç”¨æŒ‡å®šÏ„å€¼åˆ†æå­¦ç”Ÿ"""
    questions = parse_field(student_row['questions'])
    concepts = parse_field(student_row['concepts'])
    responses = parse_field(student_row['responses'])
    timestamps = parse_field(student_row['timestamps'])
    
    if len(concepts) < 2:
        return None
    
    # æŒ‰conceptåˆ†ç»„
    concept_data = defaultdict(list)
    for i in range(len(concepts)):
        concept_data[concepts[i]].append({
            'index': i,
            'question': questions[i],
            'response': responses[i],
            'timestamp': timestamps[i]
        })
    
    results = []
    
    for cid, interactions in concept_data.items():
        if len(interactions) < 2:
            continue
        
        # æŒ‰æ—¶é—´æ’åº
        interactions_sorted = sorted(interactions, key=lambda x: x['timestamp'])
        
        # å†å²æ­£ç¡®ç‡ï¼ˆä¸åŒ…æ‹¬æœ€åä¸€æ¬¡ï¼‰
        historical_responses = [inter['response'] for inter in interactions_sorted[:-1]]
        s_tc = sum(historical_responses) / len(historical_responses)
        
        # è®¡ç®—æœ€åä¸¤æ¬¡çš„æ—¶é—´é—´éš”
        last_timestamp = interactions_sorted[-1]['timestamp']
        second_last_timestamp = interactions_sorted[-2]['timestamp']
        delta_t_ms = last_timestamp - second_last_timestamp
        delta_t_minutes = max(0, delta_t_ms / (1000 * 60))
        
        # è®¡ç®—forgetting score
        forgetting_score = calculate_forgetting_score(s_tc, delta_t_minutes, tau)
        
        # è®¡ç®—æ—¶é—´å› å­
        time_factor = delta_t_minutes / (delta_t_minutes + tau) if delta_t_minutes > 0 else 0
        
        results.append({
            'concept_id': cid,
            'total_attempts': len(interactions),
            'historical_accuracy': s_tc,
            'last_response': interactions_sorted[-1]['response'],
            'delta_t_minutes': delta_t_minutes,
            'delta_t_hours': delta_t_minutes / 60,
            'delta_t_days': delta_t_minutes / (60 * 24),
            'time_factor': time_factor,
            'forgetting_score': forgetting_score,
        })
    
    if len(results) == 0:
        return None
    
    # ç»Ÿè®¡ä¿¡æ¯
    fs_values = [r['forgetting_score'] for r in results]
    
    return {
        'uid': student_row['uid'],
        'concept_results': results,
        'fs_mean': np.mean(fs_values),
        'fs_std': np.std(fs_values),
        'fs_min': np.min(fs_values),
        'fs_max': np.max(fs_values),
    }

# å¤„ç†æ¯ä¸ªæ•°æ®é›†
for dataset_key, dataset_info in DATASETS.items():
    print(f"\n{'='*120}")
    print(f"æ•°æ®é›†: {dataset_info['name']}")
    print(f"{'='*120}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset_info['path']):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {dataset_info['path']}")
        continue
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(dataset_info['path'])
    print(f"\nåŠ è½½æ•°æ®: {len(df)} ä¸ªå­¦ç”Ÿ")
    
    # ç¬¬1æ­¥ï¼šåˆ†æé—´éš”åˆ†å¸ƒ
    print(f"\nç¬¬1æ­¥ï¼šåˆ†æç­”é¢˜é—´éš”åˆ†å¸ƒ")
    print("-"*120)
    
    all_intervals = analyze_interval_distribution(df)
    
    if len(all_intervals) == 0:
        print(f"âŒ æœªæ‰¾åˆ°è¶³å¤Ÿçš„ç­”é¢˜é—´éš”æ•°æ®")
        continue
    
    print(f"âœ… å…±æ”¶é›†åˆ° {len(all_intervals):,} ä¸ªå®é™…ç­”é¢˜é—´éš”")
    print(f"\né—´éš”ç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {np.mean(all_intervals):.2f} åˆ†é’Ÿ = {np.mean(all_intervals)/60:.2f} å°æ—¶ = {np.mean(all_intervals)/(60*24):.2f} å¤©")
    print(f"  ä¸­ä½æ•°: {np.median(all_intervals):.2f} åˆ†é’Ÿ = {np.median(all_intervals)/60:.2f} å°æ—¶ = {np.median(all_intervals)/(60*24):.2f} å¤©")
    print(f"  æ ‡å‡†å·®: {np.std(all_intervals):.2f} åˆ†é’Ÿ = {np.std(all_intervals)/(60*24):.2f} å¤©")
    print(f"  25åˆ†ä½: {np.percentile(all_intervals, 25):.2f} åˆ†é’Ÿ = {np.percentile(all_intervals, 25)/60:.2f} å°æ—¶")
    print(f"  75åˆ†ä½: {np.percentile(all_intervals, 75):.2f} åˆ†é’Ÿ = {np.percentile(all_intervals, 75)/60:.2f} å°æ—¶")
    print(f"  90åˆ†ä½: {np.percentile(all_intervals, 90):.2f} åˆ†é’Ÿ = {np.percentile(all_intervals, 90)/(60*24):.2f} å¤©")
    
    # ç¬¬2æ­¥ï¼šé€‰æ‹©Ï„å€¼
    print(f"\nç¬¬2æ­¥ï¼šé€‰æ‹©åˆé€‚çš„Ï„å€¼")
    print("-"*120)
    
    # ä½¿ç”¨å¹³å‡å€¼ä½œä¸ºÏ„
    tau_selected = np.mean(all_intervals)
    tau_days = tau_selected / (60 * 24)
    
    print(f"âœ… é€‰æ‹© Ï„ = {tau_selected:.2f} åˆ†é’Ÿ = {tau_selected/60:.2f} å°æ—¶ = {tau_days:.2f} å¤© (å¹³å‡ç­”é¢˜é—´éš”)")
    print(f"   åœ¨å¹³å‡é—´éš”ä¸‹ï¼Œæ—¶é—´å› å­ â‰ˆ 0.5ï¼Œå¯¹é—å¿˜çš„æ•æ„Ÿåº¦é€‚ä¸­")
    
    # æ˜¾ç¤ºä¸åŒé—´éš”ä¸‹çš„æ—¶é—´å› å­
    print(f"\næ—¶é—´å› å­ç¤ºä¾‹:")
    intervals_examples = [
        ("1åˆ†é’Ÿ", 1),
        ("10åˆ†é’Ÿ", 10),
        ("1å°æ—¶", 60),
        ("6å°æ—¶", 360),
        ("1å¤©", 1440),
        ("3å¤©", 4320),
        ("7å¤©", 10080),
        ("30å¤©", 43200),
    ]
    for name, interval_min in intervals_examples:
        time_factor = interval_min / (interval_min + tau_selected)
        print(f"  {name:>8}: æ—¶é—´å› å­ = {time_factor:.4f}")
    
    # ç¬¬3æ­¥ï¼šé€‰æ‹©å­¦ç”Ÿå¹¶åˆ†æ
    print(f"\nç¬¬3æ­¥ï¼šé€‰æ‹©5ä¸ªå­¦ç”Ÿè¿›è¡Œè¯¦ç»†åˆ†æ")
    print("-"*120)
    
    # ç­›é€‰æœ‰è¶³å¤Ÿäº¤äº’çš„å­¦ç”Ÿ
    qualified_students = []
    for uid in df['uid'].unique():
        student_row = df[df['uid'] == uid].iloc[0]
        analysis = analyze_student_with_tau(student_row, tau_selected)
        if analysis and len(analysis['concept_results']) >= 5:
            qualified_students.append(uid)
    
    print(f"âœ… å…±æœ‰ {len(qualified_students)} ä¸ªå­¦ç”Ÿæ»¡è¶³æ¡ä»¶ (â‰¥5ä¸ªconceptsï¼Œæ¯ä¸ªâ‰¥2æ¬¡äº¤äº’)")
    
    if len(qualified_students) == 0:
        print(f"âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ")
        continue
    
    # éšæœºé€‰æ‹©5ä¸ªå­¦ç”Ÿ
    selected_students = random.sample(qualified_students, min(5, len(qualified_students)))
    print(f"âœ… éšæœºé€‰æ‹© {len(selected_students)} ä¸ªå­¦ç”Ÿ:")
    for i, uid in enumerate(selected_students, 1):
        print(f"   {i}. å­¦ç”ŸID: {uid}")
    
    # ç¬¬4æ­¥ï¼šè¯¦ç»†åˆ†ææ¯ä¸ªå­¦ç”Ÿ
    print(f"\nç¬¬4æ­¥ï¼šè¯¦ç»†åˆ†æç»“æœ")
    print("="*120)
    
    for idx, uid in enumerate(selected_students, 1):
        student_row = df[df['uid'] == uid].iloc[0]
        analysis = analyze_student_with_tau(student_row, tau_selected)
        
        if not analysis:
            continue
        
        print(f"\n{'-'*120}")
        print(f"å­¦ç”Ÿ #{idx}: ID {uid}")
        print(f"{'-'*120}")
        
        print(f"\nForgetting Scoreç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {analysis['fs_mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {analysis['fs_std']:.4f}")
        print(f"  èŒƒå›´: [{analysis['fs_min']:.4f}, {analysis['fs_max']:.4f}]")
        
        results = analysis['concept_results']
        results_sorted = sorted(results, key=lambda x: x['forgetting_score'], reverse=True)
        
        # æ˜¾ç¤ºå‰10ä¸ªæœ€é«˜FSçš„concepts
        print(f"\nå‰10ä¸ªæœ€éœ€è¦å¤ä¹ çš„Concepts (Forgetting Scoreæœ€é«˜):")
        print(f"  {'Concept':<10} {'æ¬¡æ•°':<6} {'å†å²å‡†ç¡®ç‡':<12} {'é—´éš”':<12} {'æ—¶é—´å› å­':<12} {'FS':<12} {'æœ€å':<6} {'åˆ†ç±»':<10}")
        print(f"  {'-'*108}")
        
        for result in results_sorted[:10]:
            cid = result['concept_id']
            attempts = result['total_attempts']
            hist_acc = result['historical_accuracy'] * 100
            
            # é—´éš”æ˜¾ç¤º
            if result['delta_t_days'] >= 1:
                interval_str = f"{result['delta_t_days']:.1f}d"
            elif result['delta_t_hours'] >= 1:
                interval_str = f"{result['delta_t_hours']:.1f}h"
            else:
                interval_str = f"{result['delta_t_minutes']:.1f}m"
            
            time_factor = result['time_factor']
            fs = result['forgetting_score']
            last_resp = "âœ…" if result['last_response'] == 1 else "âŒ"
            
            # åˆ†ç±»
            if fs >= 0.3:
                category = "ğŸ”´ ç´§æ€¥"
            elif fs >= 0.2:
                category = "ğŸŸ  é‡è¦"
            elif fs >= 0.1:
                category = "ğŸŸ¡ ä¸€èˆ¬"
            else:
                category = "ğŸŸ¢ ç»´æŒ"
            
            print(f"  {cid:<10} {attempts:<6} {hist_acc:<11.1f}% {interval_str:<12} "
                  f"{time_factor:<12.4f} {fs:<12.4f} {last_resp:<6} {category:<10}")
        
        # å…³é”®æ´å¯Ÿ
        print(f"\n  å…³é”®æ´å¯Ÿ:")
        
        # æŒ‰é—´éš”åˆ†ç»„
        very_short = [r for r in results if r['delta_t_hours'] < 1]
        short = [r for r in results if 1 <= r['delta_t_hours'] < 24]
        medium = [r for r in results if 1 <= r['delta_t_days'] < 7]
        long = [r for r in results if r['delta_t_days'] >= 7]
        
        if very_short:
            print(f"    æçŸ­é—´éš” (<1å°æ—¶): {len(very_short)}ä¸ªconcepts, "
                  f"å¹³å‡æ—¶é—´å› å­={np.mean([r['time_factor'] for r in very_short]):.4f}, "
                  f"å¹³å‡FS={np.mean([r['forgetting_score'] for r in very_short]):.4f}")
        if short:
            print(f"    çŸ­é—´éš” (1-24å°æ—¶): {len(short)}ä¸ªconcepts, "
                  f"å¹³å‡æ—¶é—´å› å­={np.mean([r['time_factor'] for r in short]):.4f}, "
                  f"å¹³å‡FS={np.mean([r['forgetting_score'] for r in short]):.4f}")
        if medium:
            print(f"    ä¸­é—´éš” (1-7å¤©): {len(medium)}ä¸ªconcepts, "
                  f"å¹³å‡æ—¶é—´å› å­={np.mean([r['time_factor'] for r in medium]):.4f}, "
                  f"å¹³å‡FS={np.mean([r['forgetting_score'] for r in medium]):.4f}")
        if long:
            print(f"    é•¿é—´éš” (â‰¥7å¤©): {len(long)}ä¸ªconcepts, "
                  f"å¹³å‡æ—¶é—´å› å­={np.mean([r['time_factor'] for r in long]):.4f}, "
                  f"å¹³å‡FS={np.mean([r['forgetting_score'] for r in long]):.4f}")
        
        # æŒ‰æŒæ¡ç¨‹åº¦åˆ†ç»„
        high_mastery = [r for r in results if r['historical_accuracy'] >= 0.7]
        low_mastery = [r for r in results if r['historical_accuracy'] <= 0.3]
        
        if high_mastery:
            print(f"    æŒæ¡å¥½ (â‰¥70%): {len(high_mastery)}ä¸ªconcepts, "
                  f"å¹³å‡FS={np.mean([r['forgetting_score'] for r in high_mastery]):.4f}")
        if low_mastery:
            print(f"    æŒæ¡å·® (â‰¤30%): {len(low_mastery)}ä¸ªconcepts, "
                  f"å¹³å‡FS={np.mean([r['forgetting_score'] for r in low_mastery]):.4f}")

print("\n" + "="*120)
print("æ±‡æ€»ï¼šå››ä¸ªæ•°æ®é›†çš„Ï„å€¼é€‰æ‹©")
print("="*120)

# é‡æ–°åŠ è½½å¹¶æ˜¾ç¤ºæ±‡æ€»
summary_data = []
for dataset_key, dataset_info in DATASETS.items():
    if not os.path.exists(dataset_info['path']):
        continue
    
    df = pd.read_csv(dataset_info['path'])
    all_intervals = analyze_interval_distribution(df)
    
    if len(all_intervals) > 0:
        tau = np.mean(all_intervals)
        summary_data.append({
            'dataset': dataset_info['name'],
            'tau_minutes': tau,
            'tau_hours': tau / 60,
            'tau_days': tau / (60 * 24),
            'median_minutes': np.median(all_intervals),
            'median_days': np.median(all_intervals) / (60 * 24),
        })

print(f"\n{'æ•°æ®é›†':<25} {'Ï„å€¼(å¹³å‡é—´éš”)':<25} {'ä¸­ä½æ•°é—´éš”':<25}")
print("-"*120)
for item in summary_data:
    tau_str = f"{item['tau_minutes']:.0f}m = {item['tau_hours']:.1f}h = {item['tau_days']:.2f}d"
    median_str = f"{item['median_minutes']:.0f}m = {item['median_days']:.2f}d"
    print(f"{item['dataset']:<25} {tau_str:<25} {median_str:<25}")

print("\n" + "="*120)
print("âœ… å››ä¸ªæ•°æ®é›†åˆ†æå®Œæˆï¼")
print("="*120)

