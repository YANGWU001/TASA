# TASA Lambda值实验结果分析报告

## 实验概览

本报告统计分析了TASA系统在不同lambda值（0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0）和三种LLM backbone（GPT, Qwen, Llama）上的教学效果。

**核心指标**: Average Learning Gain（平均学习增益）

---

## 一、关键发现

### 1. 最佳配置

| Backbone | 最佳Lambda值 | 平均学习增益 |
|----------|-------------|-------------|
| **QWEN** (总体最佳) | **λ=0.7** | **0.5967** |
| GPT | λ=1.0 | 0.5952 |
| LLAMA | λ=0.5 | 0.5919 |

**结论**: QWEN在λ=0.7时达到最佳性能，三种backbone的最优性能差异不大（0.5919-0.5967）

---

### 2. Lambda值影响分析

#### 从λ=0（无记忆）到λ=1.0（完全记忆）的变化

| Backbone | λ=0 | λ=1.0 | 提升幅度 |
|----------|-----|-------|---------|
| **LLAMA** | 0.4514 | 0.5752 | **↑27.4%** |
| **GPT** | 0.4895 | 0.5952 | **↑21.6%** |
| QWEN | 0.4714 | 0.4776 | ↑1.3% |

**重要发现**:
- **LLAMA和GPT显著受益于记忆机制**：加入记忆后学习增益提升20%+
- **QWEN对记忆不敏感**：从无记忆到完全记忆仅提升1.3%，说明Qwen可能自身已具备较好的上下文理解能力

---

### 3. 不同Lambda值下的性能对比

#### 平均学习增益 (Average Learning Gain)

| Lambda | GPT | QWEN | LLAMA |
|--------|-----|------|-------|
| 0.0 | 0.4895 | 0.4714 | 0.4514 |
| 0.1 | 0.4443 | 0.4762 | 0.5038 |
| 0.3 | 0.5000 | 0.4562 | 0.4252 |
| **0.5** | 0.5405 | 0.4719 | **0.5919** ⭐ |
| **0.7** | 0.5062 | **0.5967** ⭐ | 0.5029 |
| 0.9 | 0.4857 | 0.5405 | 0.5667 |
| **1.0** | **0.5952** ⭐ | 0.4776 | 0.5752 |

⭐ 标记为各backbone的最佳配置

#### 中位数学习增益 (Median Learning Gain)

| Lambda | GPT | QWEN | LLAMA |
|--------|-----|------|-------|
| 0.0 | 0.4286 | 0.4286 | 0.3810 |
| 0.1 | 0.4643 | 0.4286 | 0.4286 |
| 0.3 | 0.4286 | 0.4643 | 0.3810 |
| 0.5 | 0.5000 | 0.4286 | 0.6190 |
| 0.7 | 0.4286 | 0.6667 | 0.4286 |
| 0.9 | 0.5000 | 0.5476 | 0.6190 |
| 1.0 | 0.6429 | 0.4286 | 0.5714 |

---

## 二、详细分析

### GPT Backbone

```
Lambda  Avg Gain   Std      Median   
0.0     0.4895    0.2419    0.4286   
0.1     0.4443    0.2437    0.4643   
0.3     0.5000    0.2799    0.4286   
0.5     0.5405    0.3040    0.5000   
0.7     0.5062    0.2793    0.4286   
0.9     0.4857    0.2781    0.5000   
1.0     0.5952    0.2308    0.6429   ← 最佳
```

**特点**:
- 在λ=1.0时达到最佳性能
- **偏好完全记忆策略**（full memory）
- 性能随lambda值呈现波动，但总体趋势向上（+21.6%）

---

### QWEN Backbone

```
Lambda  Avg Gain   Std      Median   
0.0     0.4714    0.2716    0.4286   
0.1     0.4762    0.2062    0.4286   
0.3     0.4562    0.2318    0.4643   
0.5     0.4719    0.2553    0.4286   
0.7     0.5967    0.2454    0.6667   ← 最佳
0.9     0.5405    0.2677    0.5476   
1.0     0.4776    0.2205    0.4286   
```

**特点**:
- 在λ=0.7时达到峰值，**λ=1.0时性能反而下降**
- **偏好中等记忆衰减策略**（moderate forgetting）
- 对记忆机制不敏感，但需要适度遗忘来避免信息过载
- 整体趋势接近持平（+1.3%）

---

### LLAMA Backbone

```
Lambda  Avg Gain   Std      Median   
0.0     0.4514    0.2673    0.3810   
0.1     0.5038    0.2335    0.4286   
0.3     0.4252    0.2565    0.3810   
0.5     0.5919    0.2301    0.6190   ← 最佳
0.7     0.5029    0.2130    0.4286   
0.9     0.5667    0.2851    0.6190   
1.0     0.5752    0.2560    0.5714   
```

**特点**:
- 在λ=0.5时达到最优性能
- **偏好中等偏强的记忆保留**（balanced memory）
- 从无记忆到完全记忆提升最显著（+27.4%）
- 性能随lambda值波动较大

---

## 三、实践建议

### 推荐配置

1. **追求最佳性能**: 
   - 使用 **QWEN + λ=0.7** (Gain: 0.5967)

2. **按Backbone选择**:
   - **GPT**: λ=1.0 (完全记忆)
   - **QWEN**: λ=0.7 (中度遗忘)
   - **LLAMA**: λ=0.5 (平衡记忆)

3. **资源受限场景**:
   - 所有backbone在最佳lambda下性能相近
   - 可根据模型可用性和成本选择

### Lambda值设计原则

- **GPT系列**: 倾向于保留完整历史信息，建议 λ ≥ 0.9
- **QWEN系列**: 需要适度遗忘机制，建议 λ ∈ [0.5, 0.7]
- **LLAMA系列**: 需要平衡记忆和遗忘，建议 λ ∈ [0.5, 0.9]

---

## 四、统计数据

### 标准差对比

- GPT在最佳配置(λ=1.0)时标准差最低: 0.2308
- 说明完全记忆策略在GPT上表现稳定

### 中位数分析

- QWEN在λ=0.7时中位数最高: 0.6667
- 表明该配置下多数学生都能获得较好效果

---

## 五、实验结论

1. **记忆机制的重要性**: 
   - LLAMA (+27.4%) 和 GPT (+21.6%) 显著受益于记忆
   - QWEN相对不敏感 (+1.3%)

2. **最佳Lambda值因模型而异**:
   - 不同LLM backbone有不同的最优记忆衰减率
   - 需要针对具体模型调优

3. **性能接近**: 
   - 三种backbone在各自最优配置下性能差异 < 1%
   - 说明良好的记忆管理能够弥补模型本身的差异

4. **遗忘的必要性**:
   - QWEN在完全记忆(λ=1.0)时性能不佳
   - 适度遗忘可以避免信息过载

---

## 数据文件

- **详细数据**: `/mnt/localssd/lambda_analysis_results.csv`
- **原始结果**: `/mnt/localssd/bank/evaluation_results/TASA-lambda*-*/`

---

*报告生成时间: 2025-10-23*

