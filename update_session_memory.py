#!/usr/bin/env python3
"""
æ›´æ–°Sessionæ–‡ä»¶çš„Memoryå­—æ®µ
ç‰¹åˆ«å¤„ç†nips_task34æ•°æ®é›†çš„conceptæ˜ å°„é—®é¢˜
"""

import json
import os
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import argparse

def load_nips_subject_mapping():
    """åŠ è½½nips_task34çš„subjectæ˜ å°„ (æ•°å­—ID -> åç§°)"""
    metadata_file = '/mnt/localssd/pykt-toolkit/data/nips_task34/metadata/subject_metadata.csv'
    
    if not os.path.exists(metadata_file):
        return {}
    
    df = pd.read_csv(metadata_file)
    subject_map = {}
    for _, row in df.iterrows():
        subject_id = str(row['SubjectId'])  # è½¬ä¸ºå­—ç¬¦ä¸²,å¦‚"210"
        name = str(row['Name']).strip()
        subject_map[subject_id] = name
    
    return subject_map

def load_memory_for_student(dataset, student_id):
    """åŠ è½½å­¦ç”Ÿçš„æ‰€æœ‰memoryè®°å½•"""
    memory_file = f'/mnt/localssd/bank/memory/{dataset}/data/{student_id}.json'
    
    if not os.path.exists(memory_file):
        return None
    
    with open(memory_file) as f:
        return json.load(f)

def find_memory_records(memory_data, concept_text):
    """ä»memoryä¸­æ‰¾åˆ°åŒ¹é…concept_textçš„æ‰€æœ‰è®°å½•"""
    if not memory_data:
        return None
    
    memories = []
    for mem in memory_data:
        if mem.get('concept_text') == concept_text:
            memories.append({
                'description': mem.get('description'),
                'timestamp': mem.get('timestamp'),
                'response': mem.get('response')
            })
    
    if not memories:
        return None
    
    # æŒ‰timestampæ’åº
    memories.sort(key=lambda x: x.get('timestamp', 0))
    return memories

def update_session_file(session_file, dataset, nips_mapping=None):
    """æ›´æ–°å•ä¸ªsessionæ–‡ä»¶çš„memoryå­—æ®µ"""
    with open(session_file) as f:
        session = json.load(f)
    
    # å¦‚æœå·²ç»æœ‰memory,è·³è¿‡
    if session.get('memory') and len(session.get('memory', [])) > 0:
        return False, "already_has_memory"
    
    student_id = session['student_id']
    concept_text_in_session = session['concept_text']
    
    # åŠ è½½è¯¥å­¦ç”Ÿçš„memoryæ•°æ®
    memory_data = load_memory_for_student(dataset, student_id)
    
    if not memory_data:
        return False, "no_memory_file"
    
    # å¯¹äºnips_task34, concept_textæ˜¯æ•°å­—,éœ€è¦æ˜ å°„åˆ°å®é™…åç§°
    if dataset == 'nips_task34' and nips_mapping:
        # sessionä¸­çš„concept_textæ˜¯æ•°å­—(å¦‚"210")
        # æ˜ å°„åˆ°å®é™…åç§°
        actual_concept_text = nips_mapping.get(concept_text_in_session, None)
        
        if not actual_concept_text:
            # å¦‚æœæ˜ å°„ä¸åˆ°,å°è¯•ç›´æ¥ç”¨concept_id
            concept_id = session.get('concept_id')
            if concept_id is not None:
                actual_concept_text = nips_mapping.get(str(concept_id), None)
        
        if not actual_concept_text:
            return False, f"no_mapping_for_{concept_text_in_session}"
    else:
        actual_concept_text = concept_text_in_session
    
    # åœ¨memoryä¸­æ‰¾åˆ°å¯¹åº”conceptçš„è®°å½•
    memories = find_memory_records(memory_data, actual_concept_text)
    
    if memories is None:
        return False, "no_matching_memory"
    
    # æ›´æ–°session
    session['memory'] = memories
    
    # ä¿å­˜
    with open(session_file, 'w') as f:
        json.dump(session, f, indent=2)
    
    return True, None

def update_dataset(dataset):
    """æ›´æ–°æ•´ä¸ªæ•°æ®é›†çš„session"""
    print("="*80)
    print(f"æ›´æ–° {dataset.upper()} çš„Session Memory")
    print("="*80)
    
    session_dir = f'/mnt/localssd/bank/session/{dataset}'
    
    if not os.path.exists(session_dir):
        print(f"  âŒ Sessionç›®å½•ä¸å­˜åœ¨: {session_dir}")
        return
    
    # å¯¹äºnips_task34,åŠ è½½subjectæ˜ å°„
    nips_mapping = None
    if dataset == 'nips_task34':
        print("  ğŸ“š åŠ è½½NIPS_TASK34çš„Subjectæ˜ å°„...")
        nips_mapping = load_nips_subject_mapping()
        print(f"  âœ… åŠ è½½äº† {len(nips_mapping)} ä¸ªæ˜ å°„")
    
    # è·å–æ‰€æœ‰sessionæ–‡ä»¶
    session_files = list(Path(session_dir).glob('*.json'))
    print(f"  ğŸ“‚ æ‰¾åˆ° {len(session_files)} ä¸ªsessionæ–‡ä»¶")
    print()
    
    # ç»Ÿè®¡
    updated = 0
    skipped = 0
    errors = {}
    
    # æ›´æ–°æ¯ä¸ªæ–‡ä»¶
    for session_file in tqdm(session_files, desc=f"æ›´æ–°{dataset}", ncols=100):
        success, error = update_session_file(session_file, dataset, nips_mapping)
        
        if success:
            updated += 1
        else:
            skipped += 1
            if error:
                errors[error] = errors.get(error, 0) + 1
    
    # æŠ¥å‘Šç»“æœ
    print()
    print(f"âœ… æˆåŠŸæ›´æ–°: {updated} ä¸ª")
    print(f"âš ï¸  è·³è¿‡: {skipped} ä¸ª")
    
    if errors:
        print(f"\nè·³è¿‡åŸå› :")
        for reason, count in sorted(errors.items(), key=lambda x: -x[1]):
            print(f"  - {reason}: {count} ä¸ª")
    print()

def main():
    parser = argparse.ArgumentParser(description='æ›´æ–°Sessionçš„Memoryå­—æ®µ')
    parser.add_argument('--dataset', type=str,
                       choices=['nips_task34', 'assist2017', 'algebra2005', 'bridge2006', 'all'],
                       default='all',
                       help='æ•°æ®é›†åç§°')
    
    args = parser.parse_args()
    
    if args.dataset == 'all':
        datasets = ['nips_task34', 'assist2017', 'algebra2005', 'bridge2006']
    else:
        datasets = [args.dataset]
    
    for dataset in datasets:
        update_dataset(dataset)
        print()
    
    print("="*80)
    print("âœ… æ‰€æœ‰Sessionæ›´æ–°å®Œæˆï¼")
    print("="*80)

if __name__ == '__main__':
    main()

