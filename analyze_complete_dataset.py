#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´æ•°æ®é›†åˆ†ææŠ¥å‘Š
åˆ†ætrain_validå’Œtestçš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
"""

import pandas as pd
import numpy as np
import os
from datetime import datetime
from collections import Counter
from tqdm import tqdm

def analyze_dataset_split(dataset_name, data_path, split_name):
    """
    åˆ†æå•ä¸ªæ•°æ®åˆ†å‰²ï¼ˆtrain_validæˆ–testï¼‰
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {dataset_name} - {split_name}")
    print(f"{'='*80}")
    
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return None
    
    print(f"ğŸ“‚ è¯»å–æ•°æ®: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
        
        # Fold distribution (only for train_valid)
        if split_name == "Train/Valid":
            print("\nğŸ“‹ Foldåˆ†å¸ƒ:")
            print(df['fold'].value_counts().sort_index().to_string())
        
        # Student count
        unique_students = df['uid'].nunique()
        print(f"\nğŸ‘¥ å­¦ç”Ÿç»Ÿè®¡:\n  æ€»å­¦ç”Ÿæ•°: {unique_students:,}")
        
        all_interactions = []
        all_concepts = set()
        all_questions = set()
        all_responses = []
        all_timestamps = []
        sequence_lengths = []
        
        print("\nâ³ æ­£åœ¨è§£æå­¦ç”Ÿæ•°æ®...")
        total_rows = len(df)
        for i, row in tqdm(df.iterrows(), total=total_rows, desc="  å¤„ç†è¿›åº¦"):
            concepts_str = str(row['concepts'])
            responses_str = str(row['responses'])
            timestamps_str = str(row['timestamps'])
            questions_str = str(row['questions'])
            
            if concepts_str == 'NA' or responses_str == 'NA' or timestamps_str == 'NA' or questions_str == 'NA':
                continue
            
            try:
                concepts = [int(c) for c in concepts_str.split(',') if c and c != '-1']
                responses = [int(r) for r in responses_str.split(',') if r and r != '-1']
                timestamps = [int(t) for t in timestamps_str.split(',') if t and t != '-1']
                questions = [int(q) for q in questions_str.split(',') if q and q != '-1']
                
                min_len = min(len(concepts), len(responses), len(timestamps), len(questions))
                if min_len == 0:
                    continue
                
                concepts = concepts[:min_len]
                responses = responses[:min_len]
                timestamps = timestamps[:min_len]
                questions = questions[:min_len]
                
                all_interactions.extend(responses)
                all_concepts.update(concepts)
                all_questions.update(questions)
                all_responses.extend(responses)
                all_timestamps.extend(timestamps)
                sequence_lengths.append(min_len)
            except (ValueError, IndexError) as e:
                continue
        
        total_interactions = len(all_interactions)
        overall_accuracy = np.mean(all_responses) if all_responses else 0
        total_correct = sum(all_responses)
        total_incorrect = total_interactions - total_correct
        
        earliest_timestamp = datetime.fromtimestamp(min(all_timestamps) / 1000) if all_timestamps else "N/A"
        latest_timestamp = datetime.fromtimestamp(max(all_timestamps) / 1000) if all_timestamps else "N/A"
        time_span_days = (max(all_timestamps) - min(all_timestamps)) / (1000 * 60 * 60 * 24) if all_timestamps else 0
        
        # Concept frequency
        concept_counts = Counter()
        for _, row in df.iterrows():
            concepts_str = str(row['concepts'])
            if concepts_str != 'NA':
                for c_str in concepts_str.split(','):
                    if c_str and c_str != '-1':
                        try:
                            concept_counts[int(c_str)] += 1
                        except ValueError:
                            continue
        
        top_10_concepts = concept_counts.most_common(10)
        
        print("\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:\n")
        print("  ğŸ”¢ æ•°é‡ç»Ÿè®¡:")
        print(f"    - æ€»å­¦ç”Ÿæ•°: {unique_students:,}")
        print(f"    - æ€»äº¤äº’æ•°: {total_interactions:,}")
        print(f"    - å”¯ä¸€Concepts: {len(all_concepts):,}")
        print(f"    - å”¯ä¸€Questions: {len(all_questions):,}")
        print(f"    - å¹³å‡æ¯ä¸ªå­¦ç”Ÿäº¤äº’æ•°: {total_interactions / unique_students:.1f}")
        
        print("\n  ğŸ“ åºåˆ—é•¿åº¦ç»Ÿè®¡:")
        print(f"    - å¹³å‡é•¿åº¦: {np.mean(sequence_lengths):.1f}")
        print(f"    - ä¸­ä½æ•°é•¿åº¦: {np.median(sequence_lengths):.1f}")
        print(f"    - æœ€å°é•¿åº¦: {min(sequence_lengths)}")
        print(f"    - æœ€å¤§é•¿åº¦: {max(sequence_lengths)}")
        print(f"    - æ ‡å‡†å·®: {np.std(sequence_lengths):.1f}")
        
        print("\n  âœ… æ­£ç¡®ç‡ç»Ÿè®¡:")
        print(f"    - æ€»ä½“æ­£ç¡®ç‡: {overall_accuracy:.2%}")
        print(f"    - æ­£ç¡®ç­”é¢˜æ•°: {total_correct:,}")
        print(f"    - é”™è¯¯ç­”é¢˜æ•°: {total_incorrect:,}")
        
        print("\n  â±ï¸  æ—¶é—´è·¨åº¦:")
        print(f"    - æœ€æ—©æ—¶é—´: {earliest_timestamp}")
        print(f"    - æœ€æ™šæ—¶é—´: {latest_timestamp}")
        print(f"    - æ—¶é—´è·¨åº¦: {time_span_days:.1f} å¤©")
        
        print("\n  ğŸ“Š åºåˆ—é•¿åº¦åˆ†å¸ƒ:")
        print(f"    - 10th percentile: {np.percentile(sequence_lengths, 10):.0f}")
        print(f"    - 25th percentile: {np.percentile(sequence_lengths, 25):.0f}")
        print(f"    - 50th percentile: {np.percentile(sequence_lengths, 50):.0f}")
        print(f"    - 75th percentile: {np.percentile(sequence_lengths, 75):.0f}")
        print(f"    - 90th percentile: {np.percentile(sequence_lengths, 90):.0f}")
        print(f"    - 95th percentile: {np.percentile(sequence_lengths, 95):.0f}")
        print(f"    - 99th percentile: {np.percentile(sequence_lengths, 99):.0f}")
        
        print("\n  ğŸ” æœ€å¸¸è§çš„10ä¸ªConcepts:")
        for concept_id, count in top_10_concepts:
            print(f"    - Concept {concept_id}: {count:,} æ¬¡ ({count / total_interactions:.2%})")
        
        return {
            'dataset': dataset_name,
            'split': split_name,
            'total_records': len(df),
            'unique_students': unique_students,
            'total_interactions': total_interactions,
            'unique_concepts': len(all_concepts),
            'unique_questions': len(all_questions),
            'avg_interactions_per_student': total_interactions / unique_students if unique_students > 0 else 0,
            'avg_sequence_length': np.mean(sequence_lengths),
            'median_sequence_length': np.median(sequence_lengths),
            'min_sequence_length': min(sequence_lengths) if sequence_lengths else 0,
            'max_sequence_length': max(sequence_lengths) if sequence_lengths else 0,
            'std_sequence_length': np.std(sequence_lengths),
            'overall_accuracy': overall_accuracy,
            'total_correct': total_correct,
            'total_incorrect': total_incorrect,
            'earliest_timestamp': min(all_timestamps) if all_timestamps else None,
            'latest_timestamp': max(all_timestamps) if all_timestamps else None,
            'time_span_days': time_span_days,
            'p10_seq_len': np.percentile(sequence_lengths, 10) if sequence_lengths else 0,
            'p25_seq_len': np.percentile(sequence_lengths, 25) if sequence_lengths else 0,
            'p50_seq_len': np.percentile(sequence_lengths, 50) if sequence_lengths else 0,
            'p75_seq_len': np.percentile(sequence_lengths, 75) if sequence_lengths else 0,
            'p90_seq_len': np.percentile(sequence_lengths, 90) if sequence_lengths else 0,
            'p95_seq_len': np.percentile(sequence_lengths, 95) if sequence_lengths else 0,
            'p99_seq_len': np.percentile(sequence_lengths, 99) if sequence_lengths else 0,
        }
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def analyze_complete_dataset(dataset_name, data_dir):
    """
    åˆ†æå®Œæ•´æ•°æ®é›†ï¼ˆtrain_valid + testï¼‰
    """
    print(f"\n{'='*80}")
    print(f"ğŸ” å®Œæ•´æ•°æ®é›†åˆ†æ: {dataset_name.upper()}")
    print(f"{'='*80}")
    
    train_valid_path = os.path.join(data_dir, "train_valid_sequences.csv")
    test_path = os.path.join(data_dir, "test_sequences.csv")
    
    # åˆ†ætrain_valid
    train_valid_stats = analyze_dataset_split(dataset_name, train_valid_path, "Train/Valid")
    
    # åˆ†ætest
    test_stats = analyze_dataset_split(dataset_name, test_path, "Test")
    
    # æ±‡æ€»ç»Ÿè®¡
    if train_valid_stats and test_stats:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {dataset_name} - æ•´ä½“æ±‡æ€»")
        print(f"{'='*80}\n")
        
        total_students = train_valid_stats['unique_students'] + test_stats['unique_students']
        total_interactions = train_valid_stats['total_interactions'] + test_stats['total_interactions']
        
        # åˆå¹¶conceptså’Œquestions (éœ€è¦é‡æ–°è¯»å–ä»¥è·å–å‡†ç¡®çš„å”¯ä¸€å€¼)
        print("â³ è®¡ç®—æ•´ä½“å”¯ä¸€Conceptså’ŒQuestions...")
        df_train_valid = pd.read_csv(train_valid_path)
        df_test = pd.read_csv(test_path)
        
        all_concepts_combined = set()
        all_questions_combined = set()
        
        for df in [df_train_valid, df_test]:
            for _, row in df.iterrows():
                concepts_str = str(row['concepts'])
                questions_str = str(row['questions'])
                
                if concepts_str != 'NA':
                    for c_str in concepts_str.split(','):
                        if c_str and c_str != '-1':
                            try:
                                all_concepts_combined.add(int(c_str))
                            except ValueError:
                                continue
                
                if questions_str != 'NA':
                    for q_str in questions_str.split(','):
                        if q_str and q_str != '-1':
                            try:
                                all_questions_combined.add(int(q_str))
                            except ValueError:
                                continue
        
        total_correct = train_valid_stats['total_correct'] + test_stats['total_correct']
        overall_accuracy = total_correct / total_interactions if total_interactions > 0 else 0
        
        earliest_ts = min(train_valid_stats['earliest_timestamp'], test_stats['earliest_timestamp']) if train_valid_stats['earliest_timestamp'] and test_stats['earliest_timestamp'] else None
        latest_ts = max(train_valid_stats['latest_timestamp'], test_stats['latest_timestamp']) if train_valid_stats['latest_timestamp'] and test_stats['latest_timestamp'] else None
        total_time_span = (latest_ts - earliest_ts) / (1000 * 60 * 60 * 24) if earliest_ts and latest_ts else 0
        
        print("\n  ğŸŒ æ•´ä½“ç»Ÿè®¡:")
        print(f"    - æ€»å­¦ç”Ÿæ•°: {total_students:,}")
        print(f"    - æ€»äº¤äº’æ•°: {total_interactions:,}")
        print(f"    - å”¯ä¸€Concepts: {len(all_concepts_combined):,}")
        print(f"    - å”¯ä¸€Questions: {len(all_questions_combined):,}")
        print(f"    - å¹³å‡æ¯ä¸ªå­¦ç”Ÿäº¤äº’æ•°: {total_interactions / total_students:.1f}")
        print(f"    - æ€»ä½“æ­£ç¡®ç‡: {overall_accuracy:.2%}")
        print(f"    - æ—¶é—´è·¨åº¦: {total_time_span:.1f} å¤©")
        
        print("\n  ğŸ“Š Train/Valid vs Test å¯¹æ¯”:")
        print(f"    - å­¦ç”Ÿåˆ†å¸ƒ: Train/Valid {train_valid_stats['unique_students']:,} ({train_valid_stats['unique_students']/total_students*100:.1f}%) | Test {test_stats['unique_students']:,} ({test_stats['unique_students']/total_students*100:.1f}%)")
        print(f"    - äº¤äº’åˆ†å¸ƒ: Train/Valid {train_valid_stats['total_interactions']:,} ({train_valid_stats['total_interactions']/total_interactions*100:.1f}%) | Test {test_stats['total_interactions']:,} ({test_stats['total_interactions']/total_interactions*100:.1f}%)")
        print(f"    - å¹³å‡åºåˆ—é•¿åº¦: Train/Valid {train_valid_stats['avg_sequence_length']:.1f} | Test {test_stats['avg_sequence_length']:.1f}")
        print(f"    - æ­£ç¡®ç‡: Train/Valid {train_valid_stats['overall_accuracy']:.2%} | Test {test_stats['overall_accuracy']:.2%}")
        print(f"    - Conceptsè¦†ç›–: Train/Valid {train_valid_stats['unique_concepts']:,} | Test {test_stats['unique_concepts']:,}")
        print(f"    - Questionsè¦†ç›–: Train/Valid {train_valid_stats['unique_questions']:,} | Test {test_stats['unique_questions']:,}")
        
        # è®¡ç®—Testä¸­æ–°å‡ºç°çš„conceptså’Œquestions
        df_train_concepts = set()
        df_test_concepts = set()
        df_train_questions = set()
        df_test_questions = set()
        
        for _, row in df_train_valid.iterrows():
            concepts_str = str(row['concepts'])
            questions_str = str(row['questions'])
            if concepts_str != 'NA':
                for c_str in concepts_str.split(','):
                    if c_str and c_str != '-1':
                        try:
                            df_train_concepts.add(int(c_str))
                        except ValueError:
                            continue
            if questions_str != 'NA':
                for q_str in questions_str.split(','):
                    if q_str and q_str != '-1':
                        try:
                            df_train_questions.add(int(q_str))
                        except ValueError:
                            continue
        
        for _, row in df_test.iterrows():
            concepts_str = str(row['concepts'])
            questions_str = str(row['questions'])
            if concepts_str != 'NA':
                for c_str in concepts_str.split(','):
                    if c_str and c_str != '-1':
                        try:
                            df_test_concepts.add(int(c_str))
                        except ValueError:
                            continue
            if questions_str != 'NA':
                for q_str in questions_str.split(','):
                    if q_str and q_str != '-1':
                        try:
                            df_test_questions.add(int(q_str))
                        except ValueError:
                            continue
        
        new_concepts_in_test = df_test_concepts - df_train_concepts
        new_questions_in_test = df_test_questions - df_train_questions
        
        print("\n  ğŸ†• Testä¸­çš„æ–°å†…å®¹:")
        print(f"    - æ–°Concepts: {len(new_concepts_in_test):,} ({len(new_concepts_in_test)/len(df_test_concepts)*100:.1f}% of test concepts)")
        print(f"    - æ–°Questions: {len(new_questions_in_test):,} ({len(new_questions_in_test)/len(df_test_questions)*100:.1f}% of test questions)")
        
        return {
            'dataset': dataset_name,
            'train_valid': train_valid_stats,
            'test': test_stats,
            'total_students': total_students,
            'total_interactions': total_interactions,
            'total_unique_concepts': len(all_concepts_combined),
            'total_unique_questions': len(all_questions_combined),
            'overall_accuracy': overall_accuracy,
            'total_time_span_days': total_time_span,
            'new_concepts_in_test': len(new_concepts_in_test),
            'new_questions_in_test': len(new_questions_in_test)
        }
    
    return None


if __name__ == "__main__":
    print("ğŸ” å®Œæ•´æ•°æ®é›†ç»Ÿè®¡åˆ†æ")
    print("=" * 80)
    print("\nè¿™ä¸ªåˆ†æåŒ…æ‹¬ï¼š")
    print("  1. Train/Validæ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡")
    print("  2. Testæ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡")
    print("  3. æ•´ä½“æ•°æ®é›†çš„æ±‡æ€»ç»Ÿè®¡")
    print("  4. Train/Validå’ŒTestçš„å¯¹æ¯”åˆ†æ")
    
    datasets = {
        'EdNet': '/mnt/localssd/pykt-toolkit/data/ednet',
        'ASSISTments2017': '/mnt/localssd/pykt-toolkit/data/assist2017'
    }
    
    all_results = {}
    
    for dataset_name, data_dir in datasets.items():
        result = analyze_complete_dataset(dataset_name, data_dir)
        if result:
            all_results[dataset_name] = result
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    if len(all_results) > 0:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ•°æ®é›†æ•´ä½“å¯¹æ¯”")
        print(f"{'='*80}\n")
        
        comparison_data = []
        for dataset_name, result in all_results.items():
            comparison_data.append({
                'Dataset': dataset_name,
                'Total_Students': result['total_students'],
                'Total_Interactions': result['total_interactions'],
                'Unique_Concepts': result['total_unique_concepts'],
                'Unique_Questions': result['total_unique_questions'],
                'Overall_Accuracy': f"{result['overall_accuracy']:.2%}",
                'Avg_Interactions_Per_Student': f"{result['total_interactions'] / result['total_students']:.1f}",
                'Time_Span_Days': f"{result['total_time_span_days']:.1f}",
                'New_Concepts_In_Test': result['new_concepts_in_test'],
                'New_Questions_In_Test': result['new_questions_in_test']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        print(comparison_df.to_string(index=False))
        
        # ä¿å­˜ç»“æœ
        output_path = "/tmp/complete_dataset_comparison.csv"
        comparison_df.to_csv(output_path, index=False)
        print(f"\nâœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_path}")
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡åˆ°JSON
        import json
        detailed_output_path = "/tmp/complete_dataset_stats.json"
        with open(detailed_output_path, 'w', encoding='utf-8') as f:
            # Convert datetime to string for JSON serialization
            for dataset_name in all_results:
                for split in ['train_valid', 'test']:
                    if split in all_results[dataset_name] and all_results[dataset_name][split]:
                        stats = all_results[dataset_name][split]
                        if stats.get('earliest_timestamp'):
                            stats['earliest_timestamp'] = str(datetime.fromtimestamp(stats['earliest_timestamp'] / 1000))
                        if stats.get('latest_timestamp'):
                            stats['latest_timestamp'] = str(datetime.fromtimestamp(stats['latest_timestamp'] / 1000))
            
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        print(f"âœ… è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜: {detailed_output_path}")
    
    print(f"\n{'='*80}")
    print("âœ… å®Œæ•´æ•°æ®é›†åˆ†æå®Œæˆï¼")
    print(f"{'='*80}")

