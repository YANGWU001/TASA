#!/usr/bin/env python3
"""
é‡æ–°ç”Ÿæˆoverall.json - ä½¿ç”¨æœ€ä¼˜tauï¼ˆä¸­ä½æ•°ï¼‰
1. ä½¿ç”¨delta_tçš„ä¸­ä½æ•°ä½œä¸ºtau
2. é‡æ–°è®¡ç®—æ‰€æœ‰æ–¹æ³•çš„FS
3. ä¸ºæ¯ä¸ªæ–¹æ³•ç‹¬ç«‹è®¡ç®—level
"""

import os
import json
import numpy as np
from collections import defaultdict
import argparse

def load_method_data(dataset, method):
    """åŠ è½½æŸä¸ªæ–¹æ³•çš„æ•°æ®"""
    file_path = f'/mnt/localssd/bank/forgetting/{dataset}/{method}.json'
    
    if not os.path.exists(file_path):
        return None
    
    try:
        with open(file_path) as f:
            data = json.load(f)
        print(f"  âœ… {method:10} - {len(data)} å­¦ç”Ÿ")
        return data
    except Exception as e:
        print(f"  âŒ {method:10} - åŠ è½½å¤±è´¥: {e}")
        return None

def generate_overall(dataset):
    """ç”Ÿæˆoverall.json"""
    print("="*100)
    print(f"ğŸ“Š ç”Ÿæˆ Overall.json (ä¼˜åŒ–tau) for {dataset.upper()}")
    print("="*100)
    print()
    
    # 1. åŠ è½½æ‰€æœ‰æ–¹æ³•çš„æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    methods = ['history', 'lpkt', 'dkt', 'akt', 'simplekt']
    method_data = {}
    
    for method in methods:
        data = load_method_data(dataset, method)
        if data is not None:
            method_data[method] = data
    
    if 'history' not in method_data:
        print("âŒ History.json ä¸å­˜åœ¨ï¼")
        return
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(method_data)} ä¸ªæ–¹æ³•\n")
    
    # 2. è®¡ç®—æœ€ä¼˜tauï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼‰
    print("ğŸ”§ è®¡ç®—æœ€ä¼˜tau...")
    history_data = method_data['history']
    
    all_delta_t = []
    for uid, concepts in history_data.items():
        for concept_text, concept_data in concepts.items():
            delta_t = concept_data.get('delta_t')
            if delta_t is not None and delta_t > 0:
                all_delta_t.append(delta_t)
    
    optimal_tau = np.median(all_delta_t)
    print(f"  Delta_tç»Ÿè®¡:")
    print(f"    ä¸­ä½æ•°: {optimal_tau:.2f} åˆ†é’Ÿ")
    print(f"    å¹³å‡å€¼: {np.mean(all_delta_t):.2f} åˆ†é’Ÿ")
    print(f"    Q25: {np.percentile(all_delta_t, 25):.2f}, Q75: {np.percentile(all_delta_t, 75):.2f}")
    print(f"  âœ… ä½¿ç”¨æœ€ä¼˜tau = {optimal_tau:.2f} åˆ†é’Ÿ\n")
    
    # 3. ä¸ºæ¯ä¸ªæ–¹æ³•é‡æ–°è®¡ç®—FSå¹¶æ”¶é›†
    print("ğŸ”„ é‡æ–°è®¡ç®—æ‰€æœ‰FS...")
    overall_data = {}
    method_fs_collection = {m: [] for m in methods}
    
    for uid, concepts in history_data.items():
        overall_data[uid] = {}
        
        for concept_text, history_info in concepts.items():
            delta_t = history_info.get('delta_t', 0)
            
            concept_entry = {
                'methods': {}
            }
            
            # ä¸ºæ¯ä¸ªæ–¹æ³•é‡æ–°è®¡ç®—FS
            for method in methods:
                if method not in method_data:
                    continue
                
                if uid in method_data[method] and concept_text in method_data[method][uid]:
                    method_info = method_data[method][uid][concept_text]
                    s_tc = method_info.get('s_tc', 0)
                    
                    # ä½¿ç”¨æ–°çš„taué‡æ–°è®¡ç®—FS
                    if delta_t > 0:
                        time_factor = delta_t / (delta_t + optimal_tau)
                        fs = (1 - s_tc) * time_factor
                    else:
                        fs = 0.0
                    
                    concept_entry['methods'][method] = {
                        's_tc': float(s_tc),
                        'fs': float(fs),
                        'level': 'medium'  # å…ˆå ä½
                    }
                    
                    method_fs_collection[method].append((uid, concept_text, fs))
            
            # å…±åŒå­—æ®µ
            concept_entry['delta_t'] = float(delta_t)
            concept_entry['tau'] = float(optimal_tau)
            concept_entry['last_response'] = int(history_info.get('last_response', 0))
            concept_entry['num_attempts'] = int(history_info.get('num_attempts', 0))
            
            overall_data[uid][concept_text] = concept_entry
    
    print(f"  âœ… é‡æ–°è®¡ç®—å®Œæˆ\n")
    
    # 4. ä¸ºæ¯ä¸ªæ–¹æ³•ç‹¬ç«‹è®¡ç®—level
    print("ğŸ“ˆ ä¸ºæ¯ä¸ªæ–¹æ³•ç‹¬ç«‹è®¡ç®—Level...")
    
    for method in methods:
        if method not in method_data or len(method_fs_collection[method]) == 0:
            continue
        
        # æå–æ‰€æœ‰FSå€¼
        fs_values = [fs for _, _, fs in method_fs_collection[method]]
        
        # è®¡ç®—ä¸‰åˆ†ä½æ•°
        q33 = np.percentile(fs_values, 33)
        q67 = np.percentile(fs_values, 67)
        
        print(f"  {method:10} - FS: min={np.min(fs_values):.4f}, Q33={q33:.4f}, median={np.median(fs_values):.4f}, Q67={q67:.4f}, max={np.max(fs_values):.4f}")
        
        # åˆ†é…level
        for uid, concept_text, fs in method_fs_collection[method]:
            if fs < q33:
                level = 'low'
            elif fs < q67:
                level = 'medium'
            else:
                level = 'high'
            
            overall_data[uid][concept_text]['methods'][method]['level'] = level
    
    print()
    
    # 5. ç»Ÿè®¡
    total_students = len(overall_data)
    total_concepts = sum(len(concepts) for concepts in overall_data.values())
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  å­¦ç”Ÿæ•°: {total_students}")
    print(f"  Conceptæ¡ç›®: {total_concepts}")
    
    for method in methods:
        count = sum(1 for uid in overall_data for ct in overall_data[uid] if method in overall_data[uid][ct]['methods'])
        coverage = count / total_concepts * 100 if total_concepts > 0 else 0
        print(f"  {method:10} - è¦†ç›–ç‡: {coverage:5.1f}% ({count}/{total_concepts})")
    
    # 6. ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜åˆ°Bank...")
    output_dir = f'/mnt/localssd/bank/forgetting/{dataset}'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'overall.json')
    
    with open(output_file, 'w') as f:
        json.dump(overall_data, f, indent=2)
    
    file_size = os.path.getsize(output_file) / 1024 / 1024
    print(f"  âœ… å·²ä¿å­˜: {output_file}")
    print(f"  ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.1f}MB")
    
    # 7. ç¤ºä¾‹æ•°æ®
    print(f"\nğŸ“‹ æ•°æ®ç¤ºä¾‹:")
    sample_uid = list(overall_data.keys())[0]
    sample_concept = list(overall_data[sample_uid].keys())[0]
    sample_data = overall_data[sample_uid][sample_concept]
    
    print(f"  å­¦ç”ŸID: {sample_uid}")
    print(f"  Concept: {sample_concept}")
    print(f"  æ–°tau: {sample_data['tau']:.2f} åˆ†é’Ÿ, delta_t: {sample_data['delta_t']:.2f} åˆ†é’Ÿ")
    print(f"  Methods:")
    for method, values in sample_data['methods'].items():
        print(f"    {method:10} - s_tc={values['s_tc']:.4f}, fs={values['fs']:.4f}, level={values['level']}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, required=True)
    
    args = parser.parse_args()
    
    generate_overall(args.dataset)
    
    print("\n" + "="*100)
    print("âœ… å®Œæˆï¼")
    print("="*100)

if __name__ == '__main__':
    main()
