#!/usr/bin/env python3
"""
å¯¹3ä¸ªç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿè¿›è¡ŒTASAæµ‹è¯•
æ¯ä¸ªå­¦ç”Ÿ: 1æ¬¡tutoring + 3æ¬¡post-test
"""

import json
import sys
from student_roleplay_evaluation import build_student_system_prompt, load_session
from tasa_tutoring import TASATutor
from tasa_evaluation_multi_run import TASAMultiRunEvaluator

def main():
    # 3ä¸ªç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ
    student_ids = [1001, 1002, 1004]
    dataset = "assist2017"
    
    print("="*80)
    print("ğŸš€ TASAæµ‹è¯•: 3ä¸ªå­¦ç”Ÿ Ã— (1æ¬¡tutoring + 3æ¬¡post-test)")
    print("="*80)
    
    # åˆå§‹åŒ–
    tutor = TASATutor()
    multi_evaluator = TASAMultiRunEvaluator(num_runs=3)
    
    all_results = []
    
    for idx, student_id in enumerate(student_ids, 1):
        print(f"\n\n{'#'*80}")
        print(f"# å­¦ç”Ÿ {idx}/3: ID={student_id}")
        print(f"{'#'*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        concept_text = session['concept_text']
        concept_id = str(session['concept_id'])
        
        print(f"\nğŸ“‹ å­¦ç”Ÿä¿¡æ¯:")
        print(f"   ID: {student_id}")
        print(f"   Concept: {concept_text}")
        
        # æ„å»ºstudent prompt
        student_prompt = build_student_system_prompt(session)
        
        # Step 1: ç”Ÿæˆdialogueï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        dialogue_file = f'/mnt/localssd/bank/dialogue/TASA/{dataset}/{student_id}-{concept_text}.json'
        import os
        
        if not os.path.exists(dialogue_file):
            print(f"\nğŸ“ Step 1: è¿›è¡ŒTutoring (10è½®æ•™å­¦)")
            
            try:
                dialogue = tutor.conduct_tutoring_session(
                    student_id=student_id,
                    dataset=dataset,
                    concept_text=concept_text,
                    student_system_prompt=student_prompt
                )
                
                # ä¿å­˜dialogue
                tutor.save_dialogue(dialogue, student_id, concept_text, dataset)
                print(f"   âœ… Tutoringå®Œæˆ")
                
            except Exception as e:
                print(f"   âŒ Tutoringå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        else:
            print(f"\nâœ… Dialogueå·²å­˜åœ¨ï¼Œè·³è¿‡tutoring")
        
        # Step 2: è¿›è¡Œ3æ¬¡post-testè¯„ä¼°
        print(f"\nğŸ“Š Step 2: è¿›è¡Œ3æ¬¡Post-testè¯„ä¼°")
        
        try:
            result = multi_evaluator.evaluate_student_multi_runs(student_id, dataset)
            multi_evaluator.save_multi_run_result(result)
            all_results.append(result)
            
        except Exception as e:
            print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ±‡æ€»ç»“æœ
    print(f"\n\n{'='*80}")
    print(f"ğŸ“Š æœ€ç»ˆæ±‡æ€»: {len(all_results)}ä¸ªå­¦ç”Ÿçš„TASAæµ‹è¯•ç»“æœ")
    print(f"{'='*80}")
    
    for result in all_results:
        print(f"\nâ”Œ{'â”€'*78}â”")
        print(f"â”‚ å­¦ç”Ÿ{result['student_id']} - {result['concept_text']:<63} â”‚")
        print(f"â”œ{'â”€'*78}â”¤")
        print(f"â”‚ å†å²å‡†ç¡®ç‡:   {result['original_accuracy']*100:5.1f}%                                                â”‚")
        print(f"â”‚ Pre-test:     {result['pre_test_accuracy']*100:5.1f}%   (å·®è· {result['accuracy_deviation']*100:.1f}%)                              â”‚")
        print(f"â”‚ Post-test:    {result['avg_post_test_accuracy']*100:5.1f}% Â± {result['std_post_test_accuracy']*100:4.1f}%                                       â”‚")
        print(f"â”‚                                                                              â”‚")
        print(f"â”‚ Learning Gain: {result['avg_learning_gain']*100:4.1f}% Â± {result['std_learning_gain']*100:3.1f}%                                       â”‚")
        print(f"â”‚ ç»å¯¹æå‡:     {result['avg_improvement']*100:+5.1f}% Â± {result['std_improvement']*100:4.1f}%                                       â”‚")
        print(f"â””{'â”€'*78}â”˜")
        
        # æ˜¾ç¤ºæ¯æ¬¡è¿è¡Œè¯¦æƒ…
        print(f"   è¯¦ç»†ç»“æœ:")
        for run in result['runs']:
            print(f"      Run {run['run_id']}: Post={run['post_test_accuracy']*100:4.1f}%, "
                  f"Gain={run['learning_gain']*100:+5.1f}%")
    
    # è®¡ç®—æ€»ä½“å¹³å‡
    if all_results:
        import numpy as np
        
        all_gains = [r['avg_learning_gain'] for r in all_results]
        overall_avg_gain = np.mean(all_gains)
        overall_std_gain = np.std(all_gains, ddof=1) if len(all_gains) > 1 else 0
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ æ•´ä½“ç»Ÿè®¡ (3ä¸ªå­¦ç”Ÿçš„å¹³å‡)")
        print(f"{'='*80}")
        print(f"   å¹³å‡Learning Gain: {overall_avg_gain*100:.1f}% Â± {overall_std_gain*100:.1f}%")
        
        # ä¿å­˜æ•´ä½“ç»Ÿè®¡
        summary = {
            "num_students": len(all_results),
            "num_runs_per_student": 3,
            "overall_avg_learning_gain": overall_avg_gain,
            "overall_std_learning_gain": overall_std_gain,
            "students": all_results
        }
        
        summary_file = "/mnt/localssd/bank/evaluation_results/TASA-multi/assist2017/summary_3students.json"
        os.makedirs(os.path.dirname(summary_file), exist_ok=True)
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print(f"\nğŸ’¾ æ•´ä½“ç»Ÿè®¡å·²ä¿å­˜è‡³: {summary_file}")
    
    print(f"\n{'='*80}")
    print(f"âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*80}")

if __name__ == "__main__":
    main()

