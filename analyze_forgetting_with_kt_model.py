#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„KTæ¨¡å‹ï¼ˆLPKTï¼‰é¢„æµ‹s_{t,c}ï¼Œç„¶åè®¡ç®—Forgetting Score
"""

import pandas as pd
import numpy as np
from collections import defaultdict
import random
import os
import json
import torch
import torch.nn as nn
from pykt.models.lpkt import LPKT

# è®¾ç½®éšæœºç§å­
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# æ•°æ®é›†é…ç½®
DATASETS = {
    'assist2017': {
        'name': 'ASSISTments2017',
        'data_path': '/mnt/localssd/pykt-toolkit/data/assist2017',
        'model_path': '/mnt/localssd/pykt-toolkit/examples/saved_model/assist2017_lpkt_qid_saved_model_42_0_0.003_0.2_64_64_64_0.03_1_0',
    },
    'ednet': {
        'name': 'EdNet',
        'data_path': '/mnt/localssd/pykt-toolkit/data/ednet',
        'model_path': '/mnt/localssd/pykt-toolkit/examples/saved_model/ednet_lpkt_qid_saved_model_42_0_0.003_0.2_64_64_64_0.03_1_0',
    },
    'algebra2005': {
        'name': 'Algebra2005',
        'data_path': '/mnt/localssd/pykt-toolkit/data/algebra2005',
        'model_path': '/mnt/localssd/pykt-toolkit/examples/saved_model/algebra2005_lpkt_qid_saved_model_42_0_0.003_0.2_50_128_128_0.03_1_0',
    },
    'bridge2006': {
        'name': 'Bridge2Algebra2006',
        'data_path': '/mnt/localssd/pykt-toolkit/data/bridge2algebra2006',
        'model_path': '/mnt/localssd/pykt-toolkit/examples/saved_model/bridge2algebra2006_lpkt_qid_saved_model_42_0_0.003_0.2_50_128_128_0.03_1_0',
    },
}

print("="*120)
print("ä½¿ç”¨KTæ¨¡å‹ï¼ˆLPKTï¼‰é¢„æµ‹s_{t,c}æ¥è®¡ç®—Forgetting Score")
print("="*120)

# è¾…åŠ©å‡½æ•°
def parse_field(field_str):
    """è§£æCSVå­—æ®µ"""
    if pd.isna(field_str) or field_str == '' or str(field_str) == '-1':
        return []
    return [int(x) for x in str(field_str).split(',') if x.strip() != '-1' and x.strip() != '']

def calculate_forgetting_score(s_tc, delta_t_minutes, tau):
    """è®¡ç®—forgetting score"""
    if delta_t_minutes <= 0:
        return 0.0
    time_factor = delta_t_minutes / (delta_t_minutes + tau)
    return (1 - s_tc) * time_factor

def load_model_and_config(dataset_key):
    """åŠ è½½KTæ¨¡å‹å’Œé…ç½®"""
    model_dir = DATASETS[dataset_key]['model_path']
    
    # åŠ è½½é…ç½®
    config_path = os.path.join(model_dir, 'config.json')
    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return None, None, None
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    # æŸ¥æ‰¾checkpoint
    checkpoint_files = [
        'qid_model.ckpt',
        'model.ckpt',
        'best_model.ckpt'
    ]
    
    checkpoint_path = None
    for ckpt_file in checkpoint_files:
        ckpt_path = os.path.join(model_dir, ckpt_file)
        if os.path.exists(ckpt_path):
            checkpoint_path = ckpt_path
            break
    
    if checkpoint_path is None:
        print(f"âŒ æœªæ‰¾åˆ°checkpointæ–‡ä»¶")
        return None, None, None
    
    # æå–æ¨¡å‹é…ç½®ï¼ˆè¿‡æ»¤æ‰è®­ç»ƒç›¸å…³å‚æ•°ï¼‰
    data_config = config['data_config']
    model_params = config.get('model_config', {})
    params = config.get('params', {})
    
    # LPKTéœ€è¦çš„å‚æ•°
    n_question = data_config['num_q']
    n_exercise = data_config['num_c']
    n_at = data_config.get('num_at', n_question)
    n_it = data_config.get('num_it', n_question)
    d_a = model_params.get('d_a', params.get('d_a', 64))
    d_e = model_params.get('d_e', params.get('d_e', 64))
    d_k = model_params.get('d_k', params.get('d_k', 64))
    gamma = model_params.get('gamma', params.get('gamma', 0.03))
    dropout = model_params.get('dropout', params.get('dropout', 0.2))
    
    # æ„å»ºq_matrixï¼ˆquestionåˆ°conceptçš„æ˜ å°„ï¼‰
    # ç®€åŒ–ç‰ˆæœ¬ï¼šå‡è®¾æ¯ä¸ªquestionå¯¹åº”ä¸€ä¸ªconcept
    device = torch.device('cpu')  # å¼ºåˆ¶ä½¿ç”¨CPUé¿å…CUDAç´¢å¼•é—®é¢˜
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    q_matrix = torch.ones((n_question, n_exercise)) * gamma
    q_matrix = q_matrix.to(device)
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = LPKT(
        n_at=n_at,
        n_it=n_it,
        n_exercise=n_exercise,
        n_question=n_question,
        d_a=d_a,
        d_e=d_e,
        d_k=d_k,
        gamma=gamma,
        dropout=dropout,
        q_matrix=q_matrix,
        emb_type="qid",
        emb_path="",
        pretrain_dim=768,
        use_time=True
    )
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # è¿‡æ»¤æ‰ä¸åŒ¹é…çš„æƒé‡
    model_state_dict = model.state_dict()
    filtered_state_dict = {}
    for k, v in checkpoint.items():
        if k in model_state_dict and v.shape == model_state_dict[k].shape:
            filtered_state_dict[k] = v
    
    model.load_state_dict(filtered_state_dict, strict=False)
    model.to(device)
    model.eval()
    
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {checkpoint_path}")
    print(f"   num_q={data_config['num_q']}, num_c={data_config['num_c']}")
    
    return model, data_config, device

def predict_with_model(model, device, questions, concepts, responses, timestamps, num_q, num_c):
    """ä½¿ç”¨KTæ¨¡å‹é¢„æµ‹åºåˆ— - å‚è€ƒevaluate_model.pyçš„å®ç°"""
    if len(questions) < 2:
        return None
    
    try:
        # å‡†å¤‡è¾“å…¥ï¼ˆä¸åŒ…æ‹¬æœ€åä¸€æ¬¡ï¼‰
        q_seq = questions[:-1]
        r_seq = responses[:-1]
        
        # æ£€æŸ¥question IDæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        if max(q_seq) >= num_q:
            # print(f"  âš ï¸  Question IDè¶…å‡ºèŒƒå›´: max={max(q_seq)}, num_q={num_q}")
            return None
        
        # è®¡ç®—æ—¶é—´é—´éš”
        time_intervals = []
        for i in range(1, len(timestamps) - 1):
            interval_ms = timestamps[i] - timestamps[i-1]
            interval_minutes = max(0, interval_ms / (1000 * 60))
            # é™åˆ¶æ—¶é—´é—´éš”çš„æœ€å¤§å€¼ï¼Œé¿å…ç´¢å¼•é—®é¢˜
            time_intervals.append(min(int(interval_minutes), 10000))
        
        # ç¬¬ä¸€ä¸ªæ—¶é—´é—´éš”è®¾ä¸º0
        time_intervals = [0] + time_intervals
        
        # æ„å»ºshiftedåºåˆ—ï¼ˆå‚è€ƒevaluate_model.pyçš„æ–¹å¼ï¼‰
        q_tensor = torch.LongTensor([q_seq]).to(device)
        r_tensor = torch.LongTensor([r_seq]).to(device)
        it_tensor = torch.LongTensor([time_intervals]).to(device)
        
        # æ„å»ºshiftedç‰ˆæœ¬ï¼ˆå‘å³ç§»åŠ¨ä¸€ä½ï¼Œé¦–ä½è¡¥0ï¼‰
        qshft_tensor = torch.cat([torch.zeros(1, 1, dtype=torch.long).to(device), q_tensor[:, :-1]], dim=1)
        rshft_tensor = torch.cat([torch.zeros(1, 1, dtype=torch.long).to(device), r_tensor[:, :-1]], dim=1)
        itshft_tensor = torch.cat([torch.zeros(1, 1, dtype=torch.long).to(device), it_tensor[:, :-1]], dim=1)
        
        # æ‹¼æ¥åŸå§‹å’Œshiftedï¼ˆå‚è€ƒevaluate.pyçš„æ–¹å¼ï¼‰
        cq = torch.cat([q_tensor[:, 0:1], qshft_tensor], dim=1)
        cr = torch.cat([r_tensor[:, 0:1], rshft_tensor], dim=1)
        cit = torch.cat([it_tensor[:, 0:1], itshft_tensor], dim=1)
        
        # é¢„æµ‹ - å‚è€ƒevaluate_model.py line 133
        with torch.no_grad():
            y = model(cq.long(), cr.long(), cit.long())
            
            # è¿”å›æœ€åä¸€æ­¥çš„é¢„æµ‹ï¼ˆå¯¹åº”å€’æ•°ç¬¬äºŒæ¬¡ç­”é¢˜ä¹‹åçš„çŠ¶æ€ï¼‰
            pred_prob = torch.sigmoid(y[0, -1]).item()
            return pred_prob
            
    except Exception as e:
        # print(f"  âš ï¸  æ¨¡å‹é¢„æµ‹å¤±è´¥: {type(e).__name__}: {str(e)[:100]}")
        return None

def analyze_interval_distribution(df):
    """åˆ†ææ•°æ®é›†çš„ç­”é¢˜é—´éš”åˆ†å¸ƒ"""
    all_intervals = []
    for _, row in df.iterrows():
        timestamps = parse_field(row['timestamps'])
        concepts = parse_field(row['concepts'])
        if len(timestamps) < 2:
            continue
        concept_timestamps = defaultdict(list)
        for i, cid in enumerate(concepts):
            concept_timestamps[cid].append(timestamps[i])
        for cid, ts_list in concept_timestamps.items():
            if len(ts_list) >= 2:
                ts_list_sorted = sorted(ts_list)
                interval_ms = ts_list_sorted[-1] - ts_list_sorted[-2]
                interval_minutes = interval_ms / (1000 * 60)
                all_intervals.append(interval_minutes)
    return np.array(all_intervals)

def analyze_student_with_kt_model(student_row, model, device, tau, num_q, num_c):
    """ä½¿ç”¨KTæ¨¡å‹åˆ†æå­¦ç”Ÿ"""
    questions = parse_field(student_row['questions'])
    concepts = parse_field(student_row['concepts'])
    responses = parse_field(student_row['responses'])
    timestamps = parse_field(student_row['timestamps'])
    
    if len(concepts) < 2:
        return None
    
    # æŒ‰conceptåˆ†ç»„
    concept_data = defaultdict(list)
    for i in range(len(concepts)):
        concept_data[concepts[i]].append({
            'index': i,
            'question': questions[i],
            'response': responses[i],
            'timestamp': timestamps[i]
        })
    
    results = []
    
    for cid, interactions in concept_data.items():
        if len(interactions) < 2:
            continue
        
        # æŒ‰æ—¶é—´æ’åº
        interactions_sorted = sorted(interactions, key=lambda x: x['timestamp'])
        
        # æå–è¯¥conceptçš„æ‰€æœ‰ç´¢å¼•
        indices = [inter['index'] for inter in interactions_sorted]
        
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹ï¼šè¾“å…¥ä»å¼€å§‹åˆ°å€’æ•°ç¬¬äºŒæ¬¡çš„æ‰€æœ‰æ•°æ®
        # é¢„æµ‹å€’æ•°ç¬¬äºŒæ¬¡ç­”é¢˜åçš„çŸ¥è¯†çŠ¶æ€
        last_index = indices[-1]
        second_last_index = indices[-2]
        
        # è·å–ä»å¼€å§‹åˆ°å€’æ•°ç¬¬äºŒæ¬¡è¯¥conceptå‡ºç°æ—¶çš„æ‰€æœ‰æ•°æ®
        end_pos = second_last_index + 1
        
        pred_prob = predict_with_model(
            model, device,
            questions[:end_pos],
            concepts[:end_pos],
            responses[:end_pos],
            timestamps[:end_pos],
            num_q, num_c
        )
        
        if pred_prob is None:
            # å¦‚æœæ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°å†å²å‡†ç¡®ç‡
            historical_responses = [inter['response'] for inter in interactions_sorted[:-1]]
            s_tc = sum(historical_responses) / len(historical_responses)
            pred_method = 'historical'
        else:
            s_tc = pred_prob
            pred_method = 'model'
        
        # è®¡ç®—æœ€åä¸¤æ¬¡çš„æ—¶é—´é—´éš”
        last_timestamp = interactions_sorted[-1]['timestamp']
        second_last_timestamp = interactions_sorted[-2]['timestamp']
        delta_t_ms = last_timestamp - second_last_timestamp
        delta_t_minutes = max(0, delta_t_ms / (1000 * 60))
        
        # è®¡ç®—forgetting score
        forgetting_score = calculate_forgetting_score(s_tc, delta_t_minutes, tau)
        time_factor = delta_t_minutes / (delta_t_minutes + tau) if delta_t_minutes > 0 else 0
        
        results.append({
            'concept_id': cid,
            'total_attempts': len(interactions),
            'predicted_prob': s_tc,
            'pred_method': pred_method,
            'last_response': interactions_sorted[-1]['response'],
            'delta_t_minutes': delta_t_minutes,
            'delta_t_hours': delta_t_minutes / 60,
            'delta_t_days': delta_t_minutes / (60 * 24),
            'time_factor': time_factor,
            'forgetting_score': forgetting_score,
        })
    
    if len(results) == 0:
        return None
    
    fs_values = [r['forgetting_score'] for r in results]
    
    return {
        'uid': student_row['uid'],
        'concept_results': results,
        'fs_mean': np.mean(fs_values),
        'fs_std': np.std(fs_values),
        'fs_min': np.min(fs_values),
        'fs_max': np.max(fs_values),
    }

# å¤„ç†æ¯ä¸ªæ•°æ®é›†
for dataset_key in ['assist2017', 'ednet', 'algebra2005', 'bridge2006']:
    dataset_info = DATASETS[dataset_key]
    
    print(f"\n{'='*120}")
    print(f"æ•°æ®é›†: {dataset_info['name']}")
    print(f"{'='*120}")
    
    # ç¬¬1æ­¥ï¼šåŠ è½½æ¨¡å‹
    print(f"\nç¬¬1æ­¥ï¼šåŠ è½½LPKTæ¨¡å‹")
    print("-"*120)
    
    model, data_config, device = load_model_and_config(dataset_key)
    if model is None:
        print(f"âŒ è·³è¿‡è¯¥æ•°æ®é›†")
        continue
    
    num_q = data_config['num_q']
    num_c = data_config['num_c']
    
    # ç¬¬2æ­¥ï¼šåŠ è½½æ•°æ®
    test_path = os.path.join(dataset_info['data_path'], 'test_sequences.csv')
    if not os.path.exists(test_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_path}")
        continue
    
    df = pd.read_csv(test_path)
    print(f"âœ… åŠ è½½æ•°æ®: {len(df)} ä¸ªå­¦ç”Ÿ")
    
    # ç¬¬3æ­¥ï¼šåˆ†æé—´éš”åˆ†å¸ƒå¹¶ç¡®å®šÏ„
    print(f"\nç¬¬2æ­¥ï¼šç¡®å®šÏ„å€¼")
    print("-"*120)
    
    all_intervals = analyze_interval_distribution(df)
    tau_selected = np.mean(all_intervals)
    tau_days = tau_selected / (60 * 24)
    
    print(f"âœ… Ï„ = {tau_selected:.2f} åˆ†é’Ÿ = {tau_selected/60:.2f} å°æ—¶ = {tau_days:.2f} å¤©")
    
    # ç¬¬4æ­¥ï¼šé€‰æ‹©å­¦ç”Ÿ
    print(f"\nç¬¬3æ­¥ï¼šé€‰æ‹©5ä¸ªå­¦ç”Ÿè¿›è¡Œåˆ†æ")
    print("-"*120)
    
    qualified_students = []
    for uid in df['uid'].unique()[:50]:  # åªæ£€æŸ¥å‰50ä¸ªå­¦ç”Ÿä»¥åŠ é€Ÿ
        student_row = df[df['uid'] == uid].iloc[0]
        # å¿«é€Ÿæ£€æŸ¥
        concepts = parse_field(student_row['concepts'])
        concept_counts = defaultdict(int)
        for c in concepts:
            concept_counts[c] += 1
        valid_concepts = sum(1 for count in concept_counts.values() if count >= 2)
        if valid_concepts >= 5:
            qualified_students.append(uid)
    
    if len(qualified_students) == 0:
        print(f"âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ")
        continue
    
    selected_students = random.sample(qualified_students, min(5, len(qualified_students)))
    print(f"âœ… ä» {len(qualified_students)} ä¸ªå­¦ç”Ÿä¸­é€‰æ‹© {len(selected_students)} ä¸ª:")
    for i, uid in enumerate(selected_students, 1):
        print(f"   {i}. å­¦ç”ŸID: {uid}")
    
    # ç¬¬5æ­¥ï¼šè¯¦ç»†åˆ†æ
    print(f"\nç¬¬4æ­¥ï¼šä½¿ç”¨LPKTæ¨¡å‹é¢„æµ‹å¹¶è®¡ç®—Forgetting Score")
    print("="*120)
    
    for idx, uid in enumerate(selected_students, 1):
        student_row = df[df['uid'] == uid].iloc[0]
        
        print(f"\n{'-'*120}")
        print(f"å­¦ç”Ÿ #{idx}: ID {uid}")
        print(f"{'-'*120}")
        
        analysis = analyze_student_with_kt_model(
            student_row, model, device, tau_selected, num_q, num_c
        )
        
        if not analysis:
            print(f"  âš ï¸  è¯¥å­¦ç”Ÿæ•°æ®ä¸è¶³")
            continue
        
        print(f"\nForgetting Scoreç»Ÿè®¡:")
        print(f"  å¹³å‡å€¼: {analysis['fs_mean']:.4f}")
        print(f"  æ ‡å‡†å·®: {analysis['fs_std']:.4f}")
        print(f"  èŒƒå›´: [{analysis['fs_min']:.4f}, {analysis['fs_max']:.4f}]")
        
        results = analysis['concept_results']
        results_sorted = sorted(results, key=lambda x: x['forgetting_score'], reverse=True)
        
        # ç»Ÿè®¡é¢„æµ‹æ–¹æ³•
        model_pred_count = sum(1 for r in results if r['pred_method'] == 'model')
        hist_pred_count = sum(1 for r in results if r['pred_method'] == 'historical')
        print(f"  é¢„æµ‹æ–¹æ³•: æ¨¡å‹é¢„æµ‹={model_pred_count}ä¸ª, å†å²å‡†ç¡®ç‡={hist_pred_count}ä¸ª")
        
        print(f"\nå‰10ä¸ªæœ€éœ€è¦å¤ä¹ çš„Concepts:")
        print(f"  {'Concept':<10} {'æ¬¡æ•°':<6} {'é¢„æµ‹æ¦‚ç‡':<12} {'æ–¹æ³•':<8} {'é—´éš”':<12} "
              f"{'æ—¶é—´å› å­':<12} {'FS':<12} {'æœ€å':<6} {'åˆ†ç±»':<10}")
        print(f"  {'-'*116}")
        
        for result in results_sorted[:10]:
            cid = result['concept_id']
            attempts = result['total_attempts']
            pred_prob = result['predicted_prob'] * 100
            pred_method = "ğŸ¤–æ¨¡å‹" if result['pred_method'] == 'model' else "ğŸ“Šå†å²"
            
            if result['delta_t_days'] >= 1:
                interval_str = f"{result['delta_t_days']:.1f}d"
            elif result['delta_t_hours'] >= 1:
                interval_str = f"{result['delta_t_hours']:.1f}h"
            else:
                interval_str = f"{result['delta_t_minutes']:.1f}m"
            
            time_factor = result['time_factor']
            fs = result['forgetting_score']
            last_resp = "âœ…" if result['last_response'] == 1 else "âŒ"
            
            if fs >= 0.3:
                category = "ğŸ”´ ç´§æ€¥"
            elif fs >= 0.2:
                category = "ğŸŸ  é‡è¦"
            elif fs >= 0.1:
                category = "ğŸŸ¡ ä¸€èˆ¬"
            else:
                category = "ğŸŸ¢ ç»´æŒ"
            
            print(f"  {cid:<10} {attempts:<6} {pred_prob:<11.1f}% {pred_method:<8} {interval_str:<12} "
                  f"{time_factor:<12.4f} {fs:<12.4f} {last_resp:<6} {category:<10}")

print("\n" + "="*120)
print("âœ… æ‰€æœ‰æ•°æ®é›†åˆ†æå®Œæˆï¼")
print("="*120)
print("\nå¯¹æ¯”è¯´æ˜:")
print("  ğŸ¤–æ¨¡å‹: ä½¿ç”¨LPKTæ¨¡å‹é¢„æµ‹çš„s_{t,c}")
print("  ğŸ“Šå†å²: æ¨¡å‹é¢„æµ‹å¤±è´¥æ—¶å›é€€åˆ°å†å²å‡†ç¡®ç‡")
print("\nä¼˜åŠ¿:")
print("  âœ… LPKTè€ƒè™‘äº†å­¦ä¹ è½¨è¿¹å’Œåºåˆ—ä¿¡æ¯")
print("  âœ… æ¯”ç®€å•å¹³å‡æ›´èƒ½åæ˜ çœŸå®çš„çŸ¥è¯†çŠ¶æ€")
print("  âœ… èƒ½æ•æ‰å­¦ä¹ æ•ˆåº”å’Œé—å¿˜æ•ˆåº”")

