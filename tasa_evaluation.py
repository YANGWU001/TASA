"""
TASA Evaluationæ¨¡å—
è¿›è¡ŒPost-testè¯„ä¼°å¹¶è®¡ç®—Learning Gain
"""

import json
import os
from typing import List, Dict, Tuple
from openai import OpenAI

from tasa_config import *
from student_roleplay_evaluation import grade_answers

class TASAEvaluator:
    def __init__(self):
        """åˆå§‹åŒ–TASAè¯„ä¼°å™¨"""
        print("ğŸ”§ åˆå§‹åŒ–TASA Evaluator...")
        
        self.client = OpenAI(
            api_key=API_KEY,
            base_url=ENDPOINT
        )
        
        print("âœ… TASA Evaluatoråˆå§‹åŒ–å®Œæˆ")
    
    def load_dialogue(self, student_id: int, concept_text: str, dataset: str) -> List[Dict]:
        """åŠ è½½tutoring dialogueï¼Œæ ¹æ®TUTOR_MODELè‡ªåŠ¨é€‰æ‹©å¯¹åº”backboneçš„dialogue"""
        from tasa_config import TUTOR_MODEL, FORGETTING_SCORE_METHOD
        
        # æ ¹æ®TUTOR_MODELå†³å®šbackboneåç¼€
        if 'llama' in TUTOR_MODEL.lower():
            backbone_suffix = '-llama'
        elif 'qwen' in TUTOR_MODEL.lower():
            backbone_suffix = '-qwen'
        else:
            backbone_suffix = ''  # gpt-oss-120b
        
        dialogue_file = f"{DIALOGUE_DIR}{backbone_suffix}/{dataset}/{FORGETTING_SCORE_METHOD}/{student_id}-{concept_text}.json"
        
        with open(dialogue_file) as f:
            dialogue_data = json.load(f)
        
        return dialogue_data['dialogue']
    
    def extract_learned_knowledge(self, dialogue: List[Dict], concept_text: str) -> str:
        """
        ä»å¯¹è¯ä¸­æå–å­¦åˆ°çš„å…³é”®çŸ¥è¯†ç‚¹ï¼ˆè€Œä¸æ˜¯å®Œæ•´å¯¹è¯ï¼‰
        """
        # æå–æ‰€æœ‰Tutorçš„è®²è§£ï¼ˆè·³è¿‡ç¬¬ä¸€è½®ï¼Œå› ä¸ºé‚£åªæ˜¯é—®é¢˜ï¼‰
        tutor_explanations = []
        for msg in dialogue:
            if msg['role'] == 'assistant' and msg['round'] > 1:
                # æå–è®²è§£éƒ¨åˆ†ï¼ˆé€šå¸¸åœ¨é—®é¢˜ä¹‹å‰ï¼‰
                content = msg['content']
                # ç®€å•æˆªæ–­ï¼šåªå–å‰500å­—ç¬¦ä½œä¸ºå…³é”®è®²è§£
                explanation = content[:500] if len(content) > 500 else content
                tutor_explanations.append(explanation)
        
        # æ„å»ºå­¦ä¹ æ€»ç»“
        if tutor_explanations:
            # åªå–å‰3ä¸ªæœ€é‡è¦çš„è®²è§£
            key_explanations = "\n\n".join([f"- {exp}" for exp in tutor_explanations[:3]])
            return key_explanations
        else:
            return f"Key concepts about {concept_text}"
    
    def conduct_post_test(self, student_id: int, dataset: str, concept_text: str,
                         dialogue: List[Dict], questions: List[str],
                         student_system_prompt: str) -> Tuple[float, List[Dict]]:
        """
        è¿›è¡ŒPost-testè¯„ä¼°
        
        Args:
            dialogue: Tutoringå¯¹è¯å†å²
            questions: ç”¨äºæµ‹è¯•çš„é—®é¢˜åˆ—è¡¨
            student_system_prompt: å­¦ç”Ÿçš„åŸºç¡€system prompt
        
        Returns:
            post_test_accuracy: Post-testå‡†ç¡®ç‡
            answers: å­¦ç”Ÿçš„å›ç­”åˆ—è¡¨
        """
        print(f"\nğŸ“Š è¿›è¡ŒPost-testè¯„ä¼°")
        print(f"   é¢˜ç›®æ•°: {len(questions)}")
        
        # æå–å­¦åˆ°çš„å…³é”®çŸ¥è¯†
        learned_knowledge = self.extract_learned_knowledge(dialogue, concept_text)
        
        # å¢å¼ºå­¦ç”Ÿpromptï¼Œæ˜ç¡®è¯´æ˜å·²ç»å­¦ä¼šäº†è¿™äº›çŸ¥è¯†
        enhanced_prompt = f"""{student_system_prompt}

[IMPORTANT UPDATE: You Have Just Learned This Concept]

You have just completed a personalized tutoring session on {concept_text}. Through 10 rounds of practice and feedback, you have now MASTERED the following key concepts:

{learned_knowledge}

**YOU NOW UNDERSTAND THIS MATERIAL.** The tutoring has helped you overcome your previous difficulties. Your knowledge of {concept_text} has significantly improved.

When answering the following questions:
- Apply what you just learned from the tutoring session
- You should perform BETTER than before because you now understand the concepts
- Show your improved understanding and confidence
- Use the knowledge and corrections you received during tutoring"""
        
        # è®©å­¦ç”Ÿå›ç­”é—®é¢˜
        answers = []
        for i, question in enumerate(questions, 1):
            print(f"   é—®é¢˜ {i}/{len(questions)}", end='\r')
            
            try:
                response = self.client.chat.completions.create(
                    model=STUDENT_MODEL,
                    messages=[
                        {"role": "system", "content": enhanced_prompt},
                        {"role": "user", "content": question}
                    ],
                    temperature=STUDENT_TEMPERATURE,
                    max_tokens=MAX_TOKENS_STUDENT
                )
                
                content = response.choices[0].message.content
                answer = content.strip() if content else "I don't know."
                
            except Exception as e:
                print(f"\nâš ï¸ é—®é¢˜{i}å›ç­”å¤±è´¥: {e}")
                answer = "I don't know."
            
            answers.append({
                "question_number": i,
                "question": question,
                "student_answer": answer
            })
        
        print(f"\n   âœ… æ‰€æœ‰é—®é¢˜å·²å›ç­”")
        
        # æ‰¹æ”¹
        print(f"   ğŸ“ æ‰¹æ”¹ä¸­...")
        total_score, feedback, individual_scores = grade_answers(answers, concept_text)
        
        post_test_accuracy = total_score / len(questions)
        
        # å°†individual_scoresæ·»åŠ åˆ°answersä¸­
        for i, answer in enumerate(answers):
            answer['score'] = individual_scores[i]
        
        print(f"   âœ… Post-testå‡†ç¡®ç‡: {post_test_accuracy*100:.1f}%")
        
        return post_test_accuracy, answers
    
    def calculate_learning_gain(self, pre_test_accuracy: float, 
                               post_test_accuracy: float) -> float:
        """
        è®¡ç®—Learning Gain
        
        Formula: learning_gain = (post - pre) / (1 - pre)
        """
        if pre_test_accuracy >= 1.0:
            # å¦‚æœpre-testå·²ç»100%ï¼Œæ— æ³•å†æé«˜
            return 0.0
        
        learning_gain = (post_test_accuracy - pre_test_accuracy) / (1.0 - pre_test_accuracy)
        
        return learning_gain
    
    def load_pretest_result(self, student_id: int, dataset: str, concept_id: str) -> float:
        """ä»pre-testç»“æœä¸­åŠ è½½roleplayå‡†ç¡®ç‡"""
        pretest_file = f"{EVALUATION_DIR}/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json"
        
        try:
            with open(pretest_file) as f:
                pretest_data = json.load(f)
            return pretest_data['roleplay_accuracy']
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½pre-testç»“æœ: {e}")
            return None
    
    def evaluate_single_student(self, student_id: int, dataset: str, 
                               concept_text: str, concept_id: str, questions: List[str],
                               student_system_prompt: str) -> Dict:
        """
        è¯„ä¼°å•ä¸ªå­¦ç”Ÿçš„å®Œæ•´æµç¨‹
        
        Returns:
            evaluation_result: {
                "student_id": int,
                "dataset": str,
                "concept_text": str,
                "concept_id": str,
                "pre_test_accuracy": float,  # ä»pre-testç»“æœä¸­è¯»å–
                "post_test_accuracy": float,
                "learning_gain": float,
                "answers": List[Dict]
            }
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯„ä¼°å­¦ç”Ÿ {student_id} - {concept_text}")
        print(f"{'='*80}")
        
        # åŠ è½½pre-testçš„roleplayå‡†ç¡®ç‡
        pre_test_accuracy = self.load_pretest_result(student_id, dataset, concept_id)
        
        if pre_test_accuracy is None:
            print(f"âŒ æ— æ³•æ‰¾åˆ°pre-testç»“æœï¼Œæ— æ³•è®¡ç®—learning gain")
            return None
        
        print(f"âœ… Pre-testå‡†ç¡®ç‡ (æ— æ•™å­¦): {pre_test_accuracy*100:.1f}%")
        
        # åŠ è½½dialogue
        dialogue = self.load_dialogue(student_id, concept_text, dataset)
        print(f"âœ… å·²åŠ è½½å¯¹è¯å†å² ({len(dialogue)}æ¡æ¶ˆæ¯)")
        
        # Post-test
        post_test_accuracy, answers = self.conduct_post_test(
            student_id, dataset, concept_text,
            dialogue, questions, student_system_prompt
        )
        
        # è®¡ç®—Learning Gain
        learning_gain = self.calculate_learning_gain(pre_test_accuracy, post_test_accuracy)
        
        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   Pre-test (æ— æ•™å­¦):  {pre_test_accuracy*100:.1f}%")
        print(f"   Post-test (æœ‰æ•™å­¦): {post_test_accuracy*100:.1f}%")
        print(f"   ç»å¯¹æå‡: {(post_test_accuracy - pre_test_accuracy)*100:+.1f}%")
        print(f"   Learning Gain: {learning_gain:.3f}")
        
        result = {
            "student_id": student_id,
            "dataset": dataset,
            "concept_text": concept_text,
            "concept_id": concept_id,
            "pre_test_accuracy": pre_test_accuracy,
            "post_test_accuracy": post_test_accuracy,
            "learning_gain": learning_gain,
            "improvement": post_test_accuracy - pre_test_accuracy,
            "answers": answers
        }
        
        return result
    
    def save_evaluation_result(self, result: Dict, method: str = "TASA"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        dataset = result['dataset']
        student_id = result['student_id']
        concept_text = result['concept_text']
        
        # åˆ›å»ºç›®å½•
        eval_dir = f"{EVALUATION_DIR}/{method}/{dataset}"
        os.makedirs(eval_dir, exist_ok=True)
        
        # ä¿å­˜
        filename = f"{eval_dir}/student_{student_id}_{concept_text}.json"
        
        with open(filename, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {filename}")
        
        return filename

# æµ‹è¯•
if __name__ == "__main__":
    from student_roleplay_evaluation import build_student_system_prompt, load_session
    from tasa_tutoring import TASATutor
    import json
    
    # å…ˆè¿›è¡Œtutoring
    print("="*80)
    print("Step 1: è¿›è¡ŒTutoring")
    print("="*80)
    
    tutor = TASATutor()
    session = load_session('/mnt/localssd/bank/session/assist2017/1.json')
    student_prompt = build_student_system_prompt(session)
    
    # æ¨¡æ‹Ÿpre-testå‡†ç¡®ç‡
    pre_test_accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total']
    
    # Tutoring
    dialogue = tutor.conduct_tutoring_session(
        student_id=1,
        dataset="assist2017",
        concept_text=session['concept_text'],
        student_system_prompt=student_prompt
    )
    
    tutor.save_dialogue(dialogue, 1, session['concept_text'], "assist2017")
    
    # è¯„ä¼°
    print("\n" + "="*80)
    print("Step 2: è¿›è¡ŒPost-testè¯„ä¼°")
    print("="*80)
    
    evaluator = TASAEvaluator()
    
    # åŠ è½½æµ‹è¯•é¢˜ç›®
    questions_file = '/mnt/localssd/bank/test_data/assist2017/concept_questions.json'
    with open(questions_file) as f:
        all_questions = json.load(f)
    
    concept_id = str(session['concept_id'])
    questions = all_questions[concept_id]['questions']
    
    # è¯„ä¼°
    result = evaluator.evaluate_single_student(
        student_id=1,
        dataset="assist2017",
        concept_text=session['concept_text'],
        questions=questions,
        pre_test_accuracy=pre_test_accuracy,
        student_system_prompt=student_prompt
    )
    
    # ä¿å­˜
    evaluator.save_evaluation_result(result, method="TASA")

