#!/usr/bin/env python
"""
æ­£ç¡®ä½¿ç”¨è®­ç»ƒå¥½çš„KTæ¨¡å‹è¿›è¡Œè¯„ä¼°å’Œé¢„æµ‹

å±•ç¤ºï¼š
1. å¦‚ä½•åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
2. å¦‚ä½•åœ¨test setä¸Šè¯„ä¼°æ€§èƒ½ï¼ˆAUC/ACCï¼‰
3. å¦‚ä½•ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
"""

import os
import sys
import torch
import json
import pandas as pd
import numpy as np
from sklearn import metrics

# æ·»åŠ PyKTè·¯å¾„
sys.path.insert(0, '/mnt/localssd/pykt-toolkit')
from pykt.models.init_model import load_model as pykt_load_model

def find_model_checkpoint(dataset, model_name):
    """æŸ¥æ‰¾æ¨¡å‹checkpoint"""
    base_dir = '/mnt/localssd/pykt-toolkit/examples/saved_model'
    
    # æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•
    for dirname in os.listdir(base_dir):
        if dirname.startswith(f"{dataset}_{model_name}_"):
            model_dir = os.path.join(base_dir, dirname)
            
            # æŸ¥æ‰¾checkpointæ–‡ä»¶
            for ckpt_name in ['qid_model.ckpt', 'model.ckpt', 'best_model.ckpt']:
                ckpt_path = os.path.join(model_dir, ckpt_name)
                if os.path.exists(ckpt_path):
                    config_path = os.path.join(model_dir, 'config.json')
                    if os.path.exists(config_path):
                        return model_dir, ckpt_path, config_path
    
    return None, None, None

def load_trained_model(dataset, model_name, device='cpu'):
    """
    æ­£ç¡®åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    è¿™æ˜¯ä½¿ç”¨PyKTçš„æ ‡å‡†æ–¹å¼
    """
    model_dir, ckpt_path, config_path = find_model_checkpoint(dataset, model_name)
    
    if not model_dir:
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹: {dataset}_{model_name}")
        return None, None
    
    print(f"ğŸ“‚ æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"ğŸ“¦ Checkpoint: {os.path.basename(ckpt_path)}")
    
    # è¯»å–é…ç½®
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    data_config = config['data_config']
    emb_type = config.get('params', {}).get('emb_type', 'qid')
    
    # ä½¿ç”¨PyKTçš„æ ‡å‡†åŠ è½½æ–¹å¼
    model = pykt_load_model(
        model_name=model_name,
        model_config=config['model_config'],
        data_config=data_config,
        emb_type=emb_type,
        ckpt_path=model_dir
    )
    
    model = model.to(device)
    model.eval()
    
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹")
    print(f"   num_q={data_config['num_q']}, num_c={data_config['num_c']}")
    
    return model, config

def evaluate_on_processed_data(dataset, model_name):
    """
    æ–¹æ³•1: ä½¿ç”¨PyKTé¢„å¤„ç†å¥½çš„æ•°æ®è¿›è¡Œè¯„ä¼°
    
    è¿™æ˜¯æœ€æ ‡å‡†ã€æœ€æ­£ç¡®çš„æ–¹å¼
    """
    print(f"\n{'='*80}")
    print(f"è¯„ä¼°: {dataset.upper()} - {model_name.upper()}")
    print(f"{'='*80}")
    
    # åŠ è½½æ¨¡å‹
    model, config = load_trained_model(dataset, model_name)
    
    if model is None:
        return
    
    # ä½¿ç”¨PyKTé¢„å¤„ç†çš„testæ•°æ®
    data_path = f'/mnt/localssd/pykt-toolkit/data/{dataset}/test_sequences.csv'
    
    if not os.path.exists(data_path):
        print(f"âš ï¸  æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {data_path}")
        return
    
    print(f"\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {data_path}")
    
    try:
        df = pd.read_csv(data_path)
        print(f"   æµ‹è¯•é›†å¤§å°: {len(df)} ä¸ªåºåˆ—")
        
        # æ–¹æ³•1: è¯»å–å·²æœ‰çš„è¯„ä¼°ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        model_dir = os.path.dirname(find_model_checkpoint(dataset, model_name)[1])
        results_files = ['test_results.txt', 'test_metrics.json']
        
        for results_file in results_files:
            results_path = os.path.join(model_dir, results_file)
            if os.path.exists(results_path):
                print(f"\nâœ… æ‰¾åˆ°è¯„ä¼°ç»“æœ: {results_file}")
                with open(results_path, 'r') as f:
                    print(f.read())
                return
        
        print(f"\nâš ï¸  æœªæ‰¾åˆ°é¢„è®¡ç®—çš„è¯„ä¼°ç»“æœ")
        print(f"ğŸ’¡ æç¤º: è¿è¡Œä»¥ä¸‹å‘½ä»¤é‡æ–°è¯„ä¼°:")
        print(f"   cd /mnt/localssd/pykt-toolkit/examples")
        print(f"   python wandb_{model_name}_train.py \\")
        print(f"       --dataset_name={dataset} \\")
        print(f"       --fold=0 \\")
        print(f"       --use_wandb=0")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def demonstrate_prediction(dataset, model_name):
    """
    æ–¹æ³•2: æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ¨¡å‹è¿›è¡Œå•ä¸ªé¢„æµ‹
    
    æ³¨æ„: è¿™éœ€è¦æ­£ç¡®çš„æ•°æ®é¢„å¤„ç†
    """
    print(f"\n{'='*80}")
    print(f"æ¼”ç¤ºé¢„æµ‹: {dataset.upper()} - {model_name.upper()}")
    print(f"{'='*80}")
    
    model, config = load_trained_model(dataset, model_name)
    
    if model is None:
        return
    
    print(f"\nğŸ’¡ é¢„æµ‹ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰:")
    print(f"   æ³¨æ„: å®é™…ä½¿ç”¨æ—¶éœ€è¦PyKTçš„DataLoaderè¿›è¡Œæ­£ç¡®çš„æ•°æ®é¢„å¤„ç†")
    
    # ä»test setè¯»å–ä¸€ä¸ªæ ·æœ¬
    data_path = f'/mnt/localssd/pykt-toolkit/data/{dataset}/test_sequences.csv'
    
    if not os.path.exists(data_path):
        print(f"âš ï¸  æµ‹è¯•æ•°æ®ä¸å­˜åœ¨")
        return
    
    df = pd.read_csv(data_path)
    sample = df.iloc[0]
    
    print(f"\n   å­¦ç”ŸID: {sample['uid']}")
    print(f"   åºåˆ—é•¿åº¦: {len(str(sample['questions']).split(','))}")
    print(f"\n   ğŸ’¡ å®é™…é¢„æµ‹éœ€è¦:")
    print(f"      1. ä½¿ç”¨PyKTçš„DataLoader")
    print(f"      2. æ­£ç¡®çš„æ•°æ®é¢„å¤„ç†ï¼ˆIDæ˜ å°„ã€paddingç­‰ï¼‰")
    print(f"      3. æ¨¡å‹forwardçš„æ­£ç¡®è¾“å…¥æ ¼å¼")

def show_model_comparison():
    """
    æ–¹æ³•3: å±•ç¤ºæ‰€æœ‰æ¨¡å‹çš„å¯¹æ¯”
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print(f"{'='*80}\n")
    
    datasets = ['assist2017', 'ednet', 'algebra2005', 'bridge2algebra2006']
    models = ['lpkt', 'dkt', 'akt', 'simplekt']
    
    results = []
    
    for dataset in datasets:
        for model_name in models:
            model_dir, _, _ = find_model_checkpoint(dataset, model_name)
            
            if model_dir:
                status = "âœ… å·²è®­ç»ƒ"
                
                # å°è¯•è¯»å–è¯„ä¼°ç»“æœ
                metrics_file = os.path.join(model_dir, 'test_results.txt')
                auc, acc = "N/A", "N/A"
                
                if os.path.exists(metrics_file):
                    try:
                        with open(metrics_file, 'r') as f:
                            content = f.read()
                            # ç®€å•è§£æ
                            if 'AUC' in content:
                                auc = "æœ‰ç»“æœ"
                    except:
                        pass
                
                results.append({
                    'Dataset': dataset,
                    'Model': model_name.upper(),
                    'Status': status,
                    'Metrics': auc
                })
            else:
                results.append({
                    'Dataset': dataset,
                    'Model': model_name.upper(),
                    'Status': "âŒ æœªæ‰¾åˆ°",
                    'Metrics': "N/A"
                })
    
    # æ‰“å°è¡¨æ ¼
    df = pd.DataFrame(results)
    print(df.to_string(index=False))
    
    print(f"\nğŸ’¡ å¦‚ä½•ä½¿ç”¨è¿™äº›æ¨¡å‹:")
    print(f"   1. æ€§èƒ½è¯„ä¼°: æŸ¥çœ‹test setçš„AUC/ACC")
    print(f"   2. åœ¨çº¿é¢„æµ‹: ä½¿ç”¨PyKTçš„DataLoader + model.forward()")
    print(f"   3. æ¨¡å‹å¯¹æ¯”: æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„è¡¨ç°")

if __name__ == '__main__':
    print("="*80)
    print("ğŸš€ å¦‚ä½•æ­£ç¡®ä½¿ç”¨è®­ç»ƒå¥½çš„KTæ¨¡å‹")
    print("="*80)
    
    # 1. å±•ç¤ºæ¨¡å‹å¯¹æ¯”
    show_model_comparison()
    
    # 2. æ¼”ç¤ºè¯„ä¼°ï¼ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼‰
    print(f"\n")
    evaluate_on_processed_data('assist2017', 'lpkt')
    
    # 3. æ¼”ç¤ºé¢„æµ‹
    demonstrate_prediction('assist2017', 'lpkt')
    
    print(f"\n{'='*80}")
    print(f"âœ… å®Œæˆï¼")
    print(f"{'='*80}")
    
    print(f"\nğŸ“– æ›´å¤šä¿¡æ¯:")
    print(f"   - è¯¦ç»†æŒ‡å—: /mnt/localssd/HOW_TO_USE_TRAINED_MODELS.md")
    print(f"   - PyKTæ–‡æ¡£: /mnt/localssd/pykt-toolkit/README.md")

