#!/usr/bin/env python3
"""
LLM as Judge: ä»…è¯„ä¼°TASA-llamaç³»åˆ—æ–¹æ³•
"""

import sys
import os

# å¯¼å…¥ä¸»è¯„ä¼°æ¨¡å—çš„å‡½æ•°
sys.path.insert(0, '/mnt/localssd')
from llm_as_judge_personalization import batch_judge, safe_print

def main():
    """ä»…è¯„ä¼°TASA-llamaç³»åˆ—æ–¹æ³•"""
    
    # åªè¯„ä¼°TASA-llamaç›¸å…³æ–¹æ³•
    TARGET_METHODS = [
        'TASA-llama',  # llamaä¸»æ–¹æ³•
        'TASA-woForgetting-llama',  # æ¶ˆèå®éªŒ
        'TASA-woMemory-llama',
        'TASA-woPersona-llama',
        'TASA-lambda0.5-llama',  # lambdaå‚æ•°
        'TASA-lambda0.5-gpt',
        'TASA-lambda0.5-qwen',
    ]
    
    DATASETS = ['assist2017', 'nips_task34', 'algebra2005', 'bridge2006']
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘          ğŸ¯ LLM as Judge: TASA Methods Evaluation (Supplementary)          â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print("ğŸ“‹ é…ç½®:")
    print(f"  â€¢ Target Methods: {len(TARGET_METHODS)}")
    print(f"  â€¢ Datasets: {', '.join(DATASETS)}")
    print(f"  â€¢ Max Workers: 20")
    print()
    print("="*80)
    
    all_summaries = []
    
    for dataset in DATASETS:
        print(f"\n{'#'*80}")
        print(f"## Dataset: {dataset}")
        print(f"{'#'*80}\n")
        
        for method in TARGET_METHODS:
            # æ£€æŸ¥è¯¥methodåœ¨è¯¥datasetä¸Šæ˜¯å¦æœ‰dialogue
            method_dir = f'/mnt/localssd/bank/dialogue/{method}/{dataset}'
            if not os.path.exists(method_dir):
                safe_print(f"â­ï¸  è·³è¿‡{method}ï¼ˆ{dataset}æ— æ•°æ®ï¼‰\n")
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¯„ä¼°è¿‡
            backbone_suffix = ''
            if '-llama' in method:
                baseline_method = 'Vanilla-ICL-llama'
                backbone = 'llama'
            elif '-qwen' in method:
                baseline_method = 'Vanilla-ICL-qwen'
                backbone = 'qwen'
            else:
                baseline_method = 'Vanilla-ICL'
                backbone = 'gpt'
            
            result_file = f'/mnt/localssd/llm_judge_results/{method}_vs_{baseline_method}_{dataset}.json'
            if os.path.exists(result_file):
                safe_print(f"âœ… è·³è¿‡{method}ï¼ˆ{dataset}å·²è¯„ä¼°ï¼‰\n")
                continue
            
            summary = batch_judge(method, dataset, max_workers=20)
            if summary:
                all_summaries.append(summary)
    
    # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
    print("\n" + "="*100)
    print("ğŸ“Š TASAç³»åˆ—è¯„ä¼°å®Œæˆæ±‡æ€»")
    print("="*100)
    
    if all_summaries:
        print(f"\n{'Method':<40} {'Dataset':<15} {'Win Rate':<12} {'Record':<20}")
        print('-'*87)
        for s in all_summaries:
            wr_str = f"{s['win_rate']*100:.1f}%"
            record = f"{s['target_wins']}W-{s['ties']}T-{s['baseline_wins']}L ({s['total_comparisons']})"
            print(f"{s['target_method']:<40} {s['dataset']:<15} {wr_str:<12} {record:<20}")
    else:
        print("æ‰€æœ‰TASAæ–¹æ³•å‡å·²è¯„ä¼°å®Œæˆï¼")
    
    print("\n" + "="*100)
    print("âœ… TASAç³»åˆ—è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: /mnt/localssd/llm_judge_results/")
    print("="*100 + "\n")

if __name__ == '__main__':
    main()

