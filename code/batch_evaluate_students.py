#!/usr/bin/env python3
"""
æ‰¹é‡è¯„ä¼°å­¦ç”ŸRole-Playç³»ç»Ÿ
"""

import json
import argparse
from pathlib import Path
from student_roleplay_evaluation import (
    load_concept_questions, 
    evaluate_session, 
    client
)
from tqdm import tqdm
import pandas as pd

def batch_evaluate_dataset(dataset_name: str, num_students: int = None, sample_mode: str = 'first'):
    """
    æ‰¹é‡è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§° (assist2017, algebra2005, etc.)
        num_students: è¦è¯„ä¼°çš„å­¦ç”Ÿæ•°é‡ï¼ˆNone = å…¨éƒ¨ï¼‰
        sample_mode: 'first', 'random', 'range'
    """
    print(f"\n{'='*80}")
    print(f"æ‰¹é‡è¯„ä¼°æ•°æ®é›†: {dataset_name}")
    print(f"{'='*80}\n")
    
    # è·¯å¾„è®¾ç½®
    session_dir = Path(f'/mnt/localssd/bank/session/{dataset_name}')
    concept_questions_file = f'/mnt/localssd/bank/test_data/{dataset_name}/concept_questions.json'
    output_dir = f'/mnt/localssd/bank/evaluation_results/{dataset_name}'
    
    # åŠ è½½é¢˜åº“
    print(f"ğŸ“– åŠ è½½é¢˜åº“: {concept_questions_file}")
    try:
        concept_questions = load_concept_questions(concept_questions_file)
        print(f"   âœ… åŠ è½½äº† {len(concept_questions)} ä¸ªconcepts")
    except FileNotFoundError:
        print(f"   âŒ é¢˜åº“æ–‡ä»¶ä¸å­˜åœ¨: {concept_questions_file}")
        return
    
    # è·å–æ‰€æœ‰sessionæ–‡ä»¶
    session_files = sorted(list(session_dir.glob('*.json')))
    print(f"\nğŸ“‚ æ‰¾åˆ° {len(session_files)} ä¸ªsessionæ–‡ä»¶")
    
    if not session_files:
        print("   âŒ æ²¡æœ‰æ‰¾åˆ°sessionæ–‡ä»¶")
        return
    
    # é‡‡æ ·
    if num_students:
        if sample_mode == 'first':
            session_files = session_files[:num_students]
        elif sample_mode == 'random':
            import random
            session_files = random.sample(session_files, min(num_students, len(session_files)))
        print(f"   ğŸ¯ é€‰æ‹©äº† {len(session_files)} ä¸ªsessionè¿›è¡Œè¯„ä¼°")
    
    # æ‰¹é‡è¯„ä¼°
    results = []
    failed = []
    
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°...\n")
    
    for session_file in tqdm(session_files, desc="è¯„ä¼°è¿›åº¦", ncols=100):
        try:
            result = evaluate_session(str(session_file), concept_questions, output_dir)
            if result:
                results.append(result)
            else:
                failed.append(session_file.name)
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°å¤±è´¥ {session_file.name}: {e}")
            failed.append(session_file.name)
            continue
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    generate_summary_report(results, dataset_name, output_dir, failed)

def generate_summary_report(results: list, dataset_name: str, output_dir: str, failed: list):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    print(f"\n{'='*80}")
    print(f"è¯„ä¼°æ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*80}\n")
    
    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„è¯„ä¼°ç»“æœ")
        return
    
    # è½¬æ¢ä¸ºDataFrame
    df_data = []
    for r in results:
        df_data.append({
            'student_id': r['student_id'],
            'concept_id': r['concept_id'],
            'concept_text': r['concept_text'],
            'original_accuracy': r['original_accuracy'],
            'roleplay_score': r['roleplay_score'],
            'roleplay_accuracy': r['roleplay_score'] / 10,
            'delta_t_minutes': r['session_info']['delta_t_minutes'],
            'num_attempts': r['session_info']['num_attempts']
        })
    
    df = pd.DataFrame(df_data)
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
    print(f"   æ€»è¯„ä¼°æ•°: {len(results)}")
    print(f"   æˆåŠŸ: {len(results)}")
    print(f"   å¤±è´¥: {len(failed)}")
    print()
    
    print(f"ğŸ“ˆ å‡†ç¡®ç‡åˆ†æ:")
    print(f"   åŸå§‹å¹³å‡å‡†ç¡®ç‡: {df['original_accuracy'].mean()*100:.2f}%")
    print(f"   Role-playå¹³å‡å‡†ç¡®ç‡: {df['roleplay_accuracy'].mean()*100:.2f}%")
    print(f"   å‡†ç¡®ç‡ç›¸å…³æ€§: {df['original_accuracy'].corr(df['roleplay_accuracy']):.3f}")
    print()
    
    # æŒ‰åŸå§‹å‡†ç¡®ç‡åˆ†ç»„
    print(f"ğŸ“Š æŒ‰åŸå§‹å‡†ç¡®ç‡åˆ†ç»„çš„Role-playè¡¨ç°:")
    df['accuracy_group'] = pd.cut(df['original_accuracy'], 
                                   bins=[0, 0.3, 0.6, 1.0], 
                                   labels=['Low (<30%)', 'Medium (30-60%)', 'High (>60%)'])
    
    for group in ['Low (<30%)', 'Medium (30-60%)', 'High (>60%)']:
        group_df = df[df['accuracy_group'] == group]
        if len(group_df) > 0:
            print(f"   {group}: {len(group_df)} students")
            print(f"      åŸå§‹: {group_df['original_accuracy'].mean()*100:.1f}%")
            print(f"      Role-play: {group_df['roleplay_accuracy'].mean()*100:.1f}%")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜DataFrame
    csv_file = output_path / 'summary_report.csv'
    df.to_csv(csv_file, index=False)
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {csv_file}")
    
    # ä¿å­˜å®Œæ•´ç»“æœJSON
    json_file = output_path / 'all_results.json'
    with open(json_file, 'w') as f:
        json.dump({
            'dataset': dataset_name,
            'total_evaluated': len(results),
            'failed': failed,
            'results': results,
            'statistics': {
                'original_accuracy_mean': float(df['original_accuracy'].mean()),
                'roleplay_accuracy_mean': float(df['roleplay_accuracy'].mean()),
                'correlation': float(df['original_accuracy'].corr(df['roleplay_accuracy']))
            }
        }, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {json_file}")
    
    # å¤±è´¥åˆ—è¡¨
    if failed:
        print(f"\nâš ï¸  å¤±è´¥çš„è¯„ä¼° ({len(failed)}):")
        for f in failed[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {f}")
        if len(failed) > 10:
            print(f"   ... è¿˜æœ‰ {len(failed)-10} ä¸ª")

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡è¯„ä¼°å­¦ç”ŸRole-Playç³»ç»Ÿ')
    parser.add_argument('--dataset', type=str, required=True, 
                       help='æ•°æ®é›†åç§° (assist2017, algebra2005, etc.)')
    parser.add_argument('--num', type=int, default=None,
                       help='è¦è¯„ä¼°çš„å­¦ç”Ÿæ•°é‡ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰')
    parser.add_argument('--sample', type=str, default='first',
                       choices=['first', 'random'],
                       help='é‡‡æ ·æ¨¡å¼')
    
    args = parser.parse_args()
    
    batch_evaluate_dataset(args.dataset, args.num, args.sample)

if __name__ == '__main__':
    main()

