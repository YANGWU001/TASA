#!/usr/bin/env python3
"""
ä¸º Vanilla-ICL-turns28-llama è¿›è¡Œæ¯4è½®çš„evaluation
ä¸ TutorLLM çš„evaluationæ ¼å¼ä¸€è‡´
"""

import json
import os
import sys
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# è®¾ç½®ç¯å¢ƒ
os.environ['TASA_CONFIG'] = 'llama'

from tasa_config_llama import FORGETTING_SCORE_METHOD
from tasa_evaluation import TASAEvaluator

# Thread-safe print
print_lock = threading.Lock()

def safe_print(msg):
    with print_lock:
        print(msg)
        sys.stdout.flush()

def load_dialogue_from_file(dialogue_file, num_turns):
    """ä»æ–‡ä»¶åŠ è½½dialogueå¹¶æˆªå–åˆ°æŒ‡å®šè½®æ•°"""
    try:
        with open(dialogue_file, 'r') as f:
            dialogue = json.load(f)
        
        # æˆªå–åˆ°æŒ‡å®šè½®æ•°
        # roundå€¼ä»0å¼€å§‹ï¼Œnum_turns=4è¡¨ç¤ºåˆ°round 2ï¼ˆ0, 1, 2ä¸‰ä¸ªroundï¼Œå…±4-6ä¸ªturnsï¼‰
        max_round = num_turns // 2
        truncated = [d for d in dialogue if d.get('round', 0) <= max_round]
        
        return truncated
    except Exception as e:
        safe_print(f"   âŒ åŠ è½½dialogueå¤±è´¥: {e}")
        return None

def evaluate_single_student(student_id, concept_text, dataset, num_dialogue_turns):
    """è¯„ä¼°å•ä¸ªå­¦ç”Ÿ"""
    try:
        # åŠ è½½dialogue
        dialogue_file = f'/mnt/localssd/bank/dialogue/Vanilla-ICL-turns28-llama/{dataset}/dkt/{student_id}-{concept_text}.json'
        
        if not os.path.exists(dialogue_file):
            safe_print(f"   âš ï¸  å­¦ç”Ÿ{student_id}çš„dialogueä¸å­˜åœ¨")
            return None
        
        dialogue = load_dialogue_from_file(dialogue_file, num_dialogue_turns)
        if dialogue is None:
            return None
        
        safe_print(f"   ğŸ“ å­¦ç”Ÿ{student_id} | {len(dialogue)} turns")
        
        # åˆå§‹åŒ–evaluator
        evaluator = TASAEvaluator()
        
        # åŠ è½½sessionè·å–concept_idå’Œpersona
        from student_roleplay_evaluation import load_session, build_student_system_prompt
        
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        if not os.path.exists(session_file):
            safe_print(f"   âŒ Sessionæ–‡ä»¶ä¸å­˜åœ¨: {session_file}")
            return None
        
        session = load_session(session_file)
        concept_id = session['concept_id']
        
        # æ„å»ºstudent prompt
        student_system_prompt = build_student_system_prompt(session)
        
        # åŠ è½½post-testé—®é¢˜
        questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
        with open(questions_file) as f:
            all_questions = json.load(f)
        questions = all_questions[str(concept_id)]['questions']
        
        # Pre-test score - ä»pretestæ–‡ä»¶è¯»å–
        pretest_file = f'/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json'
        if not os.path.exists(pretest_file):
            safe_print(f"   âŒ Pre-testæ–‡ä»¶ä¸å­˜åœ¨: {pretest_file}")
            return None
        
        with open(pretest_file) as f:
            pretest_data = json.load(f)
        pre_test_score = pretest_data['roleplay_accuracy']
        safe_print(f"   ğŸ“Š Pre-test: {pre_test_score*100:.1f}%")
        
        # Post-test (è¿è¡Œ2æ¬¡)
        safe_print(f"   ğŸ“Š Post-test (2æ¬¡)...")
        post_test_results = []
        
        for run_id in range(1, 3):
            post_acc, _ = evaluator.conduct_post_test(
                student_id, dataset, concept_text,
                dialogue, questions, student_system_prompt
            )
            post_test_results.append(post_acc)
            safe_print(f"   Run {run_id}: {post_acc*100:.1f}%")
        
        # è®¡ç®—ä¸‰ç§ç­–ç•¥
        max_post_test = max(post_test_results)
        avg_post_test = np.mean(post_test_results)
        min_post_test = min(post_test_results)
        
        # è®¡ç®—learning gain
        if pre_test_score >= 1.0:
            learning_gain_max = 0.0
            learning_gain_avg = 0.0
            learning_gain_min = 0.0
        else:
            learning_gain_max = (max_post_test - pre_test_score) / (1.0 - pre_test_score)
            learning_gain_avg = (avg_post_test - pre_test_score) / (1.0 - pre_test_score)
            learning_gain_min = (min_post_test - pre_test_score) / (1.0 - pre_test_score)
        
        result = {
            'student_id': student_id,
            'pre_test_score': pre_test_score,
            'dialogue_turns': num_dialogue_turns,
            'method': 'Vanilla-ICL',
            'post_test_run1': post_test_results[0],
            'post_test_run2': post_test_results[1],
            # Bestç­–ç•¥
            'max_post_test_accuracy': max_post_test,
            'learning_gain_max': learning_gain_max,
            # Averageç­–ç•¥
            'avg_post_test_accuracy': avg_post_test,
            'learning_gain_avg': learning_gain_avg,
            # Worstç­–ç•¥
            'min_post_test_accuracy': min_post_test,
            'learning_gain_min': learning_gain_min
        }
        
        safe_print(f"   âœ… å­¦ç”Ÿ{student_id}å®Œæˆ | Gain(Best): {learning_gain_max*100:.1f}%")
        return result
        
    except Exception as e:
        safe_print(f"   âŒ å­¦ç”Ÿ{student_id}å¤±è´¥: {e}")
        import traceback
        safe_print(f"   {traceback.format_exc()}")
        return None

def batch_evaluate_turns(num_dialogue_turns, dataset='assist2017', max_workers=5):
    """æ‰¹é‡è¯„ä¼°æŒ‡å®šè½®æ•°"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è¯„ä¼° Vanilla-ICL-turns{num_dialogue_turns}-llama")
    print(f"{'='*80}")
    
    # åŠ è½½å­¦ç”Ÿåˆ—è¡¨
    students_file = f'/mnt/localssd/qualified_students_{dataset}_sampled10.json'
    with open(students_file, 'r') as f:
        data = json.load(f)
        students = data.get('sampled_students', data.get('students', []))
    
    # æ„å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    for student_info in students:
        if isinstance(student_info, dict):
            student_id = student_info['student_id']
            concept = student_info['target_concept']
        else:
            student_id = student_info
            # ä»dialogueæ–‡ä»¶åæ¨æ–­concept
            dialogue_dir = f'/mnt/localssd/bank/dialogue/Vanilla-ICL-turns28-llama/{dataset}/dkt'
            files = [f for f in os.listdir(dialogue_dir) if f.startswith(f'{student_id}-')]
            if files:
                concept = files[0].replace(f'{student_id}-', '').replace('.json', '')
            else:
                safe_print(f"âš ï¸  å­¦ç”Ÿ{student_id}æ‰¾ä¸åˆ°dialogue")
                continue
        
        tasks.append((student_id, concept))
    
    print(f"å­¦ç”Ÿæ•°: {len(tasks)}")
    print(f"Max workers: {max_workers}")
    print()
    
    # å¹¶è¡Œè¯„ä¼°
    all_results = []
    successful_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(evaluate_single_student, sid, concept, dataset, num_dialogue_turns): (sid, concept)
            for sid, concept in tasks
        }
        
        for future in as_completed(futures):
            sid, concept = futures[future]
            try:
                result = future.result()
                if result:
                    all_results.append(result)
                    successful_count += 1
            except Exception as e:
                safe_print(f"âŒ å­¦ç”Ÿ{sid}å¼‚å¸¸: {e}")
    
    # ç»Ÿè®¡
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ•´ä½“ç»Ÿè®¡ (Vanilla-ICL, {num_dialogue_turns} turns):")
    print(f"{'='*80}")
    
    if all_results:
        learning_gains_max = [r['learning_gain_max'] for r in all_results]
        learning_gains_avg = [r['learning_gain_avg'] for r in all_results]
        learning_gains_min = [r['learning_gain_min'] for r in all_results]
        
        print(f"\n   Bestç­–ç•¥   å¹³å‡Learning Gain: {np.mean(learning_gains_max)*100:.1f}% Â± {np.std(learning_gains_max)*100:.1f}%")
        print(f"   Averageç­–ç•¥ å¹³å‡Learning Gain: {np.mean(learning_gains_avg)*100:.1f}% Â± {np.std(learning_gains_avg)*100:.1f}%")
        print(f"   Worstç­–ç•¥  å¹³å‡Learning Gain: {np.mean(learning_gains_min)*100:.1f}% Â± {np.std(learning_gains_min)*100:.1f}%")
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼")
    print(f"   æˆåŠŸ: {successful_count}/{len(tasks)}")
    print(f"{'='*80}\n")
    
    # ä¿å­˜ç»“æœ
    result_dir = f'/mnt/localssd/bank/evaluation_results/Vanilla-ICL-turns{num_dialogue_turns}-llama/{dataset}/dkt'
    os.makedirs(result_dir, exist_ok=True)
    
    overall_result = {
        'dataset': dataset,
        'method': 'Vanilla-ICL',
        'dialogue_turns': num_dialogue_turns,
        'num_students': len(tasks),
        # Bestç­–ç•¥
        'strategy_max': {
            'avg_learning_gain': float(np.mean(learning_gains_max)) if all_results else 0.0,
            'std_learning_gain': float(np.std(learning_gains_max)) if all_results else 0.0,
            'median_learning_gain': float(np.median(learning_gains_max)) if all_results else 0.0
        },
        # Averageç­–ç•¥
        'strategy_avg': {
            'avg_learning_gain': float(np.mean(learning_gains_avg)) if all_results else 0.0,
            'std_learning_gain': float(np.std(learning_gains_avg)) if all_results else 0.0,
            'median_learning_gain': float(np.median(learning_gains_avg)) if all_results else 0.0
        },
        # Worstç­–ç•¥
        'strategy_min': {
            'avg_learning_gain': float(np.mean(learning_gains_min)) if all_results else 0.0,
            'std_learning_gain': float(np.std(learning_gains_min)) if all_results else 0.0,
            'median_learning_gain': float(np.median(learning_gains_min)) if all_results else 0.0
        },
        'students': all_results
    }
    
    with open(f'{result_dir}/overall.json', 'w') as f:
        json.dump(overall_result, f, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_dir}/overall.json\n")
    
    return overall_result

def main():
    DIALOGUE_TURNS = [0, 4, 8, 12, 16, 20, 24, 28]
    DATASET = 'assist2017'
    MAX_WORKERS = 5  # æ¯ä¸ªturnsè¯„ä¼°ä½¿ç”¨5ä¸ªworker
    
    print('='*80)
    print('ğŸ”¬ Vanilla-ICL Dialogue Turns Ablation Evaluation')
    print('='*80)
    print(f'Dialogue turns: {DIALOGUE_TURNS}')
    print(f'Dataset: {DATASET}')
    print(f'Max workers: {MAX_WORKERS}')
    print('='*80)
    print()
    
    all_results = {}
    
    for turns in DIALOGUE_TURNS:
        result = batch_evaluate_turns(turns, DATASET, MAX_WORKERS)
        if result:
            all_results[str(turns)] = result['strategy_max']['avg_learning_gain']
    
    # æ‰“å°æ±‡æ€»
    print('\n' + '='*80)
    print('ğŸ“Š æ‰€æœ‰Turnsçš„Learning Gainæ±‡æ€»ï¼ˆBestç­–ç•¥ï¼‰')
    print('='*80)
    for turns in DIALOGUE_TURNS:
        if str(turns) in all_results:
            lg = all_results[str(turns)] * 100
            print(f'  {turns:2d} turns: {lg:6.1f}%')
    print('='*80)

if __name__ == '__main__':
    main()

