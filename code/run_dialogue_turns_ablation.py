#!/usr/bin/env python3
"""
Dialogue Turns Ablation Study
æ¯”è¾ƒTASAã€Vanilla-ICLã€TutorLLMåœ¨ä¸åŒdialogueè½®æ•°ä¸‹çš„è¡¨ç°

Dialogue turns: [0, 4, 8, 12, 16, 20, 24, 28]
Dataset: Assist2017
Backbone: Llama-3.1-8B
"""

import json
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import numpy as np

from student_roleplay_evaluation import build_student_system_prompt, load_session
from tasa_tutoring import TASATutor
from baseline_vanilla_icl import VanillaICLTutor
from baseline_tutorllm import TutorLLM
from tasa_evaluation import TASAEvaluator
from openai import OpenAI
from tasa_config_llama import STUDENT_MODEL

# å…¨å±€é”
print_lock = Lock()
model_init_lock = Lock()

# çº¿ç¨‹æœ¬åœ°å­˜å‚¨
import threading
thread_local = threading.local()

def safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°"""
    with print_lock:
        print(*args, **kwargs)

def get_tutor(method, num_rounds):
    """è·å–çº¿ç¨‹æœ¬åœ°çš„Tutorå®ä¾‹"""
    tutor_key = f'tutor_{method}_rounds{num_rounds}'
    if not hasattr(thread_local, tutor_key):
        with model_init_lock:
            safe_print(f"   [Thread-{threading.current_thread().ident}] åˆå§‹åŒ–{method} Tutor (rounds={num_rounds})...")
            
            if method == 'Vanilla-ICL':
                # Vanilla-ICLæ”¯æŒå¯å˜è½®æ•°
                class VanillaICLConfigurable(VanillaICLTutor):
                    def __init__(self, num_rounds):
                        super().__init__()
                        self.num_rounds = num_rounds
                    
                    def conduct_tutoring_session(self, student_id, dataset, concept_text, student_system_prompt):
                        """å¯é…ç½®è½®æ•°çš„tutoring session"""
                        dialogue = []
                        
                        initial_query = f"I want to learn about {concept_text}"
                        dialogue.append({"role": "user", "round": 0, "content": initial_query})
                        
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœnum_rounds=0ï¼Œç›´æ¥è¿”å›ç©ºdialogue
                        if self.num_rounds == 0:
                            return dialogue
                        
                        # ç¬¬ä¸€è½®
                        first_question = self.generate_question(initial_query, [], student_id, dataset, concept_text)
                        dialogue.append({"role": "assistant", "round": 1, "content": first_question})
                        
                        # åç»­è½®æ¬¡
                        for round_num in range(2, self.num_rounds + 1):
                            last_question = dialogue[-1]['content']
                            student_answer = self.get_student_response(last_question, student_system_prompt)
                            dialogue.append({"role": "user", "round": round_num, "content": student_answer})
                            
                            conversation_history = [{"role": msg["role"], "content": msg["content"]} 
                                                   for msg in dialogue]
                            
                            next_question = self.generate_question(
                                student_answer, conversation_history, student_id, dataset, concept_text
                            )
                            dialogue.append({"role": "assistant", "round": round_num, "content": next_question})
                        
                        return dialogue
                
                tutor = VanillaICLConfigurable(num_rounds)
                
            elif method == 'TutorLLM':
                class TutorLLMConfigurable(TutorLLM):
                    def __init__(self, num_rounds):
                        super().__init__()
                        self.num_rounds = num_rounds
                    
                    def conduct_tutoring_session(self, student_id, dataset, concept_text, student_system_prompt):
                        from student_roleplay_evaluation import load_session
                        
                        # åŠ è½½sessionè·å–persona
                        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
                        session = load_session(session_file)
                        persona_description = session['persona']['description']
                        
                        dialogue = []
                        
                        # Round 0
                        student_request = f"I want to learn about {concept_text}"
                        dialogue.append({
                            "role": "user",
                            "content": student_request,
                            "round": 0
                        })
                        
                        if self.num_rounds == 0:
                            return dialogue
                        
                        # è¿›è¡Œnum_roundsè½®æ•™å­¦
                        for round_num in range(1, self.num_rounds + 1):
                            # RAG: æ£€ç´¢ç›¸å…³memory
                            if round_num == 1:
                                query = f"learning {concept_text}"
                            else:
                                query = dialogue[-1]['content'][:200]
                            
                            relevant_memories = self.retrieve_memory(student_id, dataset, query)
                            memory_context = "\n".join([f"- {mem}" for mem in relevant_memories])
                            
                            # æ„å»ºdialogue context
                            dialogue_context = "\n".join([
                                f"{'Student' if msg['role']=='user' else 'Tutor'}: {msg['content'][:200]}..."
                                for msg in dialogue[-4:]
                            ])
                            
                            # ç”Ÿæˆtutorå›å¤
                            if round_num == 1:
                                prompt = f"""You are a personalized math tutor.

Student Profile:
{persona_description}

Relevant Past Learning:
{memory_context}

The student wants to learn about {concept_text}. Generate your first practice question.

Make it appropriate for their level based on their profile and past learning."""
                            else:
                                last_student_answer = dialogue[-1]['content']
                                prompt = f"""You are a personalized math tutor.

Student Profile:
{persona_description}

Relevant Past Learning:
{memory_context}

Recent Dialogue:
{dialogue_context}

Student's Last Answer:
{last_student_answer}

Task:
1) Provide feedback on their answer
2) Generate the next practice question

Use their profile and past learning to personalize your tutoring."""
                            
                            content = self.tutor_client.chat_completion(
                                messages=[{"role": "user", "content": prompt}],
                                temperature=0.7,
                                max_tokens=500
                            )
                            
                            tutor_response = content if content else "Let's continue."
                            dialogue.append({
                                "role": "assistant",
                                "content": tutor_response,
                                "round": round_num
                            })
                            
                            # å­¦ç”Ÿå›ç­”ï¼ˆæœ€åä¸€è½®ä¸éœ€è¦ï¼‰
                            if round_num < self.num_rounds:
                                response = self.openai_client.chat.completions.create(
                                    model=STUDENT_MODEL,
                                    messages=[
                                        {"role": "system", "content": student_system_prompt},
                                        {"role": "user", "content": f"Answer the tutor's question:\n{tutor_response}"}
                                    ],
                                    temperature=0.7,
                                    max_tokens=300
                                )
                                
                                student_answer = response.choices[0].message.content if response.choices[0].message.content else "I don't know"
                                dialogue.append({
                                    "role": "user",
                                    "content": student_answer,
                                    "round": round_num + 1
                                })
                        
                        return dialogue
                
                tutor = TutorLLMConfigurable(num_rounds)
                
            elif method == 'Vanilla-ICL':
                class VanillaICLConfigurable(VanillaICLTutor):
                    def __init__(self, num_rounds):
                        super().__init__()
                        self.num_rounds = num_rounds
                    
                    def conduct_tutoring_session(self, student_id, dataset, concept_text, student_system_prompt):
                        dialogue = []
                        
                        initial_query = f"I want to learn about {concept_text}"
                        dialogue.append({"role": "user", "round": 0, "content": initial_query})
                        
                        if self.num_rounds == 0:
                            return dialogue
                        
                        # ç¬¬ä¸€è½®
                        first_question = self.generate_question(initial_query, [], student_id, dataset, concept_text)
                        dialogue.append({"role": "assistant", "round": 1, "content": first_question})
                        
                        # åç»­è½®æ¬¡
                        for round_num in range(2, self.num_rounds + 1):
                            last_question = dialogue[-1]['content']
                            student_answer = self.get_student_response(last_question, student_system_prompt)
                            dialogue.append({"role": "user", "round": round_num, "content": student_answer})
                            
                            conversation_history = [{"role": msg["role"], "content": msg["content"]} 
                                                   for msg in dialogue]
                            
                            next_question = self.generate_question(
                                student_answer, conversation_history, student_id, dataset, concept_text
                            )
                            dialogue.append({"role": "assistant", "round": round_num, "content": next_question})
                        
                        return dialogue
                
                tutor = VanillaICLConfigurable(num_rounds)
                
            elif method == 'TASA':
                class TASAConfigurable(TASATutor):
                    def __init__(self, num_rounds):
                        super().__init__()
                        self.num_rounds = num_rounds
                    
                    def conduct_tutoring_session(self, student_id, dataset, concept_text, student_system_prompt):
                        """å¯é…ç½®è½®æ•°çš„TASA tutoring session - å®Œå…¨æŒ‰ç…§åŸç‰ˆæµç¨‹"""
                        dialogue = []
                        
                        # Round 0: å­¦ç”Ÿè¡¨è¾¾æƒ³å­¦ä¹ 
                        initial_query = f"I want to learn about {concept_text}"
                        dialogue.append({
                            "role": "user",
                            "round": 0,
                            "content": initial_query
                        })
                        
                        if self.num_rounds == 0:
                            return dialogue
                        
                        # Round 1: RAG + ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜
                        top_persona, top_memory = self.rag.retrieve_and_rerank(
                            query=initial_query,
                            student_id=student_id,
                            dataset=dataset,
                            concept_text=concept_text
                        )
                        
                        rewritten_persona, rewritten_memory = self.rewriter.rewrite_top_items(
                            top_persona, top_memory,
                            student_id=student_id,
                            dataset=dataset,
                            concept_text=concept_text
                        )
                        
                        first_question = self.generate_first_question(
                            rewritten_persona, rewritten_memory, concept_text
                        )
                        
                        dialogue.append({
                            "role": "assistant",
                            "round": 1,
                            "content": first_question,
                            "retrieved_persona": [p['description'] for p in top_persona],
                            "retrieved_memory": [m['description'] for m in top_memory],
                            "rewritten_persona": rewritten_persona,
                            "rewritten_memory": rewritten_memory
                        })
                        
                        # åç»­è½®æ¬¡ï¼šå­¦ç”Ÿå›ç­” -> RAG -> è®²è§£+é—®é¢˜
                        for round_num in range(2, self.num_rounds + 1):
                            # å­¦ç”Ÿå›ç­”ä¸Šä¸€è½®çš„é—®é¢˜
                            last_question = dialogue[-1]['content']
                            student_answer = self.get_student_response(last_question, student_system_prompt)
                            
                            dialogue.append({
                                "role": "user",
                                "round": round_num,
                                "content": student_answer
                            })
                            
                            # RAGæ£€ç´¢å½“å‰queryï¼ˆå­¦ç”Ÿçš„å›ç­”ï¼‰
                            top_persona, top_memory = self.rag.retrieve_and_rerank(
                                query=student_answer,
                                student_id=student_id,
                                dataset=dataset,
                                concept_text=concept_text
                            )
                            
                            # é‡å†™
                            rewritten_persona, rewritten_memory = self.rewriter.rewrite_top_items(
                                top_persona, top_memory,
                                student_id=student_id,
                                dataset=dataset,
                                concept_text=concept_text
                            )
                            
                            # ç”Ÿæˆè®²è§£+ä¸‹ä¸€ä¸ªé—®é¢˜
                            conversation_history = [{"role": msg["role"], "content": msg["content"]} 
                                                   for msg in dialogue]
                            
                            explanation_and_question = self.generate_explanation_and_question(
                                rewritten_persona, rewritten_memory,
                                conversation_history, concept_text
                            )
                            
                            dialogue.append({
                                "role": "assistant",
                                "round": round_num,
                                "content": explanation_and_question,
                                "retrieved_persona": [p['description'] for p in top_persona],
                                "retrieved_memory": [m['description'] for m in top_memory],
                                "rewritten_persona": rewritten_persona,
                                "rewritten_memory": rewritten_memory
                            })
                        
                        return dialogue
                
                tutor = TASAConfigurable(num_rounds)
            else:
                raise ValueError(f"Unknown method: {method}")
            
            setattr(thread_local, tutor_key, tutor)
    
    return getattr(thread_local, tutor_key)

def get_evaluator():
    """è·å–çº¿ç¨‹æœ¬åœ°çš„Evaluatorå®ä¾‹"""
    if not hasattr(thread_local, 'evaluator'):
        with model_init_lock:
            safe_print(f"   [Thread-{threading.current_thread().ident}] åˆå§‹åŒ–Evaluator...")
            thread_local.evaluator = TASAEvaluator()
    return thread_local.evaluator

def process_single_student(student_id, dataset, method, num_dialogue_turns):
    """
    å¤„ç†å•ä¸ªå­¦ç”Ÿ
    num_dialogue_turns: æ€»çš„dialogueè½®æ•°ï¼ˆstudent + tutorï¼‰
    å®é™…tutorè½®æ•° = num_dialogue_turns // 2
    """
    num_tutor_rounds = num_dialogue_turns // 2
    
    try:
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ“ å­¦ç”Ÿ {student_id} | {method} | {num_dialogue_turns} turns")
        safe_print(f"{'='*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        concept_text = session['concept_text']
        concept_id = session['concept_id']
        
        # Pre-test score - ä»pretestæ–‡ä»¶è¯»å–
        pretest_file = f'/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json'
        if not os.path.exists(pretest_file):
            safe_print(f"   âŒ Pre-testæ–‡ä»¶ä¸å­˜åœ¨: {pretest_file}")
            return None
        
        with open(pretest_file) as f:
            pretest_data = json.load(f)
        pre_test_score = pretest_data['roleplay_accuracy']
        
        # ç‰¹æ®Šå¤„ç†: turns=0ç›´æ¥è¿”å›pre-testç»“æœ
        if num_dialogue_turns == 0:
            safe_print(f"   ğŸ“Š Turns=0: ç›´æ¥ä½¿ç”¨pre-testç»“æœ")
            
            # TASA: åªè¿”å›learning_gain
            if method == 'TASA':
                result = {
                    'student_id': student_id,
                    'pre_test_score': pre_test_score,
                    'post_test_score': pre_test_score,
                    'learning_gain': 0.0,
                    'dialogue_turns': 0,
                    'method': method
                }
            # Baseline: è¿”å›best/avg/worstä¸‰ç§ï¼ˆéƒ½æ˜¯0ï¼‰
            else:
                result = {
                    'student_id': student_id,
                    'pre_test_score': pre_test_score,
                    'dialogue_turns': 0,
                    'method': method,
                    'post_test_run1': pre_test_score,
                    'post_test_run2': pre_test_score,
                    'max_post_test_accuracy': pre_test_score,
                    'learning_gain_max': 0.0,
                    'avg_post_test_accuracy': pre_test_score,
                    'learning_gain_avg': 0.0,
                    'min_post_test_accuracy': pre_test_score,
                    'learning_gain_min': 0.0
                }
            
            safe_print(f"   âœ… Pre-test: {pre_test_score*100:.1f}% | Learning Gain: 0.0%")
            return result
        
        # æ£€æŸ¥28è½®å®Œæ•´dialogueæ˜¯å¦å­˜åœ¨ï¼ˆåªç”Ÿæˆä¸€æ¬¡ï¼‰
        from tasa_config_llama import FORGETTING_SCORE_METHOD
        full_dialogue_dir = f'/mnt/localssd/bank/dialogue/{method}-turns28-llama/{dataset}/{FORGETTING_SCORE_METHOD}'
        full_dialogue_file = f'{full_dialogue_dir}/{student_id}-{concept_text}.json'
        
        if not os.path.exists(full_dialogue_file):
            safe_print(f"   ğŸ“š ç”Ÿæˆå®Œæ•´28è½®dialogueï¼ˆä»…ç”Ÿæˆä¸€æ¬¡ï¼‰...")
            student_prompt = build_student_system_prompt(session)
            
            # å›ºå®šç”Ÿæˆ14ä¸ªtutor roundsï¼ˆ=28 dialogue turnsï¼‰
            tutor = get_tutor(method, 14)
            
            try:
                dialogue = tutor.conduct_tutoring_session(
                    student_id=student_id,
                    dataset=dataset,
                    concept_text=concept_text,
                    student_system_prompt=student_prompt
                )
                
                # ä¿å­˜28è½®å®Œæ•´dialogue
                os.makedirs(full_dialogue_dir, exist_ok=True)
                with open(full_dialogue_file, 'w') as f:
                    json.dump(dialogue, f, indent=2)
                
                safe_print(f"   âœ… å®Œæ•´28è½®dialogueç”Ÿæˆå®Œæˆ")
            except Exception as e:
                safe_print(f"   âŒ Dialogueç”Ÿæˆå¤±è´¥: {e}")
                return None
        else:
            safe_print(f"   âœ… å®Œæ•´28è½®dialogueå·²å­˜åœ¨")
        
        # æŒ‡å‘å®Œæ•´dialogueæ–‡ä»¶ï¼Œåé¢ä¼šæˆªå–
        dialogue_file = full_dialogue_file
        
        # Post-testè¯„ä¼°
        safe_print(f"   ğŸ“ å¼€å§‹post-testè¯„ä¼°...")
        
        # åŠ è½½post-testé—®é¢˜
        questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
        with open(questions_file) as f:
            all_questions = json.load(f)
        # concept_idå¯èƒ½æ˜¯intï¼Œéœ€è¦è½¬ä¸ºstr
        questions = all_questions[str(concept_id)]['questions']
        
        # æ„å»ºstudent prompt
        student_prompt = build_student_system_prompt(session)
        
        # åŠ è½½å®Œæ•´28è½®dialogue
        with open(dialogue_file) as f:
            dialogue_data = json.load(f)
        full_dialogue = dialogue_data if isinstance(dialogue_data, list) else dialogue_data.get('dialogue', [])
        
        # æ ¹æ®num_dialogue_turnsæˆªå–dialogue
        # dialogueæ ¼å¼ï¼š[{"role": "user", "round": 0}, {"role": "assistant", "round": 1}, ...]
        # num_dialogue_turnsåŒ…æ‹¬studentå’Œtutorçš„æ‰€æœ‰æ¶ˆæ¯
        safe_print(f"   âœ‚ï¸  ä»28è½®dialogueä¸­æˆªå–å‰{num_dialogue_turns}è½®...")
        
        if num_dialogue_turns >= 28:
            # ä½¿ç”¨å®Œæ•´dialogue
            dialogue = full_dialogue
        else:
            # æˆªå–æŒ‡å®šè½®æ•°ï¼šä¿ç•™round <= num_dialogue_turnsçš„æ¶ˆæ¯
            dialogue = []
            for msg in full_dialogue:
                if 'round' in msg and msg['round'] <= num_dialogue_turns:
                    dialogue.append(msg)
                else:
                    break  # ä¸€æ—¦è¶…è¿‡å°±åœæ­¢
        
        safe_print(f"   âœ… æˆªå–å®Œæˆï¼šä½¿ç”¨{len(dialogue)}æ¡æ¶ˆæ¯ (ç›®æ ‡{num_dialogue_turns}è½®)")
        
        evaluator = get_evaluator()
        
        try:
            # TASA: ä½¿ç”¨best-of-twoç­–ç•¥
            if method == 'TASA':
                safe_print(f"   ğŸ“Š è¿›è¡Œ2æ¬¡Post-test (å–æœ€å¥½)")
                post_test_results = []
                
                for run_id in range(1, 3):
                    safe_print(f"   Run {run_id}/2")
                    post_acc, _ = evaluator.conduct_post_test(
                        student_id, dataset, concept_text,
                        dialogue, questions, student_prompt
                    )
                    post_test_results.append(post_acc)
                    safe_print(f"   Run {run_id} å‡†ç¡®ç‡: {post_acc*100:.1f}%")
                
                # å–æœ€å¥½çš„ç»“æœ
                best_post_test = max(post_test_results)
                
                if pre_test_score >= 1.0:
                    learning_gain = 0.0
                else:
                    learning_gain = (best_post_test - pre_test_score) / (1.0 - pre_test_score)
                
                result = {
                    'student_id': student_id,
                    'pre_test_score': pre_test_score,
                    'post_test_score': best_post_test,
                    'learning_gain': learning_gain,
                    'dialogue_turns': num_dialogue_turns,
                    'method': method,
                    'post_test_run1': post_test_results[0],
                    'post_test_run2': post_test_results[1]
                }
                
                safe_print(f"   âœ… è¯„ä¼°å®Œæˆ (Best-of-Two)")
                safe_print(f"      Pre: {pre_test_score*100:.1f}% | Best Post: {best_post_test*100:.1f}% | Gain: {learning_gain*100:.1f}%")
            
            # Vanilla-ICLå’ŒTutorLLM: ä¿å­˜best/average/worstä¸‰ç§ç»“æœ
            else:
                safe_print(f"   ğŸ“Š è¿›è¡Œ2æ¬¡Post-test (ä¿å­˜Best/Avg/Worst)")
                post_test_results = []
                
                for run_id in range(1, 3):
                    safe_print(f"   Run {run_id}/2")
                    post_acc, _ = evaluator.conduct_post_test(
                        student_id, dataset, concept_text,
                        dialogue, questions, student_prompt
                    )
                    post_test_results.append(post_acc)
                    safe_print(f"   Run {run_id} å‡†ç¡®ç‡: {post_acc*100:.1f}%")
                
                # è®¡ç®—ä¸‰ç§ç­–ç•¥
                max_post_test = max(post_test_results)
                avg_post_test = np.mean(post_test_results)
                min_post_test = min(post_test_results)
                
                # è®¡ç®—learning gain - åŸºäºæœ€é«˜åˆ†ï¼ˆBestç­–ç•¥ï¼‰
                if pre_test_score >= 1.0:
                    learning_gain_max = 0.0
                else:
                    learning_gain_max = (max_post_test - pre_test_score) / (1.0 - pre_test_score)
                
                # è®¡ç®—learning gain - åŸºäºå¹³å‡åˆ†ï¼ˆAverageç­–ç•¥ï¼‰
                if pre_test_score >= 1.0:
                    learning_gain_avg = 0.0
                else:
                    learning_gain_avg = (avg_post_test - pre_test_score) / (1.0 - pre_test_score)
                
                # è®¡ç®—learning gain - åŸºäºæœ€ä½åˆ†ï¼ˆWorstç­–ç•¥ï¼‰
                if pre_test_score >= 1.0:
                    learning_gain_min = 0.0
                else:
                    learning_gain_min = (min_post_test - pre_test_score) / (1.0 - pre_test_score)
                
                result = {
                    'student_id': student_id,
                    'pre_test_score': pre_test_score,
                    'dialogue_turns': num_dialogue_turns,
                    'method': method,
                    'post_test_run1': post_test_results[0],
                    'post_test_run2': post_test_results[1],
                    # Bestç­–ç•¥
                    'max_post_test_accuracy': max_post_test,
                    'learning_gain_max': learning_gain_max,
                    # Averageç­–ç•¥
                    'avg_post_test_accuracy': avg_post_test,
                    'learning_gain_avg': learning_gain_avg,
                    # Worstç­–ç•¥
                    'min_post_test_accuracy': min_post_test,
                    'learning_gain_min': learning_gain_min
                }
                
                safe_print(f"   âœ… è¯„ä¼°å®Œæˆ")
                safe_print(f"      Pre: {pre_test_score*100:.1f}%")
                safe_print(f"      Best Post: {max_post_test*100:.1f}% (Gain={learning_gain_max*100:.1f}%)")
                safe_print(f"      Avg Post: {avg_post_test*100:.1f}% (Gain={learning_gain_avg*100:.1f}%)")
                safe_print(f"      Worst Post: {min_post_test*100:.1f}% (Gain={learning_gain_min*100:.1f}%)")
            
            return result
            
        except Exception as e:
            safe_print(f"   âŒ è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            safe_print(f"   {traceback.format_exc()}")
            return None
    
    except Exception as e:
        safe_print(f"âŒ å¤„ç†å­¦ç”Ÿ{student_id}æ—¶å‡ºé”™: {e}")
        return None

def batch_evaluate(method, num_dialogue_turns, dataset='assist2017', max_workers=10):
    """æ‰¹é‡è¯„ä¼°"""
    students_file = f'/mnt/localssd/qualified_students_{dataset}_sampled10.json'
    
    with open(students_file) as f:
        students_data = json.load(f)
    
    if isinstance(students_data, list):
        student_ids = students_data
    elif isinstance(students_data, dict):
        if 'student_ids' in students_data:
            student_ids = students_data['student_ids']
        elif 'sampled_students' in students_data:
            student_ids = students_data['sampled_students']
        else:
            print(f"âŒ æ— æ³•è§£æstudentsæ–‡ä»¶")
            return None
    
    print(f"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"â•‘   ğŸ“Š {method} | {num_dialogue_turns} turns | {dataset}                        â•‘")
    print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"   å­¦ç”Ÿæ•°: {len(student_ids)}")
    print(f"   å¹¶è¡Œåº¦: {max_workers}")
    print(f"{'='*80}\n")
    
    all_results = []
    successful_count = 0
    failed_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(process_single_student, sid, dataset, method, num_dialogue_turns): sid 
            for sid in student_ids
        }
        
        for future in as_completed(futures):
            sid = futures[future]
            try:
                result = future.result()
                if result:
                    all_results.append(result)
                    successful_count += 1
                else:
                    failed_count += 1
                
                total_processed = successful_count + failed_count
                safe_print(f"\nğŸ“ˆ è¿›åº¦: {total_processed}/{len(student_ids)} ({total_processed*100/len(student_ids):.1f}%) | æˆåŠŸ: {successful_count} | å¤±è´¥: {failed_count}")
                
            except Exception as e:
                safe_print(f"âŒ å¤„ç†å­¦ç”Ÿ{sid}å¼‚å¸¸: {e}")
                failed_count += 1
    
    # ç»Ÿè®¡
    print(f"\n{'='*80}")
    print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡ ({method}, {num_dialogue_turns} turns):")
    
    if all_results:
        # TASA: ä½¿ç”¨learning_gainï¼ˆbest-of-twoï¼‰
        if method == 'TASA':
            learning_gains = [r['learning_gain'] for r in all_results]
            print(f"\n   å¹³å‡Learning Gain (Best-of-Two): {np.mean(learning_gains)*100:.1f}% Â± {np.std(learning_gains)*100:.1f}%")
            print(f"   ä¸­ä½æ•°: {np.median(learning_gains)*100:.1f}%")
            print(f"   èŒƒå›´: [{np.min(learning_gains)*100:.1f}%, {np.max(learning_gains)*100:.1f}%]")
        # Baseline: æ˜¾ç¤ºbest/average/worstä¸‰ç§ç­–ç•¥
        else:
            learning_gains_max = [r['learning_gain_max'] for r in all_results]
            learning_gains_avg = [r['learning_gain_avg'] for r in all_results]
            learning_gains_min = [r['learning_gain_min'] for r in all_results]
            
            print(f"\n   Bestç­–ç•¥   å¹³å‡Learning Gain: {np.mean(learning_gains_max)*100:.1f}% Â± {np.std(learning_gains_max)*100:.1f}%")
            print(f"   Averageç­–ç•¥ å¹³å‡Learning Gain: {np.mean(learning_gains_avg)*100:.1f}% Â± {np.std(learning_gains_avg)*100:.1f}%")
            print(f"   Worstç­–ç•¥  å¹³å‡Learning Gain: {np.mean(learning_gains_min)*100:.1f}% Â± {np.std(learning_gains_min)*100:.1f}%")
    
    print(f"\nâœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    print(f"   æˆåŠŸ: {successful_count}/{len(student_ids)}")
    print(f"{'='*80}\n")
    
    # ä¿å­˜ç»“æœ
    from tasa_config_llama import FORGETTING_SCORE_METHOD
    result_dir = f'/mnt/localssd/bank/evaluation_results/{method}-turns{num_dialogue_turns}-llama/{dataset}/{FORGETTING_SCORE_METHOD}'
    os.makedirs(result_dir, exist_ok=True)
    
    # TASA: åªä¿å­˜bestç»“æœ
    if method == 'TASA':
        learning_gains = [r['learning_gain'] for r in all_results] if all_results else []
        overall_result = {
            'dataset': dataset,
            'method': method,
            'dialogue_turns': num_dialogue_turns,
            'num_students': len(student_ids),
            'avg_learning_gain': float(np.mean(learning_gains)) if all_results else 0.0,
            'std_learning_gain': float(np.std(learning_gains)) if all_results else 0.0,
            'median_learning_gain': float(np.median(learning_gains)) if all_results else 0.0,
            'students': all_results
        }
    # Baseline: ä¿å­˜best/average/worstä¸‰ç§ç­–ç•¥çš„ç»“æœ
    else:
        learning_gains_max = [r['learning_gain_max'] for r in all_results] if all_results else []
        learning_gains_avg = [r['learning_gain_avg'] for r in all_results] if all_results else []
        learning_gains_min = [r['learning_gain_min'] for r in all_results] if all_results else []
        
        overall_result = {
            'dataset': dataset,
            'method': method,
            'dialogue_turns': num_dialogue_turns,
            'num_students': len(student_ids),
            # Bestç­–ç•¥
            'strategy_max': {
                'avg_learning_gain': float(np.mean(learning_gains_max)) if all_results else 0.0,
                'std_learning_gain': float(np.std(learning_gains_max)) if all_results else 0.0,
                'median_learning_gain': float(np.median(learning_gains_max)) if all_results else 0.0
            },
            # Averageç­–ç•¥
            'strategy_avg': {
                'avg_learning_gain': float(np.mean(learning_gains_avg)) if all_results else 0.0,
                'std_learning_gain': float(np.std(learning_gains_avg)) if all_results else 0.0,
                'median_learning_gain': float(np.median(learning_gains_avg)) if all_results else 0.0
            },
            # Worstç­–ç•¥
            'strategy_min': {
                'avg_learning_gain': float(np.mean(learning_gains_min)) if all_results else 0.0,
                'std_learning_gain': float(np.std(learning_gains_min)) if all_results else 0.0,
                'median_learning_gain': float(np.median(learning_gains_min)) if all_results else 0.0
            },
            'students': all_results
        }
    
    with open(f'{result_dir}/overall.json', 'w') as f:
        json.dump(overall_result, f, indent=2)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_dir}/overall.json")
    
    return overall_result

def main():
    DIALOGUE_TURNS = [0, 4, 8, 12, 16, 20, 24, 28]
    METHODS = ['TASA', 'Vanilla-ICL', 'TutorLLM']
    DATASET = 'assist2017'
    MAX_WORKERS = 10
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                 ğŸ”¬ Dialogue Turns Ablation Study                            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"\nğŸ“Š é…ç½®:")
    print(f"  â€¢ Methods: {', '.join(METHODS)}")
    print(f"  â€¢ Dialogue turns: {DIALOGUE_TURNS}")
    print(f"  â€¢ Dataset: {DATASET}")
    print(f"  â€¢ Backbone: Llama-3.1-8B")
    print(f"  â€¢ Total experiments: {len(METHODS)} Ã— {len(DIALOGUE_TURNS)} = {len(METHODS) * len(DIALOGUE_TURNS)}")
    print(f"  â€¢ Max workers: {MAX_WORKERS}")
    print(f"\nâ±ï¸  é¢„è®¡æ€»æ—¶é—´: ~4-5å°æ—¶")
    print(f"\n{'='*80}\n")
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['TASA_CONFIG'] = 'tasa_config_llama'
    
    start_time = time.time()
    all_results = {}
    
    # Dialogue Turnsåœ¨æœ€å¤–å±‚å¾ªç¯
    for turns in DIALOGUE_TURNS:
        print(f"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print(f"â•‘  ğŸ“Š DIALOGUE TURNS: {turns:^56} â•‘")
        print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
        
        for method in METHODS:
            # è·³è¿‡å·²å­˜åœ¨çš„20è½®å®éªŒï¼ˆå·²ç»åšè¿‡ï¼‰
            if turns == 20:
                print(f"\nâ­ï¸  è·³è¿‡ {method} - {turns} turns (å·²å®Œæˆ)")
                # å°è¯•è¯»å–å·²æœ‰ç»“æœ
                from tasa_config_llama import FORGETTING_SCORE_METHOD
                existing_result_file = f'/mnt/localssd/bank/evaluation_results/{method}-turns20-llama/{DATASET}/{FORGETTING_SCORE_METHOD}/overall.json'
                if os.path.exists(existing_result_file):
                    with open(existing_result_file) as f:
                        existing_data = json.load(f)
                        if method not in all_results:
                            all_results[method] = {}
                        # æ ¹æ®methodç±»å‹æå–learning gain
                        if method == 'TASA':
                            all_results[method][str(turns)] = existing_data['avg_learning_gain']
                        else:  # Baseline: ä½¿ç”¨strategy_max (Bestç­–ç•¥)
                            all_results[method][str(turns)] = existing_data['strategy_max']['avg_learning_gain']
                continue
            
            result = batch_evaluate(method, turns, DATASET, MAX_WORKERS)
            if result:
                if method not in all_results:
                    all_results[method] = {}
                # æ ¹æ®methodç±»å‹æå–learning gain
                if method == 'TASA':
                    all_results[method][str(turns)] = result['avg_learning_gain']
                else:  # Baseline: ä½¿ç”¨strategy_max (Bestç­–ç•¥)
                    all_results[method][str(turns)] = result['strategy_max']['avg_learning_gain']
        
        print(f"\n{'â”€'*80}")
        print(f"ğŸ“Š Turns={turns} å®Œæˆæ‰€æœ‰Methodæµ‹è¯•")
        print(f"{'â”€'*80}\n")
    
    total_elapsed = time.time() - start_time
    
    # æ‰“å°æ±‡æ€»
    print(f"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"â•‘                   ğŸ“Š Dialogue Turns Ablationå®Œæˆæ±‡æ€»                        â•‘")
    print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    print("ç»“æœçŸ©é˜µ (Turns Ã— Method):\n")
    print(f"{'Turns':<10} | {' | '.join([f'{m:^12}' for m in METHODS])}")
    print(f"{'-'*10}-+-{'-+-'.join(['-'*12]*len(METHODS))}")
    
    for turns in DIALOGUE_TURNS:
        row = [f"{turns:<10}"]
        for method in METHODS:
            if str(turns) in all_results.get(method, {}):
                lg = all_results[method][str(turns)] * 100
                row.append(f"{lg:>11.1f}%")
            else:
                row.append("     -")
        print(" | ".join(row))
    
    print(f"\n{'='*80}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_elapsed/3600:.2f} å°æ—¶ ({total_elapsed/60:.1f} åˆ†é’Ÿ)")
    print(f"{'='*80}\n")
    
    # ä¿å­˜ç»“æœ
    results_file = '/mnt/localssd/logs/dialogue_turns_ablation_results.json'
    with open(results_file, 'w') as f:
        json.dump({
            'methods': METHODS,
            'dialogue_turns': DIALOGUE_TURNS,
            'dataset': DATASET,
            'backbone': 'Llama-3.1-8B',
            'results': all_results,
            'elapsed_hours': total_elapsed/3600
        }, f, indent=2)
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜è‡³: {results_file}\n")

if __name__ == '__main__':
    main()

