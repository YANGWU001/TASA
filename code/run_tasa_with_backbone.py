#!/usr/bin/env python3
"""
TASAæ‰¹é‡è¯„ä¼°è„šæœ¬ - æ”¯æŒä¸åŒBackbone
åŸºäºrun_tasa_batch_best_of_two.pyï¼Œæ·»åŠ backboneæ”¯æŒ
"""

import json
import os
import sys
import time
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import numpy as np

from student_roleplay_evaluation import build_student_system_prompt, load_session
from tasa_tutoring import TASATutor
from tasa_evaluation_best_of_two import TASABestOfTwoEvaluator

# å…¨å±€é”
print_lock = Lock()
model_init_lock = Lock()

import threading
thread_local = threading.local()

def safe_print(*args, **kwargs):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°"""
    with print_lock:
        print(*args, **kwargs)

def get_method_name(backbone):
    """æ ¹æ®backboneç”Ÿæˆmethodåç§°"""
    if backbone == "gpt-oss-120b":
        return "TASA-best-of-2"
    elif "llama" in backbone.lower():
        return "TASA-llama-best-of-2"
    elif "qwen" in backbone.lower():
        return "TASA-qwen-best-of-2"
    else:
        return f"TASA-{backbone}-best-of-2"

def evaluate_student_wrapper(args):
    """è¯„ä¼°å•ä¸ªå­¦ç”Ÿçš„wrapper"""
    student_id, dataset, backbone = args
    
    # è·å–çº¿ç¨‹æœ¬åœ°çš„evaluator
    if not hasattr(thread_local, 'evaluator'):
        with model_init_lock:
            thread_local.evaluator = TASABestOfTwoEvaluator()
    
    try:
        method_name = get_method_name(backbone)
        result = thread_local.evaluator.evaluate_student_best_of_two(student_id, dataset)
        
        if result:
            # ä¿å­˜æ—¶ä½¿ç”¨å¸¦backboneçš„methodåç§°
            thread_local.evaluator.save_result(result, method=method_name)
            return (True, result)
        else:
            return (False, None)
    except Exception as e:
        safe_print(f"âŒ å­¦ç”Ÿ{student_id}è¯„ä¼°å¤±è´¥: {e}")
        return (False, None)

def load_student_list(dataset, args):
    """åŠ è½½å­¦ç”Ÿåˆ—è¡¨"""
    if args.students_file:
        list_file = args.students_file
        safe_print(f"ğŸ“‹ ä½¿ç”¨æŒ‡å®šçš„å­¦ç”Ÿåˆ—è¡¨: {list_file}")
    elif args.range20to60:
        list_file = f'/mnt/localssd/qualified_students_{dataset}_20to60.json'
        safe_print(f"ğŸ“‹ ä½¿ç”¨20%-60%è¿‡æ»¤çš„å­¦ç”Ÿåˆ—è¡¨")
    elif args.lt60:
        list_file = f'/mnt/localssd/qualified_students_{dataset}_lt60.json'
        safe_print(f"ğŸ“‹ ä½¿ç”¨<60%è¿‡æ»¤çš„å­¦ç”Ÿåˆ—è¡¨")
    elif args.filtered:
        list_file = f'/mnt/localssd/qualified_students_{dataset}_filtered.json'
        safe_print(f"ğŸ“‹ ä½¿ç”¨è¿‡æ»¤åçš„å­¦ç”Ÿåˆ—è¡¨ï¼ˆæ’é™¤pre-test=100%ï¼‰")
    else:
        list_file = f'/mnt/localssd/qualified_students_{dataset}.json'
        safe_print(f"ğŸ“‹ ä½¿ç”¨å®Œæ•´çš„å­¦ç”Ÿåˆ—è¡¨")
    
    with open(list_file) as f:
        data = json.load(f)
    
    if 'sampled_students' in data:
        students = data['sampled_students']
    elif 'students' in data:
        students = data['students']
    else:
        students = list(data.keys())
    
    return students

def generate_overall_stats(results, dataset, backbone):
    """ç”Ÿæˆoverallç»Ÿè®¡"""
    method_name = get_method_name(backbone)
    
    learning_gains = [r['best_learning_gain'] for r in results]
    improvements = [r['best_improvement'] for r in results]
    
    avg_gain = np.mean(learning_gains)
    std_gain = np.std(learning_gains, ddof=1) if len(learning_gains) > 1 else 0
    median_gain = np.median(learning_gains)
    avg_improvement = np.mean(improvements)
    
    # æŒ‰æ°´å¹³åˆ†ç»„
    struggling = [r for r in results if r['pre_test_accuracy'] < 0.4]
    developing = [r for r in results if 0.4 <= r['pre_test_accuracy'] < 0.6]
    competent = [r for r in results if 0.6 <= r['pre_test_accuracy'] < 0.8]
    strong = [r for r in results if r['pre_test_accuracy'] >= 0.8]
    
    overall_stats = {
        "dataset": dataset,
        "num_students": len(results),
        "method": method_name,
        "backbone": backbone,
        "overall": {
            "avg_learning_gain": avg_gain,
            "std_learning_gain": std_gain,
            "median_learning_gain": median_gain,
            "avg_improvement": avg_improvement,
            "min_gain": min(learning_gains),
            "max_gain": max(learning_gains)
        },
        "by_level": {
            "struggling": {
                "count": len(struggling),
                "avg_gain": np.mean([r['best_learning_gain'] for r in struggling]) if struggling else 0,
                "avg_pre_test": np.mean([r['pre_test_accuracy'] for r in struggling]) if struggling else 0
            },
            "developing": {
                "count": len(developing),
                "avg_gain": np.mean([r['best_learning_gain'] for r in developing]) if developing else 0,
                "avg_pre_test": np.mean([r['pre_test_accuracy'] for r in developing]) if developing else 0
            },
            "competent": {
                "count": len(competent),
                "avg_gain": np.mean([r['best_learning_gain'] for r in competent]) if competent else 0,
                "avg_pre_test": np.mean([r['pre_test_accuracy'] for r in competent]) if competent else 0
            },
            "strong": {
                "count": len(strong),
                "avg_gain": np.mean([r['best_learning_gain'] for r in strong]) if strong else 0,
                "avg_pre_test": np.mean([r['pre_test_accuracy'] for r in strong]) if strong else 0
            }
        },
        "students": results
    }
    
    # ä¿å­˜
    output_dir = f"/mnt/localssd/bank/evaluation_results/{method_name}/{dataset}"
    os.makedirs(output_dir, exist_ok=True)
    output_file = f"{output_dir}/overall.json"
    
    with open(output_file, 'w') as f:
        json.dump(overall_stats, f, indent=2)
    
    safe_print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
    safe_print(f"   å­¦ç”Ÿæ•°: {len(results)}")
    safe_print(f"   å¹³å‡Learning Gain: {avg_gain*100:.1f}% Â± {std_gain*100:.1f}%")
    safe_print(f"   ä¸­ä½æ•°: {median_gain*100:.1f}%")
    safe_print(f"   èŒƒå›´: [{min(learning_gains)*100:.1f}%, {max(learning_gains)*100:.1f}%]")
    safe_print(f"\n   æŒ‰æ°´å¹³åˆ†ç»„:")
    safe_print(f"      Struggling (<40%): {len(struggling)}äºº, å¹³å‡Gain={np.mean([r['best_learning_gain'] for r in struggling])*100:.1f}%" if struggling else "      Struggling: 0äºº")
    safe_print(f"      Developing (40-60%): {len(developing)}äºº, å¹³å‡Gain={np.mean([r['best_learning_gain'] for r in developing])*100:.1f}%" if developing else "      Developing: 0äºº")
    safe_print(f"      Competent (60-80%): {len(competent)}äºº, å¹³å‡Gain={np.mean([r['best_learning_gain'] for r in competent])*100:.1f}%" if competent else "      Competent: 0äºº")
    safe_print(f"      Strong (â‰¥80%): {len(strong)}äºº, å¹³å‡Gain={np.mean([r['best_learning_gain'] for r in strong])*100:.1f}%" if strong else "      Strong: 0äºº")
    
    safe_print(f"\nğŸ’¾ æ•´ä½“ç»Ÿè®¡å·²ä¿å­˜è‡³: {output_file}")
    
    return overall_stats

def main():
    parser = argparse.ArgumentParser(description='TASAæ‰¹é‡è¯„ä¼° - æ”¯æŒä¸åŒBackbone')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç§°')
    parser.add_argument('--backbone', type=str, default='gpt-oss-120b', 
                       help='Backboneæ¨¡å‹ (gpt-oss-120b, llama-3.1-8b, qwen3-4b)')
    parser.add_argument('--students-file', type=str, help='ç›´æ¥æŒ‡å®šå­¦ç”Ÿåˆ—è¡¨æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--range20to60', action='store_true', help='ä½¿ç”¨20%-60%èŒƒå›´çš„å­¦ç”Ÿ')
    parser.add_argument('--lt60', action='store_true', help='ä½¿ç”¨<60%çš„å­¦ç”Ÿ')
    parser.add_argument('--filtered', action='store_true', help='ä½¿ç”¨è¿‡æ»¤åçš„å­¦ç”Ÿï¼ˆæ’é™¤100%ï¼‰')
    parser.add_argument('--all', action='store_true', help='è¯„ä¼°æ‰€æœ‰å­¦ç”Ÿ')
    parser.add_argument('--max-workers', type=int, default=10, help='æœ€å¤§å¹¶è¡Œæ•°')
    
    args = parser.parse_args()
    
    dataset = args.dataset
    backbone = args.backbone
    method_name = get_method_name(backbone)
    
    safe_print("="*80)
    safe_print(f"ğŸš€ TASAæ‰¹é‡è¯„ä¼° (Best-of-2ç­–ç•¥)")
    safe_print("="*80)
    safe_print(f"   Dataset: {dataset}")
    safe_print(f"   Backbone: {backbone}")
    safe_print(f"   Method: {method_name}")
    safe_print(f"   Max Workers: {args.max_workers}")
    safe_print("="*80)
    
    # åŠ è½½å­¦ç”Ÿåˆ—è¡¨
    students = load_student_list(dataset, args)
    
    if not args.all:
        safe_print(f"\nè¯·ä½¿ç”¨ --all å‚æ•°æ¥è¯„ä¼°æ‰€æœ‰å­¦ç”Ÿ")
        return
    
    safe_print(f"\nå°†è¯„ä¼° {len(students)} ä¸ªå­¦ç”Ÿ")
    
    # å¹¶è¡Œè¯„ä¼°
    start_time = time.time()
    results = []
    failed = []
    
    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        future_to_student = {
            executor.submit(evaluate_student_wrapper, (s['student_id'] if isinstance(s, dict) else int(s), dataset, backbone)): 
            (s['student_id'] if isinstance(s, dict) else int(s)) for s in students
        }
        
        for future in as_completed(future_to_student):
            student_id = future_to_student[future]
            try:
                success, result = future.result()
                if success and result:
                    results.append(result)
                    safe_print(f"âœ… å­¦ç”Ÿ{student_id}è¯„ä¼°å®Œæˆ ({len(results)}/{len(students)})")
                else:
                    failed.append(student_id)
                    safe_print(f"âŒ å­¦ç”Ÿ{student_id}è¯„ä¼°å¤±è´¥ ({len(results)}/{len(students)})")
            except Exception as e:
                failed.append(student_id)
                safe_print(f"âŒ å­¦ç”Ÿ{student_id}å‘ç”Ÿå¼‚å¸¸: {e}")
    
    elapsed_time = (time.time() - start_time) / 60
    
    safe_print(f"\nâœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    safe_print(f"   æ€»ç”¨æ—¶: {elapsed_time:.1f}åˆ†é’Ÿ")
    safe_print(f"   æˆåŠŸ: {len(results)}/{len(students)}")
    safe_print(f"   å¤±è´¥: {len(failed)}/{len(students)}")
    
    if failed:
        safe_print(f"\nå¤±è´¥çš„å­¦ç”ŸID: {failed}")
    
    if results:
        generate_overall_stats(results, dataset, backbone)

if __name__ == '__main__':
    main()

