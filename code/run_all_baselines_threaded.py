#!/opt/venv/bin/python3
"""
ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œè¿è¡Œæ‰€æœ‰baselineä»»åŠ¡
"""

import subprocess
import os
import threading
from queue import Queue
from datetime import datetime
import time

# é…ç½®
BACKBONES = {
    'llama': 'llama-3.1-8B-Instruct',
    'qwen': 'Qwen3-4B-Instruct'
}
DATASETS = ['assist2017', 'algebra2005', 'bridge2006', 'nips_task34']
METHODS = ['Vanilla-ICL', 'MathChat', 'TutorLLM', 'PSS-MV']
MAX_WORKERS = 40
MAX_PARALLEL = 8  # æœ€å¤šåŒæ—¶è¿è¡Œ8ä¸ªä»»åŠ¡

results_lock = threading.Lock()
results = {'completed': 0, 'failed': 0, 'total': 0}

def update_tasa_config(tutor_model):
    """æ›´æ–°tasa_config.py"""
    config_file = '/mnt/localssd/tasa_config.py'
    with open(config_file, 'r') as f:
        content = f.read()
    lines = content.split('\n')
    for i, line in enumerate(lines):
        if line.startswith('TUTOR_MODEL = '):
            lines[i] = f'TUTOR_MODEL = "{tutor_model}"'
    with open(config_file, 'w') as f:
        f.write('\n'.join(lines))

def run_single_baseline(method, dataset, backbone, backbone_suffix, task_id):
    """è¿è¡Œå•ä¸ªbaselineä»»åŠ¡"""
    try:
        print(f"[ä»»åŠ¡{task_id}] å¼€å§‹: {method} on {dataset} with {backbone}")
        
        # æ›´æ–°é…ç½®ï¼ˆéœ€è¦åŠ é”é¿å…å†²çªï¼‰
        with results_lock:
            update_tasa_config(backbone)
            time.sleep(0.5)  # ç­‰å¾…é…ç½®å†™å…¥
        
        # æ„å»ºå‘½ä»¤
        students_file = f'/mnt/localssd/qualified_students_{dataset}_sampled10.json'
        log_file = f'/mnt/localssd/logs/baseline_{method}_{backbone_suffix}_{dataset}.log'
        
        cmd = [
            '/opt/venv/bin/python3', '-u',
            '/mnt/localssd/baseline_evaluation_conservative.py',
            '--method', method,
            '--dataset', dataset,
            '--students-file', students_file,
            '--max-workers', str(MAX_WORKERS)
        ]
        
        # è¿è¡Œ
        with open(log_file, 'w') as f:
            result = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT)
        
        # ç§»åŠ¨ç»“æœåˆ°æ­£ç¡®ç›®å½•
        if result.returncode == 0:
            source_dir = f'/mnt/localssd/bank/evaluation_results/{method}-conservative/{dataset}'
            target_dir = f'/mnt/localssd/bank/evaluation_results/{method}-conservative-{backbone_suffix}/{dataset}'
            
            if os.path.exists(source_dir):
                os.makedirs(os.path.dirname(target_dir), exist_ok=True)
                import shutil
                shutil.move(source_dir, target_dir)
                
                with results_lock:
                    results['completed'] += 1
                print(f"[ä»»åŠ¡{task_id}] âœ… {method} on {dataset} å®Œæˆ")
                return True
        
        with results_lock:
            results['failed'] += 1
        print(f"[ä»»åŠ¡{task_id}] âŒ {method} on {dataset} å¤±è´¥")
        return False
        
    except Exception as e:
        with results_lock:
            results['failed'] += 1
        print(f"[ä»»åŠ¡{task_id}] âŒ {method} on {dataset} å¼‚å¸¸: {e}")
        return False

def worker(queue):
    """å·¥ä½œçº¿ç¨‹"""
    while True:
        task = queue.get()
        if task is None:
            break
        
        method, dataset, backbone, backbone_suffix, task_id = task
        run_single_baseline(method, dataset, backbone, backbone_suffix, task_id)
        queue.task_done()

def main():
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸš€ å¹¶è¡Œè¿è¡Œæ‰€æœ‰Baselineä»»åŠ¡ (Llama + Qwen)")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    # ç”Ÿæˆæ‰€æœ‰ä»»åŠ¡
    tasks = []
    task_id = 1
    
    for backbone_suffix, backbone in BACKBONES.items():
        for dataset in DATASETS:
            for method in METHODS:
                tasks.append((method, dataset, backbone, backbone_suffix, task_id))
                task_id += 1
    
    results['total'] = len(tasks)
    
    print(f"âš™ï¸  é…ç½®:")
    print(f"   â€¢ æ€»ä»»åŠ¡æ•°: {len(tasks)}")
    print(f"   â€¢ æœ€å¤§å¹¶è¡Œæ•°: {MAX_PARALLEL}")
    print(f"   â€¢ æ¯ä»»åŠ¡Workers: {MAX_WORKERS}")
    print(f"   â€¢ æ ·æœ¬é‡: 10äºº/dataset")
    print()
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    queue = Queue()
    for task in tasks:
        queue.put(task)
    
    # åˆ›å»ºå·¥ä½œçº¿ç¨‹
    threads = []
    for i in range(MAX_PARALLEL):
        t = threading.Thread(target=worker, args=(queue,))
        t.start()
        threads.append(t)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    start_time = datetime.now()
    queue.join()
    
    # åœæ­¢å·¥ä½œçº¿ç¨‹
    for i in range(MAX_PARALLEL):
        queue.put(None)
    for t in threads:
        t.join()
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    # æ‰“å°ç»“æœ
    print("\n" + "â”"*80)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("â”"*80)
    print(f"æ€»ä»»åŠ¡æ•°: {results['total']}")
    print(f"æˆåŠŸ: {results['completed']}")
    print(f"å¤±è´¥: {results['failed']}")
    print(f"æ€»è€—æ—¶: {duration}")
    print("â”"*80)

if __name__ == '__main__':
    main()

