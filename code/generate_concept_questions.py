#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸ºæ¯ä¸ªæ•°æ®é›†çš„conceptsç”Ÿæˆæµ‹è¯•é—®é¢˜é›†
- ä½¿ç”¨GPT-4oç”Ÿæˆé—®é¢˜
- æ¯ä¸ªconceptç”Ÿæˆ10ä¸ªé—®é¢˜
- 30ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†
"""

import os
import json
import argparse
from openai import OpenAI
import concurrent.futures
from tqdm import tqdm
import time

# LLMé…ç½®
ENDPOINT = ""  # Your API endpoint
KEY = ""  # Your API key
MODEL = "gpt-4o"
TEMPERATURE = 0.7  # ç”Ÿæˆé—®é¢˜ä½¿ç”¨0.7ï¼Œä¿æŒå¤šæ ·æ€§ä½†ä¸ä¼šå¤ªéšæœº

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    api_key="Bearer " + KEY,
    base_url=ENDPOINT,
)

# æ•°æ®é›†é…ç½®
DATASET_MAPPING = {
    'assist2017': 'assist2017',
    'nips_task34': 'nips_task34',
    'algebra2005': 'algebra2005',
    'bridge2006': 'bridge2algebra2006'
}

def load_concepts(dataset_name):
    """åŠ è½½æ•°æ®é›†çš„conceptåˆ—è¡¨"""
    # æ˜ å°„åˆ°å®é™…çš„æ•°æ®é›†ç›®å½•å
    actual_dataset = DATASET_MAPPING.get(dataset_name, dataset_name)
    
    keyid_file = f'/mnt/localssd/pykt-toolkit/data/{actual_dataset}/keyid2idx.json'
    
    if not os.path.exists(keyid_file):
        print(f"âŒ Keyidæ–‡ä»¶ä¸å­˜åœ¨: {keyid_file}")
        return {}
    
    with open(keyid_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # conceptså­—æ®µï¼š{concept_text: concept_id}
    concepts_dict = data.get('concepts', {})
    
    # è½¬æ¢ä¸º {concept_id: concept_text}
    concepts = {
        concept_id: concept_text 
        for concept_text, concept_id in concepts_dict.items()
    }
    
    print(f"  âœ… åŠ è½½äº† {len(concepts)} ä¸ªconcepts")
    return concepts

def generate_questions_for_concept(concept_id, concept_text, retry=3):
    """ä¸ºå•ä¸ªconceptç”Ÿæˆ10ä¸ªé—®é¢˜"""
    
    system_prompt = """You are an expert educational content creator. Your task is to generate 10 diverse and high-quality test questions based on the given concept description.

Requirements:
1. Generate exactly 10 questions
2. Questions should test understanding of the concept at different difficulty levels
3. Include a mix of question types (factual, conceptual, application)
4. Make questions clear, specific, and answerable
5. Return ONLY a JSON array of 10 questions, no other text

Example output format:
["Question 1 text here", "Question 2 text here", ..., "Question 10 text here"]"""

    user_prompt = f"""Generate 10 test questions for the following concept:

Concept: {concept_text}

Generate questions that assess students' understanding of this concept. Return ONLY a JSON array of 10 question strings."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    for attempt in range(retry):
        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=messages,
                temperature=TEMPERATURE
            )
            
            content = response.choices[0].message.content.strip()
            
            # å°è¯•è§£æJSON
            # å¦‚æœresponseè¢«åŒ…åœ¨markdownä»£ç å—ä¸­ï¼Œå…ˆæå–
            if content.startswith('```'):
                # æå–JSONéƒ¨åˆ†
                start = content.find('[')
                end = content.rfind(']') + 1
                if start != -1 and end > start:
                    content = content[start:end]
            
            questions = json.loads(content)
            
            # éªŒè¯æ ¼å¼
            if isinstance(questions, list) and len(questions) == 10:
                return {
                    'concept_id': concept_id,
                    'concept_description': concept_text,
                    'questions': questions
                }
            else:
                print(f"  âš ï¸  Concept {concept_id}: è¿”å›çš„é—®é¢˜æ•°é‡ä¸å¯¹ ({len(questions)}), é‡è¯•...")
                
        except json.JSONDecodeError as e:
            print(f"  âš ï¸  Concept {concept_id}: JSONè§£æå¤±è´¥, é‡è¯• {attempt+1}/{retry}...")
            time.sleep(1)
            
        except Exception as e:
            print(f"  âŒ Concept {concept_id}: é”™è¯¯ - {e}, é‡è¯• {attempt+1}/{retry}...")
            time.sleep(1)
    
    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºé—®é¢˜
    print(f"  âŒ Concept {concept_id}: æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨å ä½ç¬¦")
    return {
        'concept_id': concept_id,
        'concept_description': concept_text,
        'questions': [f"Question {i+1} for {concept_text}" for i in range(10)]
    }

def generate_questions_for_dataset(dataset_name, max_workers=30):
    """ä¸ºæ•´ä¸ªæ•°æ®é›†ç”Ÿæˆé—®é¢˜"""
    print("="*100)
    print(f"ğŸ“š ä¸º {dataset_name.upper()} ç”ŸæˆConcepté—®é¢˜é›†")
    print("="*100)
    print()
    
    # 1. åŠ è½½concepts
    print("ğŸ“‚ åŠ è½½Concepts...")
    concepts = load_concepts(dataset_name)
    
    if not concepts:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°conceptsï¼Œé€€å‡º")
        return
    
    print()
    
    # 2. ä½¿ç”¨å¤šçº¿ç¨‹ç”Ÿæˆé—®é¢˜
    print(f"ğŸ¤– ä½¿ç”¨GPT-4oç”Ÿæˆé—®é¢˜ (æ¸©åº¦={TEMPERATURE}, çº¿ç¨‹={max_workers})...")
    print()
    
    results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(generate_questions_for_concept, concept_id, concept_text): (concept_id, concept_text)
            for concept_id, concept_text in concepts.items()
        }
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for future in tqdm(concurrent.futures.as_completed(futures), 
                          total=len(futures), 
                          desc="ç”Ÿæˆé—®é¢˜", 
                          ncols=100):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                concept_id, concept_text = futures[future]
                print(f"  âŒ Concept {concept_id} å¤±è´¥: {e}")
    
    # 3. æŒ‰concept_idæ’åº
    results.sort(key=lambda x: x['concept_id'])
    
    # 4. è½¬æ¢ä¸ºç›®æ ‡æ ¼å¼
    output_data = {
        str(item['concept_id']): {
            'concept_description': item['concept_description'],
            'questions': item['questions']
        }
        for item in results
    }
    
    print()
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(output_data)} ä¸ªconceptsçš„é—®é¢˜é›†")
    print()
    
    # 5. ç»Ÿè®¡
    total_questions = sum(len(item['questions']) for item in output_data.values())
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  Conceptsæ€»æ•°: {len(output_data)}")
    print(f"  é—®é¢˜æ€»æ•°: {total_questions}")
    print(f"  å¹³å‡æ¯ä¸ªconcept: {total_questions / len(output_data):.1f} ä¸ªé—®é¢˜")
    print()
    
    # 6. ä¿å­˜
    output_dir = f'/mnt/localssd/bank/test_data/{dataset_name}'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'concept_questions.json')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    file_size = os.path.getsize(output_file) / 1024 / 1024
    print("ğŸ’¾ ä¿å­˜ç»“æœ:")
    print(f"  æ–‡ä»¶: {output_file}")
    print(f"  å¤§å°: {file_size:.2f}MB")
    print()
    
    # 7. æ˜¾ç¤ºç¤ºä¾‹
    if output_data:
        sample_id = list(output_data.keys())[0]
        sample_data = output_data[sample_id]
        
        print("ğŸ“‹ ç¤ºä¾‹æ•°æ®:")
        print(f"  Concept ID: {sample_id}")
        print(f"  Description: {sample_data['concept_description']}")
        print(f"  Questions ({len(sample_data['questions'])}):")
        for i, q in enumerate(sample_data['questions'][:3], 1):
            print(f"    {i}. {q}")
        if len(sample_data['questions']) > 3:
            print(f"    ... (è¿˜æœ‰ {len(sample_data['questions']) - 3} ä¸ªé—®é¢˜)")

def main():
    parser = argparse.ArgumentParser(description='ä¸ºæ•°æ®é›†çš„conceptsç”Ÿæˆæµ‹è¯•é—®é¢˜é›†')
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['assist2017', 'nips_task34', 'algebra2005', 'bridge2006'],
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--workers', type=int, default=30,
                       help='å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤: 30)')
    
    args = parser.parse_args()
    
    generate_questions_for_dataset(args.dataset, max_workers=args.workers)
    
    print("="*100)
    print("âœ… å®Œæˆï¼")
    print("="*100)

if __name__ == '__main__':
    main()

