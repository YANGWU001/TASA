#!/usr/bin/env python
"""
é‡æ–°ç”ŸæˆMemory - å…¨é‡ç‰ˆæœ¬
ä¸ä½¿ç”¨LLMï¼Œç›´æ¥ç”¨æ¨¡æ¿+éšæœºåŒ–ç”Ÿæˆ
å¤„ç†æ‰€æœ‰å†å²è®°å½•ï¼Œä¸é™åˆ¶æ•°é‡
"""

import os
import json
import numpy as np
import pandas as pd
from collections import defaultdict
from tqdm import tqdm
import argparse
import random

# BGEæ¨¡å‹
try:
    from FlagEmbedding import BGEM3FlagModel
except ImportError:
    from FlagEmbedding import FlagModel as BGEM3FlagModel

# æ•°æ®é›†é…ç½®
DATASET_MAPPING = {
    'assist2017': 'assist2017',
    'nips_task34': 'nips_task34',
    'algebra2005': 'algebra2005',
    'bridge2006': 'bridge2algebra2006'
}

# æ¨¡æ¿åº“ - å›ç­”æ­£ç¡®
TEMPLATES_CORRECT = [
    "The student successfully solved a {concept} problem.",
    "The student correctly answered a question on {concept}.",
    "The student demonstrated understanding of {concept} by answering correctly.",
    "The student tackled a {concept} question and got it right.",
    "The student showed mastery of {concept} in this attempt.",
    "The student nailed the {concept} concept.",
    "The student aced a problem involving {concept}.",
    "The student cracked a {concept} question successfully.",
    "The student confidently handled {concept}.",
    "The student skillfully worked through {concept}.",
    "The student demonstrated proficiency in {concept}.",
    "The student solved a {concept} problem with ease.",
    "The student correctly applied {concept} knowledge.",
    "The student answered a {concept} question accurately.",
    "The student mastered the {concept} task."
]

# æ¨¡æ¿åº“ - å›ç­”é”™è¯¯
TEMPLATES_INCORRECT = [
    "The student struggled with a {concept} question.",
    "The student made an error on a {concept} problem.",
    "The student found {concept} challenging in this attempt.",
    "The student attempted {concept} but answered incorrectly.",
    "The student had difficulty with a {concept} question.",
    "The student stumbled on {concept}.",
    "The student fumbled a {concept} problem.",
    "The student missed a {concept} question.",
    "The student struggled to apply {concept}.",
    "The student encountered challenges with {concept}.",
    "The student made a mistake on {concept}.",
    "The student answered a {concept} question incorrectly.",
    "The student faced difficulty understanding {concept}.",
    "The student got a {concept} problem wrong.",
    "The student had trouble with {concept} in this attempt."
]

def load_concept_mapping(dataset_name):
    """åŠ è½½concept IDåˆ°æ–‡æœ¬çš„æ˜ å°„"""
    actual_dataset = DATASET_MAPPING.get(dataset_name, dataset_name)
    keyid_file = f'/mnt/localssd/pykt-toolkit/data/{actual_dataset}/keyid2idx.json'
    
    if not os.path.exists(keyid_file):
        return {}
    
    with open(keyid_file) as f:
        data = json.load(f)
    
    concepts_dict = data.get('concepts', {})
    idx2concept = {v: k for k, v in concepts_dict.items()}
    
    # å¯¹äº nips_task34ï¼Œconcept æ˜¯æ•°å­— IDï¼Œéœ€è¦æ˜ å°„åˆ°å®é™…çš„ subject åç§°
    if dataset_name == 'nips_task34':
        metadata_file = f'/mnt/localssd/pykt-toolkit/data/{actual_dataset}/metadata/subject_metadata.csv'
        if os.path.exists(metadata_file):
            df_subject = pd.read_csv(metadata_file)
            subject_map = {}
            for _, row in df_subject.iterrows():
                subject_id = str(row['SubjectId'])
                name = row['Name']
                subject_map[subject_id] = name
            
            # å°† idx2concept ä¸­çš„æ•°å­— ID æ›¿æ¢ä¸ºå®é™…åç§°
            new_idx2concept = {}
            for idx, concept_id in idx2concept.items():
                if concept_id in subject_map:
                    new_idx2concept[idx] = subject_map[concept_id]
                else:
                    new_idx2concept[idx] = concept_id
            idx2concept = new_idx2concept
            
            print(f"  ğŸ“‹ ä¸º nips_task34 åŠ è½½äº† {len(subject_map)} ä¸ª Subject æ˜ å°„")
    
    return idx2concept

def parse_csv_field(field_str):
    """è§£æCSVå­—æ®µ"""
    if pd.isna(field_str) or field_str == '':
        return []
    return [int(x) for x in str(field_str).split(',')]

def extract_student_data(row, idx2concept):
    """æå–å­¦ç”Ÿæ•°æ®ï¼Œæ’é™¤æ¯ä¸ªconceptçš„æœ€åä¸€æ¬¡
    
    Args:
        row: å·²åˆå¹¶çš„å•è¡Œå­¦ç”Ÿæ•°æ®
        idx2concept: conceptæ˜ å°„
    """
    uid = row['uid']
    
    # è§£æå­—æ®µï¼ˆå·²ç»æ˜¯åˆå¹¶åçš„æ•°æ®ï¼‰
    questions = parse_csv_field(row['questions'])
    concepts = parse_csv_field(row['concepts'])
    responses = parse_csv_field(row['responses'])
    timestamps = parse_csv_field(row['timestamps'])
    
    # æ„å»ºäº¤äº’
    interactions = []
    for i in range(min(len(questions), len(concepts), len(responses), len(timestamps))):
        interactions.append({
            'question_id': questions[i],
            'concept_id': concepts[i],
            'concept_text': idx2concept.get(concepts[i], f'Concept {concepts[i]}'),
            'response': responses[i],
            'timestamp': timestamps[i]
        })
    
    # æŒ‰conceptåˆ†ç»„
    concept_groups = defaultdict(list)
    for inter in interactions:
        concept_groups[inter['concept_id']].append(inter)
    
    # åˆ†ç¦»å†å²å’Œæœ€åä¸€æ¬¡
    history = []
    last_interactions = {}
    
    for cid, inters in concept_groups.items():
        concept_text = inters[0]['concept_text']
        if len(inters) > 1:
            history.extend(inters[:-1])
            last_interactions[str(cid)] = {
                'concept_id': cid,
                'concept_text': concept_text,
                'question_id': inters[-1]['question_id'],
                'response': inters[-1]['response'],
                'timestamp': inters[-1]['timestamp']
            }
        elif len(inters) == 1:
            last_interactions[str(cid)] = {
                'concept_id': cid,
                'concept_text': concept_text,
                'question_id': inters[0]['question_id'],
                'response': inters[0]['response'],
                'timestamp': inters[0]['timestamp']
            }
    
    return {
        'uid': uid,
        'history': history,
        'last_interactions': last_interactions
    }

def generate_memory_template(history):
    """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆmemory - å¤„ç†æ‰€æœ‰å†å²è®°å½•"""
    memories = []
    
    # æŒ‰æ—¶é—´æ’åº
    history_sorted = sorted(history, key=lambda x: x['timestamp'])
    
    # ä¸ºæ¯æ¡å†å²è®°å½•ç”Ÿæˆæè¿°
    for inter in history_sorted:
        concept_text = inter['concept_text']
        
        # æ ¹æ®responseé€‰æ‹©æ¨¡æ¿
        if inter['response'] == 1:
            templates = TEMPLATES_CORRECT
        else:
            templates = TEMPLATES_INCORRECT
        
        # ä½¿ç”¨question_idå’Œconcept_idæ¥"éšæœº"é€‰æ‹©æ¨¡æ¿ï¼ˆä¿è¯å¯å¤ç°ï¼‰
        template_idx = (inter['question_id'] + inter['concept_id']) % len(templates)
        description = templates[template_idx].format(concept=concept_text)
        
        memories.append({
            'concept_id': int(inter['concept_id']),
            'concept_text': concept_text,
            'description': description,
            'keywords': concept_text,
            'question_id': inter['question_id'],
            'response': inter['response'],
            'timestamp': inter['timestamp']
        })
    
    return memories

def generate_embeddings_batch(texts, model):
    """æ‰¹é‡ç”Ÿæˆembeddings"""
    if not texts:
        return []
    
    try:
        result = model.encode(texts, batch_size=min(32, len(texts)))
        return result
    except Exception as e:
        print(f"  Embeddingç”Ÿæˆå¤±è´¥: {e}")
        return None

def process_student(row, dataset_name, idx2concept, bge_model):
    """å¤„ç†å•ä¸ªå­¦ç”Ÿ"""
    try:
        # æå–æ•°æ®ï¼ˆrowå·²ç»æ˜¯åˆå¹¶åçš„æ•°æ®ï¼‰
        data = extract_student_data(row, idx2concept)
        uid = str(data['uid'])
        
        if len(data['history']) == 0:
            return {'uid': uid, 'status': 'skipped', 'reason': 'no_history', 'memory_count': 0, 'unique_concepts': 0}
        
        # ç”Ÿæˆmemoryï¼ˆå…¨é‡ï¼Œä¸é™åˆ¶æ•°é‡ï¼‰
        memories = generate_memory_template(data['history'])
        
        # ä¿å­˜memoryæ•°æ®æ–‡ä»¶
        base_dir = f"/mnt/localssd/bank"
        memory_data_file = f"{base_dir}/memory/{dataset_name}/data/{uid}.json"
        os.makedirs(os.path.dirname(memory_data_file), exist_ok=True)
        with open(memory_data_file, 'w', encoding='utf-8') as f:
            json.dump(memories, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆmemory embeddings
        if memories and bge_model is not None:
            desc_texts = [m['description'] for m in memories]
            key_texts = [m['keywords'] for m in memories]
            
            desc_embs = generate_embeddings_batch(desc_texts, bge_model)
            key_embs = generate_embeddings_batch(key_texts, bge_model)
            
            emb_dir = f"{base_dir}/memory/{dataset_name}/embeddings"
            os.makedirs(emb_dir, exist_ok=True)
            
            if desc_embs is not None:
                desc_emb_file = f"{emb_dir}/{uid}_description.npz"
                np.savez_compressed(desc_emb_file, embeddings=desc_embs)
            
            if key_embs is not None:
                key_emb_file = f"{emb_dir}/{uid}_keywords.npz"
                np.savez_compressed(key_emb_file, embeddings=key_embs)
        
        # ç»Ÿè®¡ä¿¡æ¯
        unique_concepts = len(set(m['concept_id'] for m in memories))
        
        return {
            'uid': uid,
            'status': 'success',
            'memory_count': len(memories),
            'unique_concepts': unique_concepts
        }
    
    except Exception as e:
        return {'uid': str(row['uid']), 'status': 'error', 'error': str(e)}

def verify_memory_persona_consistency(dataset_name, student_ids):
    """éªŒè¯memoryå’Œpersonaçš„conceptä¸€è‡´æ€§"""
    print(f"\n{'='*80}")
    print(f"éªŒè¯ {dataset_name} çš„Memoryå’ŒPersonaä¸€è‡´æ€§")
    print(f"{'='*80}\n")
    
    for uid in student_ids:
        uid_str = str(uid)
        
        # è¯»å–persona
        persona_file = f'/mnt/localssd/bank/persona/{dataset_name}/data/{uid_str}.json'
        if not os.path.exists(persona_file):
            print(f"  å­¦ç”Ÿ{uid}: Personaæ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        with open(persona_file) as f:
            persona = json.load(f)
        
        persona_concepts = set(p['concept_id'] for p in persona)
        
        # è¯»å–memory
        memory_file = f'/mnt/localssd/bank/memory/{dataset_name}/data/{uid_str}.json'
        if not os.path.exists(memory_file):
            print(f"  å­¦ç”Ÿ{uid}: Memoryæ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        with open(memory_file) as f:
            memory = json.load(f)
        
        memory_concepts = set(m['concept_id'] for m in memory)
        
        # å¯¹æ¯”
        match = persona_concepts == memory_concepts
        status = 'âœ… å®Œå…¨åŒ¹é…' if match else 'âŒ ä¸åŒ¹é…'
        
        print(f"  å­¦ç”Ÿ{uid}:")
        print(f"    Persona concepts: {len(persona_concepts)} ä¸ª")
        print(f"    Memory concepts:  {len(memory_concepts)} ä¸ª")
        print(f"    Memory records:   {len(memory)} æ¡")
        print(f"    çŠ¶æ€: {status}")
        
        if not match:
            only_in_persona = persona_concepts - memory_concepts
            only_in_memory = memory_concepts - persona_concepts
            if only_in_persona:
                print(f"    åªåœ¨Personaä¸­: {sorted(only_in_persona)[:10]}...")
            if only_in_memory:
                print(f"    åªåœ¨Memoryä¸­:  {sorted(only_in_memory)[:10]}...")
        print()

def main(dataset_name, test_mode=False, test_students=3):
    """ä¸»å‡½æ•°"""
    print("="*100)
    print(f"é‡æ–°ç”ŸæˆMemory - {dataset_name.upper()}")
    print(f"æ¨¡å¼: {'æµ‹è¯•æ¨¡å¼ (å‰{}ä¸ªå­¦ç”Ÿ)'.format(test_students) if test_mode else 'å…¨é‡æ¨¡å¼'}")
    print("="*100)
    print()
    
    # 1. åŠ è½½conceptæ˜ å°„
    print("ğŸ“‚ åŠ è½½Conceptæ˜ å°„...")
    idx2concept = load_concept_mapping(dataset_name)
    print(f"  âœ… åŠ è½½äº† {len(idx2concept)} ä¸ªconceptæ˜ å°„")
    print()
    
    # 2. åŠ è½½æ•°æ®ï¼ˆtrain + testï¼‰å¹¶åˆå¹¶åŒä¸€å­¦ç”Ÿçš„å¤šè¡Œæ•°æ®
    actual_dataset = DATASET_MAPPING.get(dataset_name, dataset_name)
    train_file = f'/mnt/localssd/pykt-toolkit/data/{actual_dataset}/train_valid_sequences.csv'
    test_file = f'/mnt/localssd/pykt-toolkit/data/{actual_dataset}/test_sequences.csv'
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®:")
    print(f"  Train+Valid: {train_file}")
    df_train = pd.read_csv(train_file)
    print(f"    è¡Œæ•°: {len(df_train)}, uniqueå­¦ç”Ÿ: {df_train['uid'].nunique()}")
    
    # åŠ è½½testæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(test_file):
        print(f"  Test: {test_file}")
        df_test = pd.read_csv(test_file)
        print(f"    è¡Œæ•°: {len(df_test)}, uniqueå­¦ç”Ÿ: {df_test['uid'].nunique()}")
        
        # åˆå¹¶trainå’Œtest
        df = pd.concat([df_train, df_test], ignore_index=True)
        print(f"  åˆå¹¶åæ€»è¡Œæ•°: {len(df)}, uniqueå­¦ç”Ÿ: {df['uid'].nunique()}")
    else:
        print(f"  âš ï¸  Testæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåªä½¿ç”¨train+validæ•°æ®")
        df = df_train
    
    # åˆå¹¶åŒä¸€å­¦ç”Ÿçš„å¤šè¡Œæ•°æ®ï¼ˆå‚è€ƒcreate_student_bank_final.pyçš„é€»è¾‘ï¼‰
    print(f"  åˆå¹¶åŒä¸€å­¦ç”Ÿçš„å¤šä¸ªsequence...")
    grouped = df.groupby('uid')
    merged_data = []
    
    for uid, group in grouped:
        merged_row = {'uid': uid}
        # åˆå¹¶questions, concepts, responses, timestampså­—æ®µ
        for col in ['questions', 'concepts', 'responses', 'timestamps']:
            if col in group.columns:
                all_vals = []
                for val in group[col]:
                    if pd.notna(val) and val != '' and str(val) != '-1':
                        vals = [v.strip() for v in str(val).split(',') if v.strip() != '-1' and v.strip() != '']
                        all_vals.extend(vals)
                merged_row[col] = ','.join(all_vals) if all_vals else ''
        merged_data.append(merged_row)
    
    df = pd.DataFrame(merged_data)
    print(f"  åˆå¹¶å: {len(df)} ä¸ªuniqueå­¦ç”Ÿ")
    
    if test_mode:
        df = df.head(test_students)
        print(f"  âš ï¸  æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰{test_students}ä¸ªå­¦ç”Ÿ")
    
    print(f"  âœ… å°†å¤„ç† {len(df)} ä¸ªå­¦ç”Ÿ")
    print()
    
    # 3. åˆå§‹åŒ–BGEæ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–BGEæ¨¡å‹...")
    bge_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
    print("  âœ… BGEæ¨¡å‹åŠ è½½å®Œæˆ")
    print()
    
    # 4. å¤„ç†å­¦ç”Ÿï¼ˆæ•°æ®å·²ç»åˆå¹¶è¿‡äº†ï¼‰
    print(f"ğŸ”„ å¤„ç†å­¦ç”Ÿ (å…±{len(df)}ä¸ª)...")
    print()
    
    results = []
    for _, row in tqdm(df.iterrows(), total=len(df), desc="ç”ŸæˆMemory", ncols=100):
        result = process_student(row, dataset_name, idx2concept, bge_model)
        results.append(result)
    
    # 5. ç»Ÿè®¡
    print()
    print("ğŸ“Š å¤„ç†ç»“æœ:")
    success_count = sum(1 for r in results if r['status'] == 'success')
    skipped_count = sum(1 for r in results if r['status'] == 'skipped')
    error_count = sum(1 for r in results if r['status'] == 'error')
    
    print(f"  æˆåŠŸ: {success_count}")
    print(f"  è·³è¿‡: {skipped_count}")
    print(f"  é”™è¯¯: {error_count}")
    
    if success_count > 0:
        total_memories = sum(r.get('memory_count', 0) for r in results if r['status'] == 'success')
        avg_memories = total_memories / success_count
        total_concepts = sum(r.get('unique_concepts', 0) for r in results if r['status'] == 'success')
        avg_concepts = total_concepts / success_count
        
        print(f"\n  Memoryç»Ÿè®¡:")
        print(f"    æ€»è®°å½•æ•°: {total_memories}")
        print(f"    å¹³å‡æ¯å­¦ç”Ÿ: {avg_memories:.1f} æ¡")
        print(f"    å¹³å‡unique concepts: {avg_concepts:.1f} ä¸ª")
    
    print()
    
    # 6. éªŒè¯ï¼ˆæµ‹è¯•æ¨¡å¼ä¸‹ï¼‰
    if test_mode:
        test_uids = df['uid'].unique().tolist()
        verify_memory_persona_consistency(dataset_name, test_uids)
    
    print("="*100)
    print("âœ… å®Œæˆï¼")
    print("="*100)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='é‡æ–°ç”ŸæˆMemoryï¼ˆå…¨é‡ï¼‰')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç§°')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å‰Nä¸ªå­¦ç”Ÿï¼‰')
    parser.add_argument('--test-students', type=int, default=3, help='æµ‹è¯•æ¨¡å¼ä¸‹å¤„ç†çš„å­¦ç”Ÿæ•°')
    
    args = parser.parse_args()
    main(args.dataset, test_mode=args.test, test_students=args.test_students)

