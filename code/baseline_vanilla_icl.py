"""
Baseline 1: Vanilla ICL (In-Context Learning)
åªä½¿ç”¨persona descriptionï¼Œä¸æ¶‰åŠknowledge tracingå’Œmemory
"""

import json
import os
from openai import OpenAI
from typing import List, Dict
from student_roleplay_evaluation import load_session

# ä»tasa_configå¯¼å…¥APIé…ç½®ï¼ˆæ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©ï¼‰
import os
import sys

# æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©é…ç½®æ–‡ä»¶
_config_module = os.environ.get('TASA_CONFIG', 'tasa_config')
if _config_module == 'tasa_config_llama':
    from tasa_config_llama import ENDPOINT, GPT_ENDPOINT, API_KEY, TUTOR_MODEL, STUDENT_MODEL
elif _config_module == 'tasa_config_qwen':
    from tasa_config_qwen import ENDPOINT, GPT_ENDPOINT, API_KEY, TUTOR_MODEL, STUDENT_MODEL
elif _config_module == 'tasa_config_gpt':
    from tasa_config_gpt import ENDPOINT, API_KEY, TUTOR_MODEL, STUDENT_MODEL
    GPT_ENDPOINT = ENDPOINT  # GPT config uses same endpoint
else:
    from tasa_config import ENDPOINT, API_KEY, TUTOR_MODEL, STUDENT_MODEL
    GPT_ENDPOINT = ENDPOINT  # Default uses same endpoint

from llm_client_unified import UnifiedLLMClient

def get_backbone_suffix():
    """æ ¹æ®TUTOR_MODELç¡®å®šbackboneåç¼€"""
    if 'llama' in TUTOR_MODEL.lower():
        return '-llama'
    elif 'qwen' in TUTOR_MODEL.lower():
        return '-qwen'
    else:
        return ''  # GPTé»˜è®¤æ— åç¼€

class VanillaICLTutor:
    def __init__(self):
        """åˆå§‹åŒ–Vanilla ICL Tutor"""
        # Tutorä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯æ”¯æŒä¸åŒbackbone (llama/qwen/gpt)
        self.tutor_client = UnifiedLLMClient(TUTOR_MODEL)
        # Student roleplayå›ºå®šä½¿ç”¨GPT endpoint
        print(f"   ğŸ” DEBUG: GPT_ENDPOINT={GPT_ENDPOINT}")
        print(f"   ğŸ” DEBUG: STUDENT_MODEL={STUDENT_MODEL}")
        self.openai_client = OpenAI(api_key=API_KEY, base_url=GPT_ENDPOINT)
        print(f"   ğŸ” DEBUG: OpenAI client base_url={self.openai_client.base_url}")
        self.model = TUTOR_MODEL
        print("ğŸ”§ åˆå§‹åŒ–Vanilla ICL Tutor")
    
    def conduct_tutoring_session(self, student_id: int, dataset: str, 
                                concept_text: str, student_system_prompt: str) -> bool:
        """
        è¿›è¡Œ10è½®æ•™å­¦å¯¹è¯
        
        Args:
            student_id: å­¦ç”ŸID
            dataset: æ•°æ®é›†
            concept_text: æ¦‚å¿µåç§°
            student_system_prompt: å­¦ç”Ÿçš„system prompt
        """
        print(f"\nğŸ“ Vanilla ICL Tutoring Session")
        print(f"   å­¦ç”ŸID: {student_id}")
        print(f"   Concept: {concept_text}")
        
        # åŠ è½½sessionè·å–persona
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        # æå–persona description
        persona_description = session['persona']['description']
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        dialogue = []
        
        # Round 1: å­¦ç”Ÿè¯·æ±‚å­¦ä¹ 
        student_request = f"I want to learn about {concept_text}"
        dialogue.append({
            "role": "user",
            "content": student_request,
            "round": 0
        })
        
        # è¿›è¡Œ10è½®æ•™å­¦
        for round_num in range(1, 11):
            print(f"ğŸ“š Round {round_num}")
            
            # æ„å»ºtutor prompt
            if round_num == 1:
                # ç¬¬ä¸€è½®ï¼šç›´æ¥ç”Ÿæˆé—®é¢˜
                prompt = f"""You are a math tutor helping a student learn {concept_text}.

Student Profile:
{persona_description}

The student wants to learn about {concept_text}. Generate your first practice question for them.

Format:
- Provide a clear question appropriate for their level
- Make it engaging and educational"""
            else:
                # åç»­è½®æ¬¡ï¼šè§£é‡Šä¸Šä¸€é¢˜ + ç”Ÿæˆæ–°é—®é¢˜
                # è·å–å­¦ç”Ÿçš„ä¸Šä¸€æ¬¡å›ç­”
                last_student_answer = dialogue[-1]['content']
                
                # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
                dialogue_context = "\n".join([
                    f"{'Student' if msg['role']=='user' else 'Tutor'}: {msg['content'][:200]}..."
                    for msg in dialogue[-4:]  # æœ€è¿‘2è½®å¯¹è¯
                ])
                
                prompt = f"""You are a math tutor helping a student learn {concept_text}.

Student Profile:
{persona_description}

Recent Dialogue:
{dialogue_context}

Student's Last Answer:
{last_student_answer}

Task:
1) Provide feedback on the student's answer (correct/incorrect with explanation)
2) Generate the next practice question to help them learn

Keep your response focused and educational."""
            
            # è°ƒç”¨LLMç”Ÿæˆtutorå›å¤
            try:
                tutor_response = self.tutor_client.chat_completion(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=500
                )
                
                if not tutor_response:
                    print(f"   âš ï¸ Tutorå›å¤ä¸ºç©ºï¼Œè·³è¿‡")
                    return False
                
                dialogue.append({
                    "role": "assistant",
                    "content": tutor_response,
                    "round": round_num
                })
                
            except Exception as e:
                print(f"   âŒ Tutorç”Ÿæˆå¤±è´¥: {e}")
                return False
            
            # å­¦ç”Ÿå›ç­”ï¼ˆå¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼‰
            if round_num < 10:
                student_prompt_text = f"""Based on the tutor's question, provide your answer.

Student Profile: {persona_description}

Tutor's Question:
{tutor_response}

Provide your answer as this student would."""
                
                try:
                    print(f"   ğŸ” DEBUG: Calling OpenAI with model={STUDENT_MODEL}, base_url={self.openai_client.base_url}")
                    response = self.openai_client.chat.completions.create(
                        model=STUDENT_MODEL,  # Student roleplayå›ºå®šä½¿ç”¨GPT
                        messages=[
                            {"role": "system", "content": student_system_prompt},
                            {"role": "user", "content": student_prompt_text}
                        ],
                        temperature=0.7,
                        max_tokens=300
                    )
                    
                    if response.choices[0].message.content:
                        student_answer = response.choices[0].message.content
                        print(f"   âœ… Studentå›ç­”æˆåŠŸ")
                    else:
                        print(f"   âš ï¸ Studentå›å¤ä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å›ç­”")
                        student_answer = "I don't know"
                    
                    dialogue.append({
                        "role": "user",
                        "content": student_answer,
                        "round": round_num + 1
                    })
                    
                except Exception as e:
                    print(f"   âŒ Studentå›ç­”å¤±è´¥: {e}")
                    print(f"   ğŸ” DEBUG: Exception type: {type(e).__name__}")
                    import traceback
                    traceback.print_exc()
                    return False
        
        # ä¿å­˜å¯¹è¯
        backbone_suffix = get_backbone_suffix()
        dialogue_dir = f'/mnt/localssd/bank/dialogue/Vanilla-ICL{backbone_suffix}/{dataset}'
        os.makedirs(dialogue_dir, exist_ok=True)
        
        dialogue_file = f'{dialogue_dir}/{student_id}-{concept_text}.json'
        
        dialogue_data = {
            "student_id": student_id,
            "dataset": dataset,
            "concept": concept_text,
            "method": "Vanilla-ICL",
            "total_rounds": 10,
            "dialogue": dialogue
        }
        
        with open(dialogue_file, 'w') as f:
            json.dump(dialogue_data, f, indent=2)
        
        print(f"   âœ… Dialogueå·²ä¿å­˜: {dialogue_file}")
        return True

if __name__ == "__main__":
    # æµ‹è¯•
    tutor = VanillaICLTutor()
    from student_roleplay_evaluation import build_student_system_prompt, load_session
    
    student_id = 1
    dataset = 'assist2017'
    
    session = load_session(f'/mnt/localssd/bank/session/{dataset}/{student_id}.json')
    concept_text = session['concept_text']
    student_prompt = build_student_system_prompt(session)
    
    tutor.conduct_tutoring_session(student_id, dataset, concept_text, student_prompt)

