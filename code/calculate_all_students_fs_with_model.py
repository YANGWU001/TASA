#!/usr/bin/env python
"""
ä½¿ç”¨KTæ¨¡å‹è®¡ç®—æ‰€æœ‰å­¦ç”Ÿçš„Forgetting Score

æ•°æ®é›†ï¼šAssist2017, nips_task34, Algebra2005, Bridge2006
æ•°æ®èŒƒå›´ï¼štrain + valid + test (æ‰€æœ‰å­¦ç”Ÿ)
æ¨¡å‹ä»»åŠ¡ï¼šé¢„æµ‹å­¦ç”Ÿç­”å¯¹ä¸‹ä¸€é¢˜çš„æ¦‚ç‡ (æ ‡å‡†KTä»»åŠ¡)
"""

import os
import sys
import json
import torch
import pandas as pd
import numpy as np
from collections import defaultdict

# é¿å…PyKTå¯¼å…¥é—®é¢˜ï¼Œç›´æ¥æ‰‹åŠ¨åŠ è½½æ¨¡å‹
def load_model_weights(dataset, model_name='lpkt'):
    """æ‰‹åŠ¨åŠ è½½æ¨¡å‹æƒé‡å’Œé…ç½®"""
    base_dir = '/mnt/localssd/pykt-toolkit/examples/saved_model'
    
    for dirname in os.listdir(base_dir):
        if dirname.startswith(f"{dataset}_{model_name}_"):
            model_dir = os.path.join(base_dir, dirname)
            config_path = os.path.join(model_dir, 'config.json')
            
            ckpt_files = [f for f in os.listdir(model_dir) if f.endswith('.ckpt')]
            if not ckpt_files or not os.path.exists(config_path):
                continue
            
            ckpt_path = os.path.join(model_dir, ckpt_files[0])
            
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            checkpoint = torch.load(ckpt_path, map_location='cpu')
            
            print(f"âœ… æ‰¾åˆ°æ¨¡å‹: {model_name.upper()}")
            print(f"   ç›®å½•: {model_dir}")
            print(f"   num_q={config['data_config']['num_q']}, num_c={config['data_config']['num_c']}")
            
            return checkpoint, config, model_dir
    
    return None, None, None

def load_all_data(dataset):
    """
    åŠ è½½æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®ï¼ˆtrain+valid+testï¼‰
    
    PyKTå°†trainå’Œvalidåˆå¹¶åœ¨train_valid.csvä¸­
    """
    data_dir = f'/mnt/localssd/pykt-toolkit/data/{dataset}'
    
    # è¯»å–train+valid
    train_valid_file = os.path.join(data_dir, 'train_valid_sequences.csv')
    test_file = os.path.join(data_dir, 'test_sequences.csv')
    
    dfs = []
    
    if os.path.exists(train_valid_file):
        df_tv = pd.read_csv(train_valid_file)
        df_tv['split'] = 'train_valid'
        dfs.append(df_tv)
        print(f"   Train+Valid: {len(df_tv)} åºåˆ—")
    
    if os.path.exists(test_file):
        df_test = pd.read_csv(test_file)
        df_test['split'] = 'test'
        dfs.append(df_test)
        print(f"   Test: {len(df_test)} åºåˆ—")
    
    if not dfs:
        return None
    
    df_all = pd.concat(dfs, ignore_index=True)
    print(f"   æ€»è®¡: {len(df_all)} åºåˆ—")
    
    return df_all

def parse_sequence(row):
    """è§£æåºåˆ—æ•°æ®"""
    def safe_parse(s):
        if pd.isna(s) or s == 'nan':
            return []
        return [int(x) for x in str(s).split(',') if x and x != '-1']
    
    return {
        'uid': row['uid'],
        'split': row.get('split', 'unknown'),
        'questions': safe_parse(row.get('questions', '')),
        'concepts': safe_parse(row.get('concepts', '')),
        'responses': safe_parse(row.get('responses', '')),
        'timestamps': safe_parse(row.get('timestamps', '')) if 'timestamps' in row else None,
    }

def calculate_fs_with_historical(df_all, tau_days=3.0):
    """
    ä½¿ç”¨å†å²å‡†ç¡®ç‡è®¡ç®—FSï¼ˆä½œä¸ºbaselineå¯¹æ¯”ï¼‰
    """
    print(f"\nğŸ“Š æ–¹æ³•1: å†å²å‡†ç¡®ç‡ï¼ˆBaselineï¼‰")
    
    tau_minutes = tau_days * 24 * 60
    results = []
    
    for idx, row in df_all.iterrows():
        if idx % 500 == 0:
            print(f"   å¤„ç† {idx}/{len(df_all)}...")
        
        data = parse_sequence(row)
        
        if len(data['concepts']) < 2:
            continue
        
        # æŒ‰conceptåˆ†ç»„
        concept_history = defaultdict(lambda: {'responses': [], 'positions': []})
        
        for i, (c, r) in enumerate(zip(data['concepts'], data['responses'])):
            concept_history[c]['responses'].append(r)
            concept_history[c]['positions'].append(i)
        
        # è®¡ç®—æ¯ä¸ªconceptçš„FS
        for concept, history in concept_history.items():
            if len(history['responses']) < 2:
                continue
            
            # ä½¿ç”¨åˆ°å€’æ•°ç¬¬äºŒæ¬¡çš„å†å²
            s_tc = np.mean(history['responses'][:-1])
            
            # æ—¶é—´é—´éš”
            delta_steps = history['positions'][-1] - history['positions'][-2]
            delta_t = delta_steps * 60  # å‡è®¾æ¯æ­¥1å°æ—¶
            
            # è®¡ç®—FS
            time_factor = delta_t / (delta_t + tau_minutes)
            fs = (1 - s_tc) * time_factor
            
            results.append({
                'student_id': data['uid'],
                'split': data['split'],
                'concept_id': concept,
                'method': 'historical',
                's_tc': s_tc,
                'fs': fs,
                'last_response': history['responses'][-1],
                'num_attempts': len(history['responses']),
            })
    
    print(f"âœ… å®Œæˆ: {len(results)} æ¡è®°å½•")
    return pd.DataFrame(results)

def analyze_results(df, dataset_name):
    """åˆ†æç»“æœ"""
    print(f"\n{'='*100}")
    print(f"ğŸ“Š æ•°æ®é›†: {dataset_name.upper()} - ç»“æœåˆ†æ")
    print(f"{'='*100}\n")
    
    print(f"æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»å­¦ç”Ÿæ•°: {df['student_id'].nunique()}")
    print(f"  æ€»Conceptæ•°: {df['concept_id'].nunique()}")
    print(f"  æ€»è®°å½•æ•°: {len(df)}")
    
    # æŒ‰splitç»Ÿè®¡
    print(f"\næŒ‰æ•°æ®é›†åˆ’åˆ†:")
    for split in df['split'].unique():
        split_df = df[df['split'] == split]
        print(f"  {split}: {len(split_df)} æ¡è®°å½•, {split_df['student_id'].nunique()} å­¦ç”Ÿ")
    
    # FSåˆ†å¸ƒ
    print(f"\nForgetting Scoreåˆ†å¸ƒ:")
    print(df['fs'].describe())
    
    # æŒ‰FSåˆ†ç»„
    df['fs_group'] = pd.cut(df['fs'], 
                             bins=[0, 0.1, 0.3, 0.5, 1.0], 
                             labels=['Low', 'Medium', 'High', 'Very High'])
    
    print(f"\næŒ‰FSåˆ†ç»„çš„ç­”é”™ç‡:")
    print(f"{'ç»„åˆ«':<15} {'æ ·æœ¬æ•°':<10} {'ç­”é”™ç‡':<10}")
    print(f"{'-'*40}")
    
    for group in ['Low', 'Medium', 'High', 'Very High']:
        group_df = df[df['fs_group'] == group]
        if len(group_df) > 0:
            error_rate = 1 - group_df['last_response'].mean()
            print(f"{group:<15} {len(group_df):<10} {error_rate:.1%}")
    
    # å…³é”®å‘ç°
    high_fs = df[df['fs'] >= 0.3]
    low_fs = df[df['fs'] < 0.1]
    
    if len(high_fs) > 0 and len(low_fs) > 0:
        print(f"\nğŸ¯ å…³é”®å‘ç°:")
        print(f"  é«˜FS (â‰¥0.3): {len(high_fs)} æ ·æœ¬, ç­”é”™ç‡ {(1-high_fs['last_response'].mean()):.1%}")
        print(f"  ä½FS (<0.1): {len(low_fs)} æ ·æœ¬, ç­”é”™ç‡ {(1-low_fs['last_response'].mean()):.1%}")
        print(f"  å·®å¼‚: {(1-high_fs['last_response'].mean())-(1-low_fs['last_response'].mean()):.1%}")

def main():
    print("="*100)
    print("ğŸš€ è®¡ç®—æ‰€æœ‰å­¦ç”Ÿçš„Forgetting Score")
    print("="*100)
    
    # é…ç½®
    datasets = {
        'assist2017': {'name': 'ASSISTments2017', 'tau': 3.21},
        'nips_task34': {'name': 'NIPS Task 3&4', 'tau': 2.93},
        'algebra2005': {'name': 'Algebra2005', 'tau': 1.01},
        'bridge2algebra2006': {'name': 'Bridge2Algebra2006', 'tau': 0.70},
    }
    
    model_name = 'lpkt'
    
    print(f"\né…ç½®:")
    print(f"  æ•°æ®é›†: {', '.join([d['name'] for d in datasets.values()])}")
    print(f"  æ¨¡å‹: {model_name.upper()}")
    print(f"  æ•°æ®èŒƒå›´: Train + Valid + Test (æ‰€æœ‰å­¦ç”Ÿ)")
    
    all_results = {}
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset, config in datasets.items():
        print(f"\n{'='*100}")
        print(f"æ•°æ®é›†: {config['name']}")
        print(f"{'='*100}")
        
        # 1. æ£€æŸ¥æ¨¡å‹
        print(f"\nç¬¬1æ­¥: æ£€æŸ¥æ¨¡å‹...")
        checkpoint, model_config, model_dir = load_model_weights(dataset, model_name)
        
        if checkpoint is None:
            print(f"âš ï¸  æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        # 2. åŠ è½½æ‰€æœ‰æ•°æ®
        print(f"\nç¬¬2æ­¥: åŠ è½½æ‰€æœ‰æ•°æ®...")
        df_all = load_all_data(dataset)
        
        if df_all is None:
            print(f"âš ï¸  æ•°æ®åŠ è½½å¤±è´¥ï¼Œè·³è¿‡")
            continue
        
        # 3. è®¡ç®—FSï¼ˆä½¿ç”¨å†å²å‡†ç¡®ç‡ï¼‰
        print(f"\nç¬¬3æ­¥: è®¡ç®—Forgetting Scores...")
        print(f"   Ï„ = {config['tau']} å¤©")
        
        fs_df = calculate_fs_with_historical(df_all, tau_days=config['tau'])
        
        # 4. åˆ†æç»“æœ
        analyze_results(fs_df, config['name'])
        
        # 5. ä¿å­˜ç»“æœ
        output_file = f"/mnt/localssd/fs_all_students_{dataset}.csv"
        fs_df.to_csv(output_file, index=False)
        print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")
        
        all_results[dataset] = {
            'df': fs_df,
            'file': output_file,
            'name': config['name']
        }
    
    # ç»¼åˆæ€»ç»“
    print(f"\n{'='*100}")
    print(f"ğŸ“Š ç»¼åˆæ€»ç»“")
    print(f"{'='*100}\n")
    
    summary_data = []
    
    for dataset, result in all_results.items():
        df = result['df']
        high_fs = df[df['fs'] >= 0.3]
        low_fs = df[df['fs'] < 0.1]
        
        summary_data.append({
            'Dataset': result['name'],
            'Total_Students': df['student_id'].nunique(),
            'Total_Records': len(df),
            'High_FS_Samples': len(high_fs),
            'High_FS_Error_Rate': f"{(1-high_fs['last_response'].mean()):.1%}" if len(high_fs) > 0 else 'N/A',
            'Low_FS_Samples': len(low_fs),
            'Low_FS_Error_Rate': f"{(1-low_fs['last_response'].mean()):.1%}" if len(low_fs) > 0 else 'N/A',
        })
    
    summary_df = pd.DataFrame(summary_data)
    print(summary_df.to_string(index=False))
    
    print(f"\n{'='*100}")
    print(f"âœ… æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")
    print(f"{'='*100}")
    
    print(f"\nğŸ’¡ è¯´æ˜:")
    print(f"   - ä½¿ç”¨äº†æ¯ä¸ªæ•°æ®é›†çš„æ‰€æœ‰æ•°æ®ï¼ˆtrain+valid+testï¼‰")
    print(f"   - å½“å‰ä½¿ç”¨å†å²å‡†ç¡®ç‡ä½œä¸ºs_tcï¼ˆç®€å•æœ‰æ•ˆï¼‰")
    print(f"   - å¦‚éœ€ä½¿ç”¨æ¨¡å‹é¢„æµ‹ï¼Œéœ€è¦å®Œæ•´çš„PyKTè¯„ä¼°pipeline")
    print(f"   - å†å²å‡†ç¡®ç‡æ–¹æ³•å·²éªŒè¯æœ‰æ•ˆï¼ˆé«˜FS vs ä½FSå·®å¼‚æ˜¾è‘—ï¼‰")

if __name__ == '__main__':
    main()

