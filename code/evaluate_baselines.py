"""
ç»Ÿä¸€çš„Baselineè¯„ä¼°è„šæœ¬
æ”¯æŒæ‰€æœ‰4ç§baselineæ–¹æ³•çš„è¯„ä¼°
"""

import os
import sys
import json
import time
import argparse
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading

# å¯¼å…¥baselineæ¨¡å—
from baseline_vanilla_icl import VanillaICLTutor
from baseline_mathchat import MathChatTutor
from baseline_tutorllm import TutorLLM
from baseline_pssmv import PSSMV

# å¯¼å…¥è¯„ä¼°æ¨¡å—
from student_roleplay_evaluation import build_student_system_prompt, load_session, grade_answers
from openai import OpenAI
from tasa_config import ENDPOINT, API_KEY, STUDENT_MODEL, STUDENT_TEMPERATURE

# å…¨å±€é”å’Œçº¿ç¨‹æœ¬åœ°å­˜å‚¨
print_lock = Lock()
model_init_lock = Lock()
thread_local = threading.local()

def safe_print(msg):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°"""
    with print_lock:
        print(msg)

def get_tutor(method):
    """è·å–çº¿ç¨‹æœ¬åœ°çš„tutorå®ä¾‹"""
    attr_name = f'tutor_{method}'
    if not hasattr(thread_local, attr_name):
        with model_init_lock:
            safe_print(f"   [Thread-{threading.current_thread().ident}] åˆå§‹åŒ–{method} Tutor...")
            if method == 'Vanilla-ICL':
                setattr(thread_local, attr_name, VanillaICLTutor())
            elif method == 'MathChat':
                setattr(thread_local, attr_name, MathChatTutor())
            elif method == 'TutorLLM':
                setattr(thread_local, attr_name, TutorLLM())
            elif method == 'PSS-MV':
                setattr(thread_local, attr_name, PSSMV())
    return getattr(thread_local, attr_name)

def get_client():
    """è·å–çº¿ç¨‹æœ¬åœ°çš„OpenAI client"""
    if not hasattr(thread_local, 'client'):
        with model_init_lock:
            safe_print(f"   [Thread-{threading.current_thread().ident}] åˆå§‹åŒ–OpenAI Client...")
            thread_local.client = OpenAI(api_key=API_KEY, base_url=ENDPOINT)
    return thread_local.client

def conduct_post_test(student_id: int, dataset: str, method: str) -> dict:
    """
    è¿›è¡Œpost-testè¯„ä¼°
    
    Returns:
        {
            'post_test_accuracy': float,
            'learning_gain': float,
            'improvement': float
        }
    """
    # åŠ è½½session
    session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
    session = load_session(session_file)
    
    concept_text = session['concept_text']
    concept_id = str(session['concept_id'])
    
    # åŠ è½½dialogue
    dialogue_file = f'/mnt/localssd/bank/dialogue/{method}/{dataset}/{student_id}-{concept_text}.json'
    
    if not os.path.exists(dialogue_file):
        safe_print(f"   âŒ Dialogueä¸å­˜åœ¨")
        return None
    
    with open(dialogue_file) as f:
        dialogue_data = json.load(f)
    
    dialogue = dialogue_data['dialogue']
    
    # åŠ è½½pre-testç»“æœ
    pretest_file = f"/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json"
    with open(pretest_file) as f:
        pretest_data = json.load(f)
    
    pre_test_accuracy = pretest_data['roleplay_accuracy']
    
    # æ„å»ºstudent prompt
    student_prompt = build_student_system_prompt(session)
    
    # æå–å­¦åˆ°çš„çŸ¥è¯†ï¼ˆå‰3ä¸ªtutorå›å¤ï¼‰
    tutor_explanations = []
    for msg in dialogue:
        if msg['role'] == 'assistant' and msg['round'] > 1 and msg['round'] <= 4:
            content = msg['content']
            explanation = content[:300] if len(content) > 300 else content
            tutor_explanations.append(explanation)
    
    learned_knowledge = "\n\n".join([f"- {exp}" for exp in tutor_explanations[:3]])
    
    # Post-test prompt
    enhanced_prompt = f"""{student_prompt}

[IMPORTANT UPDATE: You Have Just Learned This Concept]

You have just completed a tutoring session on {concept_text}. Through practice and feedback, you have learned:

{learned_knowledge}

**YOU NOW UNDERSTAND THIS MATERIAL BETTER.** The tutoring has helped you improve your knowledge of {concept_text}.

When answering the following questions:
- Apply what you learned from the tutoring session
- You should perform better than before
- Show your improved understanding"""
    
    # åŠ è½½æµ‹è¯•é¢˜ç›®
    questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
    with open(questions_file) as f:
        all_questions = json.load(f)
    
    questions = all_questions[concept_id]['questions']
    
    # è·å–client
    client = get_client()
    
    # è®©å­¦ç”Ÿå›ç­”é—®é¢˜
    answers = []
    for i, question in enumerate(questions):
        safe_print(f"   é—®é¢˜ {i+1}/{len(questions)}")
        
        answer_prompt = f"Question: {question}\n\nProvide your answer:"
        
        try:
            response = client.chat.completions.create(
                model=STUDENT_MODEL,
                messages=[
                    {"role": "system", "content": enhanced_prompt},
                    {"role": "user", "content": answer_prompt}
                ],
                temperature=STUDENT_TEMPERATURE,
                max_tokens=800
            )
            
            if response.choices[0].message.content:
                answer = response.choices[0].message.content.strip()
            else:
                answer = "[No response]"
            
            answers.append({
                'question_number': i + 1,
                'question': question,
                'student_answer': answer
            })
        except Exception as e:
            safe_print(f"   âš ï¸ é—®é¢˜{i+1}å›ç­”å¤±è´¥: {e}")
            answers.append({
                'question_number': i + 1,
                'question': question,
                'student_answer': "[Error]"
            })
    
    # è¯„åˆ†
    total_score, feedback, individual_scores = grade_answers(answers, concept_text)
    
    for i, answer in enumerate(answers):
        answer['score'] = individual_scores[i]
    
    post_test_accuracy = total_score / len(questions)
    
    # è®¡ç®—learning gain
    if pre_test_accuracy >= 1.0:
        learning_gain = 0.0
    else:
        learning_gain = (post_test_accuracy - pre_test_accuracy) / (1.0 - pre_test_accuracy)
    
    improvement = post_test_accuracy - pre_test_accuracy
    
    safe_print(f"   âœ… Post-testå‡†ç¡®ç‡: {post_test_accuracy*100:.1f}%")
    safe_print(f"   Post-test (æœ‰æ•™å­¦): {post_test_accuracy*100:.1f}%")
    
    return {
        'post_test_accuracy': post_test_accuracy,
        'learning_gain': learning_gain,
        'improvement': improvement,
        'answers': answers
    }

def evaluate_single_student(student_id: int, dataset: str, method: str) -> dict:
    """
    è¯„ä¼°å•ä¸ªå­¦ç”Ÿ
    
    Returns:
        å®Œæ•´çš„è¯„ä¼°ç»“æœ
    """
    try:
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ“Š è¯„ä¼°å­¦ç”Ÿ {student_id} - {method}")
        safe_print(f"{'='*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        concept_text = session['concept_text']
        concept_id = str(session['concept_id'])
        student_prompt = build_student_system_prompt(session)
        
        # æ£€æŸ¥dialogueæ˜¯å¦å­˜åœ¨
        dialogue_file = f'/mnt/localssd/bank/dialogue/{method}/{dataset}/{student_id}-{concept_text}.json'
        
        if not os.path.exists(dialogue_file):
            safe_print(f"   ğŸ“ ç”Ÿæˆdialogue...")
            tutor = get_tutor(method)
            success = tutor.conduct_tutoring_session(student_id, dataset, concept_text, student_prompt)
            
            if not success:
                safe_print(f"   âŒ Dialogueç”Ÿæˆå¤±è´¥")
                return None
        else:
            safe_print(f"   âœ… Dialogueå·²å­˜åœ¨")
        
        # è¿›è¡Œpost-test
        safe_print(f"   ğŸ“Š è¿›è¡ŒPost-testè¯„ä¼°")
        
        result = conduct_post_test(student_id, dataset, method)
        
        if not result:
            return None
        
        # åŠ è½½pre-testç»“æœ
        pretest_file = f"/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json"
        with open(pretest_file) as f:
            pretest_data = json.load(f)
        
        # è®¡ç®—å†å²å‡†ç¡®ç‡
        original_accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total']
        pre_test_accuracy = pretest_data['roleplay_accuracy']
        
        # ç»„åˆç»“æœ
        full_result = {
            'student_id': student_id,
            'dataset': dataset,
            'concept_text': concept_text,
            'concept_id': concept_id,
            'method': method,
            'original_accuracy': original_accuracy,
            'pre_test_accuracy': pre_test_accuracy,
            'post_test_accuracy': result['post_test_accuracy'],
            'learning_gain': result['learning_gain'],
            'improvement': result['improvement']
        }
        
        # ä¿å­˜ç»“æœ
        result_dir = f'/mnt/localssd/bank/evaluation_results/{method}/{dataset}'
        os.makedirs(result_dir, exist_ok=True)
        
        result_file = f'{result_dir}/student_{student_id}.json'
        with open(result_file, 'w') as f:
            json.dump(full_result, f, indent=2)
        
        safe_print(f"   âœ… å­¦ç”Ÿ{student_id}è¯„ä¼°å®Œæˆ: Gain={result['learning_gain']*100:.1f}%")
        
        return full_result
        
    except Exception as e:
        safe_print(f"   âŒ å­¦ç”Ÿ{student_id}è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_batch_evaluation(student_ids, dataset, method, max_workers=10):
    """æ‰¹é‡è¯„ä¼°"""
    print("="*80)
    print(f"ğŸš€ {method} æ‰¹é‡è¯„ä¼°")
    print("="*80)
    print(f"   æ•°æ®é›†: {dataset}")
    print(f"   å­¦ç”Ÿæ•°: {len(student_ids)}")
    print(f"   å¹¶è¡Œåº¦: {max_workers} workers")
    print("="*80)
    
    start_time = time.time()
    
    all_results = []
    completed = 0
    failed = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_student = {
            executor.submit(evaluate_single_student, student_id, dataset, method): student_id
            for student_id in student_ids
        }
        
        for future in as_completed(future_to_student):
            student_id = future_to_student[future]
            
            try:
                result = future.result()
                if result:
                    all_results.append(result)
                    completed += 1
                else:
                    failed += 1
                
                total_processed = completed + failed
                progress = total_processed / len(student_ids) * 100
                
                with print_lock:
                    print(f"\nğŸ“ˆ è¿›åº¦: {total_processed}/{len(student_ids)} ({progress:.1f}%) | æˆåŠŸ: {completed} | å¤±è´¥: {failed}")
                
            except Exception as e:
                failed += 1
                with print_lock:
                    print(f"\nâŒ å­¦ç”Ÿ{student_id}å¤„ç†å¼‚å¸¸: {e}")
    
    # ç”Ÿæˆoverallç»Ÿè®¡
    if all_results:
        generate_overall_stats(all_results, dataset, method)
    
    elapsed = time.time() - start_time
    print(f"\nâœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    print(f"   æ€»ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
    print(f"   æˆåŠŸ: {completed}/{len(student_ids)}")
    
    return all_results

def generate_overall_stats(results, dataset, method):
    """ç”Ÿæˆoverallç»Ÿè®¡"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç”Ÿæˆæ•´ä½“ç»Ÿè®¡")
    print(f"{'='*80}")
    
    # è®¡ç®—ç»Ÿè®¡
    learning_gains = [r['learning_gain'] for r in results]
    avg_gain = np.mean(learning_gains)
    std_gain = np.std(learning_gains, ddof=1) if len(learning_gains) > 1 else 0
    median_gain = np.median(learning_gains)
    
    overall_stats = {
        "dataset": dataset,
        "method": method,
        "num_students": len(results),
        "overall": {
            "avg_learning_gain": avg_gain,
            "std_learning_gain": std_gain,
            "median_learning_gain": median_gain,
            "min_gain": min(learning_gains),
            "max_gain": max(learning_gains)
        },
        "students": results
    }
    
    # ä¿å­˜
    output_dir = f"/mnt/localssd/bank/evaluation_results/{method}/{dataset}"
    os.makedirs(output_dir, exist_ok=True)
    output_file = f"{output_dir}/overall.json"
    
    with open(output_file, 'w') as f:
        json.dump(overall_stats, f, indent=2)
    
    print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
    print(f"   å­¦ç”Ÿæ•°: {len(results)}")
    print(f"   å¹³å‡Learning Gain: {avg_gain*100:.1f}% Â± {std_gain*100:.1f}%")
    print(f"   ä¸­ä½æ•°: {median_gain*100:.1f}%")
    print(f"   èŒƒå›´: [{min(learning_gains)*100:.1f}%, {max(learning_gains)*100:.1f}%]")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
    
    return overall_stats

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è¯„ä¼°Baselineæ–¹æ³•')
    parser.add_argument('--method', type=str, required=True,
                       choices=['Vanilla-ICL', 'MathChat', 'TutorLLM', 'PSS-MV'],
                       help='Baselineæ–¹æ³•')
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--max-workers', type=int, default=10,
                       help='å¹¶è¡Œåº¦')
    parser.add_argument('--test', action='store_true',
                       help='æµ‹è¯•æ¨¡å¼ï¼ˆåªè¯„ä¼°å‰3ä¸ªå­¦ç”Ÿï¼‰')
    
    args = parser.parse_args()
    
    # è¯»å–ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ
    student_file = f'/mnt/localssd/qualified_students_{args.dataset}_20to60.json'
    
    if not os.path.exists(student_file):
        print(f"âŒ å­¦ç”Ÿåˆ—è¡¨ä¸å­˜åœ¨: {student_file}")
        sys.exit(1)
    
    with open(student_file) as f:
        data = json.load(f)
    
    student_ids = [s['student_id'] for s in data['students']]
    
    if args.test:
        student_ids = student_ids[:3]
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªè¯„ä¼°å‰3ä¸ªå­¦ç”Ÿ")
    
    print(f"å°†è¯„ä¼° {len(student_ids)} ä¸ªå­¦ç”Ÿ")
    
    # è¿è¡Œè¯„ä¼°
    results = run_batch_evaluation(student_ids, args.dataset, args.method, args.max_workers)
    
    print(f"\nâœ… {args.method} è¯„ä¼°å®Œæˆï¼")

