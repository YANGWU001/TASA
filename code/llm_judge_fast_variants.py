#!/usr/bin/env python3
"""
LLM as Judge: è¯„ä¼°dialogueçš„ä¸ªæ€§åŒ–ç¨‹åº¦
æ¯”è¾ƒtarget method vs Vanilla-ICL baseline
"""

import json
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from openai import OpenAI
import numpy as np
import random

# Judge modelé…ç½®
JUDGE_MODEL = "gpt-5-chat"
API_KEY = os.getenv("API_KEY", "")
ENDPOINT = os.getenv("ENDPOINT", "")

# å…¨å±€é”
print_lock = Lock()

def safe_print(msg):
    with print_lock:
        print(msg)

def get_backbone(method_name):
    """ä»methodåç§°ä¸­æå–backbone"""
    if '-llama' in method_name:
        return 'llama'
    elif '-qwen' in method_name:
        return 'qwen'
    else:
        return 'gpt'

def load_persona(student_id, dataset):
    """åŠ è½½å­¦ç”Ÿpersona"""
    persona_file = f'/mnt/localssd/bank/persona/{dataset}/data/{student_id}.json'
    try:
        with open(persona_file) as f:
            personas = json.load(f)
        # æå–description
        persona_texts = [p['description'] for p in personas]
        return "\n".join([f"- {p}" for p in persona_texts])
    except:
        return "No persona data available."

def load_memory(student_id, dataset):
    """åŠ è½½å­¦ç”Ÿmemory"""
    memory_file = f'/mnt/localssd/bank/memory/{dataset}/data/{student_id}.json'
    try:
        with open(memory_file) as f:
            memories = json.load(f)
        # æå–description
        memory_texts = [m['description'] for m in memories]
        return "\n".join([f"- {m}" for m in memory_texts[:10]])  # é™åˆ¶å‰10æ¡
    except:
        return "No memory data available."

def load_dialogue(method, dataset, student_id, concept_text):
    """åŠ è½½dialogue"""
    dialogue_file = f'/mnt/localssd/bank/dialogue/{method}/{dataset}/{student_id}-{concept_text}.json'
    
    if not os.path.exists(dialogue_file):
        return None
    
    try:
        with open(dialogue_file) as f:
            data = json.load(f)
        
        dialogue = data.get('dialogue', data) if isinstance(data, dict) else data
        
        # æ ¼å¼åŒ–ä¸º student/tutorå¯¹è¯
        formatted_dialogue = []
        for msg in dialogue:
            role = "Student" if msg['role'] == 'user' else "Tutor"
            formatted_dialogue.append(f"{role}: {msg['content']}")
        
        return "\n\n".join(formatted_dialogue)
    except Exception as e:
        safe_print(f"   âš ï¸ åŠ è½½dialogueå¤±è´¥ ({method}/{dataset}/{student_id}): {e}")
        return None

def create_judge_prompt(persona, memory, target_dialogue, baseline_dialogue, target_method, baseline_method):
    """åˆ›å»ºjudge prompt - è¶…ä¸¥æ ¼å¹³è¡¡ç‰ˆæœ¬"""
    prompt = f"""You are an EXTREMELY CRITICAL educational AI evaluator. Your primary goal is to identify which dialogue **demonstrably produces better learning outcomes**, not which one mentions more student data.

**Student Profile (Persona):**
{persona}

**Student's Past Learning History (Memory):**
{memory}

**Dialogue A ({target_method}):**
{target_dialogue}

**Dialogue B ({baseline_method}):**
{baseline_dialogue}

---

**Evaluation Criteria:**

1. **Learning Effectiveness** (50% weight)
   - Does the dialogue help THIS specific student learn THIS concept effectively?
   - Are explanations clear and appropriate for this student's level?
   - Does it address the student's specific needs and challenges?

2. **Personalization Value** (35% weight)
   - Does the dialogue adapt to this student's profile (knowledge, weaknesses, learning style)?
   - Are references to student history used to improve teaching (not just mentioned)?
   - Does it build on student's strengths or address their specific gaps?
   - **KEY**: If personalization demonstrably improves the learning experience, favor it

3. **Teaching Quality** (15% weight)
   - Is the pedagogy sound (scaffolding, examples, feedback)?
   - Is the dialogue efficient and well-paced?

---

**Critical Decision Rules:**

âš ï¸ **You MUST choose "Tie" if (expect ~15-25% Ties):**
- Both dialogues achieve similar learning outcomes AND similar teaching quality
- One has slightly better personalization but the other has slightly better explanations (trade-offs cancel out)
- The "personalized" dialogue is overly repetitive/verbose, negating its personalization benefits
- Both are generic OR both are similarly personalized
- **When truly equal in overall effectiveness, choose Tie**

âœ… **Choose a winner if (should be ~60-80% of comparisons):**
- One has **meaningfully better learning outcomes** for THIS student
- One demonstrates **effective personalization** that improves teaching (not just mentions data)
- One has significantly clearer explanations OR better pedagogical approach
- One adapts to student's level/needs while the other is generic
- One builds on student strengths/addresses weaknesses effectively
- The quality difference is **clear and meaningful** (doesn't need to be huge)
- **If one dialogue is noticeably more effective for this student, choose it**

â›” **Baseline should win if:**
- It teaches more clearly/efficiently despite being less "personalized"
- The "personalized" dialogue is confusing or overly complex
- Personalization adds verbosity without improving learning

â›” **Do NOT favor a dialogue just because:**
- It references student profile/memory extensively (DATA â‰  PERSONALIZATION)
- It mentions past struggles/strengths (MENTIONING â‰  USING EFFECTIVELY)
- It has longer responses or more sophisticated language
- It seems to "try harder" without demonstrable learning benefit
- It sounds more "personalized" but achieves similar outcomes
- **CRITICAL RED FLAG**: Extensive data references but generic teaching = SUPERFICIAL

**What counts as EFFECTIVE personalization (should win):**
1. Adapts difficulty/pacing based on student's demonstrated abilities
2. Uses examples that connect to student's known strengths
3. Addresses specific misconceptions from student's history
4. Scaffolds learning by referencing what student already knows well
5. Adjusts teaching strategy based on student's learning patterns

**What is SUPERFICIAL (should be Tie or lose):**
1. Only mentions student data but teaches generically
2. Lists weaknesses without adapting teaching approach
3. Verbose explanations that don't improve learning

**Key principle:** If personalization **demonstrably helps THIS student learn better**, favor it. If it's just mentions without impact, don't.

---

**Your Response Format:**

**Winner: [Dialogue A / Dialogue B / Tie]**

**Reasoning:**
[4-5 sentences. For EACH dialogue, comment on: (1) instructional quality, (2) whether personalization actually helps learning. Then explain your decision with specific examples.]

**Instructional Quality: A [X/10], B [X/10]**
**Personalization Impact: A [X/10], B [X/10]**
**Overall Score: A [X/10], B [X/10]**

**Confidence: [High / Medium / Low]**"""

    return prompt

def judge_comparison(student_id, dataset, concept_text, target_method, baseline_method):
    """ä½¿ç”¨LLM judgeæ¯”è¾ƒä¸¤ä¸ªdialogue"""
    try:
        # åŠ è½½personaå’Œmemory
        persona = load_persona(student_id, dataset)
        memory = load_memory(student_id, dataset)
        
        # åŠ è½½ä¸¤ä¸ªdialogue
        target_dialogue = load_dialogue(target_method, dataset, student_id, concept_text)
        baseline_dialogue = load_dialogue(baseline_method, dataset, student_id, concept_text)
        
        if not target_dialogue or not baseline_dialogue:
            safe_print(f"   âš ï¸ å­¦ç”Ÿ{student_id}çš„dialogueç¼ºå¤±")
            return None
        
        # åˆ›å»ºjudge prompt
        prompt = create_judge_prompt(
            persona, memory, target_dialogue, baseline_dialogue,
            target_method, baseline_method
        )
        
        # è°ƒç”¨judge model
        client = OpenAI(api_key=API_KEY, base_url=ENDPOINT)
        response = client.chat.completions.create(
            model=JUDGE_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=1.0  # gpt-5åªæ”¯æŒtemperature=1
            # ä¸è®¾ç½®max_tokensï¼Œå…è®¸å®Œæ•´è¾“å‡º
        )
        
        judgment = response.choices[0].message.content
        
        # è§£æç»“æœï¼ˆæ”¯æŒä¸¤ç§æ ¼å¼: "Winner: A" å’Œ "Winner: Dialogue A"ï¼‰
        winner = None
        winner_match = re.search(r'\*\*Winner:\s*\[?(.+?)\]?\*\*', judgment)
        if not winner_match:
            winner_match = re.search(r'Winner:\s*\[?(.+?)\]?(?:\n|\*\*)', judgment)
        
        if winner_match:
            winner_text = winner_match.group(1).strip().lower()
            if 'dialogue a' in winner_text or winner_text == 'a':
                winner = "target"
            elif 'dialogue b' in winner_text or winner_text == 'b':
                winner = "baseline"
            elif 'tie' in winner_text:
                winner = "tie"
        
        # å°è¯•æå–Overall Scoresï¼ˆæ–°æ ¼å¼ï¼‰æˆ–Personalization Scoresï¼ˆæ—§æ ¼å¼ï¼‰
        score_a = None
        score_b = None
        
        # ä¼˜å…ˆåŒ¹é…æ–°æ ¼å¼: "Overall Score: A [X/10], B [X/10]"
        overall_match = re.search(r'Overall Score:\s*A\s*\[?(\d+(?:\.\d+)?)/10\]?,?\s*B\s*\[?(\d+(?:\.\d+)?)/10\]?', judgment)
        if overall_match:
            score_a = float(overall_match.group(1))
            score_b = float(overall_match.group(2))
        else:
            # å›é€€åˆ°æ—§æ ¼å¼: "Personalization Score A: X/10" æˆ– "**Personalization Score A: X/10**"
            match_a = re.search(r'Personalization Score A:\s*(\d+(?:\.\d+)?)/10', judgment)
            match_b = re.search(r'Personalization Score B:\s*(\d+(?:\.\d+)?)/10', judgment)
            
            if match_a:
                score_a = float(match_a.group(1))
            if match_b:
                score_b = float(match_b.group(1))
        
        return {
            'student_id': student_id,
            'concept_text': concept_text,
            'winner': winner,
            'score_a': score_a,
            'score_b': score_b,
            'judgment': judgment
        }
        
    except Exception as e:
        safe_print(f"   âŒ è¯„ä¼°å­¦ç”Ÿ{student_id}å¤±è´¥: {e}")
        return None

def batch_judge(target_method, dataset='assist2017', max_workers=20):
    """æ‰¹é‡è¯„ä¼°ä¸€ä¸ªtarget method vs baseline"""
    
    # ç¡®å®šbackbone
    target_backbone = get_backbone(target_method)
    
    # ç¡®å®šbaseline method
    if target_backbone == 'llama':
        baseline_method = 'Vanilla-ICL-llama'
    elif target_backbone == 'qwen':
        baseline_method = 'Vanilla-ICL-qwen'
    else:
        baseline_method = 'Vanilla-ICL'
    
    safe_print(f"\n{'='*80}")
    safe_print(f"ğŸ“Š LLM as Judge: {target_method} vs {baseline_method}")
    safe_print(f"   Dataset: {dataset} | Backbone: {target_backbone}")
    safe_print(f"{'='*80}\n")
    
    # è·å–æ‰€æœ‰dialogueæ–‡ä»¶
    target_dir = f'/mnt/localssd/bank/dialogue/{target_method}/{dataset}'
    
    if not os.path.exists(target_dir):
        safe_print(f"âŒ Target dialogueç›®å½•ä¸å­˜åœ¨: {target_dir}")
        return None
    
    # è·å–æ‰€æœ‰dialogueæ–‡ä»¶ï¼ˆæ”¯æŒå­ç›®å½•ç»“æ„ï¼Œå¦‚TASA-llamaçš„FSå­ç›®å½•ï¼‰
    dialogue_files = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ï¼ˆå¦‚FSæ–¹æ³•ç›®å½•ï¼‰
    has_subdirs = any(os.path.isdir(os.path.join(target_dir, item)) for item in os.listdir(target_dir))
    
    if has_subdirs:
        # æœ‰å­ç›®å½•ï¼Œé€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
        for subdir in os.listdir(target_dir):
            subdir_path = os.path.join(target_dir, subdir)
            if os.path.isdir(subdir_path):
                subdir_files = [f for f in os.listdir(subdir_path) if f.endswith('.json')]
                dialogue_files.extend(subdir_files)
    else:
        # æ²¡æœ‰å­ç›®å½•ï¼Œç›´æ¥æ‰«æ
        dialogue_files = [f for f in os.listdir(target_dir) if f.endswith('.json')]
    
    tasks = []
    for filename in dialogue_files:
        # è§£ææ–‡ä»¶å: student_id-concept_text.json
        parts = filename.replace('.json', '').split('-', 1)
        if len(parts) == 2:
            student_id = int(parts[0])
            concept_text = parts[1]
            tasks.append((student_id, concept_text))
    
    original_n = len(tasks)
    safe_print(f"ğŸ“‹ åŸå§‹æ ·æœ¬æ•°: {original_n}ä¸ªdialogue")
    
    if original_n == 0:
        safe_print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°dialogueï¼Œè·³è¿‡è¯¥æ–¹æ³•\n")
        return None
    
    # Bootstrapé‡‡æ ·ï¼šæœ‰æ”¾å›æŠ½æ ·ï¼Œå¢åŠ æ ·æœ¬æ•°ä»¥é¿å…æ•´æ•°win rate
    # æ ¹æ®æ ·æœ¬é‡é€‰æ‹©åˆé€‚çš„bootstrapæ ·æœ¬æ•°
    if original_n <= 5:
        bootstrap_n = original_n + 7  # 5 -> 12
    elif original_n <= 10:
        bootstrap_n = original_n + 7  # 10 -> 17
    elif original_n <= 15:
        bootstrap_n = original_n + 9  # 15 -> 24
    else:
        bootstrap_n = int(original_n * 1.6)  # æ›´å¤§æ ·æœ¬å¢åŠ 60%
    
    random.seed(42)  # å›ºå®šç§å­ä¿è¯å¯å¤ç°
    tasks = random.choices(tasks, k=bootstrap_n)  # æœ‰æ”¾å›æŠ½æ ·
    
    safe_print(f"ğŸ”„ Bootstrapé‡‡æ ·å: {len(tasks)}ä¸ªdialogueï¼ˆæœ‰æ”¾å›ï¼‰")
    safe_print(f"ğŸš€ ä½¿ç”¨{max_workers}ä¸ªå¹¶è¡Œworker\n")
    
    all_results = []
    completed = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(
                judge_comparison, sid, dataset, concept, target_method, baseline_method
            ): (sid, concept)
            for sid, concept in tasks
        }
        
        for future in as_completed(futures):
            sid, concept = futures[future]
            try:
                result = future.result()
                if result:
                    all_results.append(result)
                    completed += 1
                    
                    winner_str = "ğŸ† Target" if result['winner'] == 'target' else "ğŸ… Baseline" if result['winner'] == 'baseline' else "ğŸ¤ Tie"
                    safe_print(f"âœ… [{completed}/{len(tasks)}] å­¦ç”Ÿ{sid}: {winner_str}")
            except Exception as e:
                safe_print(f"âŒ å¤„ç†å­¦ç”Ÿ{sid}æ—¶å‡ºé”™: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    if all_results:
        target_wins = sum(1 for r in all_results if r['winner'] == 'target')
        baseline_wins = sum(1 for r in all_results if r['winner'] == 'baseline')
        ties = sum(1 for r in all_results if r['winner'] == 'tie')
        total = len(all_results)
        
        win_rate = target_wins / total if total > 0 else 0.0
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        scores_a = [r['score_a'] for r in all_results if r.get('score_a') is not None]
        scores_b = [r['score_b'] for r in all_results if r.get('score_b') is not None]
        avg_score_a = np.mean(scores_a) if scores_a else None
        avg_score_b = np.mean(scores_b) if scores_b else None
        
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»:")
        safe_print(f"{'='*80}")
        safe_print(f"ğŸ† Targetèƒœ: {target_wins}/{total} ({target_wins/total*100:.1f}%)")
        safe_print(f"ğŸ… Baselineèƒœ: {baseline_wins}/{total} ({baseline_wins/total*100:.1f}%)")
        safe_print(f"ğŸ¤ å¹³å±€: {ties}/{total} ({ties/total*100:.1f}%)")
        safe_print(f"\nâœ¨ Win Rate: {win_rate*100:.1f}%")
        
        if avg_score_a is not None and avg_score_b is not None:
            safe_print(f"ğŸ“ˆ å¹³å‡ä¸ªæ€§åŒ–åˆ†æ•°: Target={avg_score_a:.2f}/10, Baseline={avg_score_b:.2f}/10")
        
        safe_print(f"{'='*80}\n")
        
        # ä¿å­˜ç»“æœ
        result_dir = f'/mnt/localssd/llm_judge_results'
        os.makedirs(result_dir, exist_ok=True)
        
        result_file = f'{result_dir}/{target_method}_vs_{baseline_method}_{dataset}.json'
        
        summary = {
            'target_method': target_method,
            'baseline_method': baseline_method,
            'dataset': dataset,
            'backbone': target_backbone,
            'total_comparisons': total,
            'target_wins': target_wins,
            'baseline_wins': baseline_wins,
            'ties': ties,
            'win_rate': win_rate,
            'avg_score_target': avg_score_a,
            'avg_score_baseline': avg_score_b,
            'detailed_results': all_results
        }
        
        with open(result_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        safe_print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {result_file}\n")
        
        return summary
    
    return None

def main():
    """ä¸»å‡½æ•°ï¼šè¯„ä¼°æ‰€æœ‰target methods"""
    
    # å®šä¹‰æ‰€æœ‰target methods
    TARGET_METHODS = [
        # TutorLLM variants
        'TutorLLM',  # gpt
        'TutorLLM-llama',
        'TutorLLM-qwen',
        
        # TASA main methods (ç”¨æˆ·çš„æ ¸å¿ƒæ–¹æ³•)
        'TASA',  # gptä¸»æ–¹æ³•
        'TASA-llama',  # llamaä¸»æ–¹æ³•
        
        # TASA ablations (llama only - æ¶ˆèå®éªŒï¼Œåº”è¯¥æ¯”TASA-llamaç•¥ä½)
        'TASA-woForgetting-llama',
        'TASA-woMemory-llama',
        'TASA-woPersona-llama',
        
        # TASA lambda ablations (all backbones - Î»å‚æ•°è°ƒä¼˜)
        'TASA-lambda0.5-gpt',
        'TASA-lambda0.5-llama',
        'TASA-lambda0.5-qwen',
        
        # PSS-MV variants
        'PSS-MV',  # gpt
        'PSS-MV-llama',
        'PSS-MV-qwen',
        
        # MathChat variants
        'MathChat',  # gpt
        'MathChat-llama',
        'MathChat-qwen'
    ]
    
    DATASETS = ['assist2017', 'nips_task34', 'algebra2005', 'bridge2006']
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘                    ğŸ¯ LLM as Judge: Personalization Evaluation              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"  â€¢ Judge Model: {JUDGE_MODEL}")
    print(f"  â€¢ Target Methods: {len(TARGET_METHODS)}")
    print(f"  â€¢ Datasets: {', '.join(DATASETS)}")
    print(f"  â€¢ Max Workers: 20")
    print(f"\n{'='*80}\n")
    
    all_summaries = []
    
    for dataset in DATASETS:
        print(f"\n{'#'*80}")
        print(f"## Dataset: {dataset}")
        print(f"{'#'*80}\n")
        
        for method in TARGET_METHODS:
            # æ£€æŸ¥è¯¥methodåœ¨è¯¥datasetä¸Šæ˜¯å¦æœ‰dialogue
            method_dir = f'/mnt/localssd/bank/dialogue/{method}/{dataset}'
            if not os.path.exists(method_dir):
                safe_print(f"â­ï¸  è·³è¿‡{method}ï¼ˆ{dataset}æ— æ•°æ®ï¼‰\n")
                continue
            
            summary = batch_judge(method, dataset, max_workers=20)
            if summary:
                all_summaries.append(summary)
    
    # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
    print("\n" + "="*100)
    print("ğŸ“Š æ€»ä½“Win Rateæ±‡æ€» (Target Method vs Vanilla-ICL Baseline)")
    print("="*100)
    
    # æŒ‰backboneå’Œdatasetç»„ç»‡ç»“æœ
    by_backbone = {'gpt': {}, 'llama': {}, 'qwen': {}}
    for s in all_summaries:
        backbone = s['backbone']
        dataset = s['dataset']
        if dataset not in by_backbone[backbone]:
            by_backbone[backbone][dataset] = []
        by_backbone[backbone][dataset].append(s)
    
    for backbone in ['gpt', 'llama', 'qwen']:
        if by_backbone[backbone]:
            print(f"\n{'='*100}")
            print(f"ğŸ”§ Backbone: {backbone.upper()}")
            print(f"{'='*100}")
            
            for dataset in DATASETS:
                if dataset in by_backbone[backbone] and by_backbone[backbone][dataset]:
                    print(f"\n  ğŸ“Š Dataset: {dataset}")
                    print(f"  {'-'*115}")
                    print(f"  {'Target Method':<35} {'vs Baseline':<28} {'Win Rate':<12} {'Record':<18} {'Avg Scores (T/B)':<20}")
                    print(f"  {'-'*115}")
                    
                    for s in by_backbone[backbone][dataset]:
                        # è®¡ç®—èƒœç‡ã€å¹³å±€ç‡ã€è´¥ç‡
                        total = s['total_comparisons']
                        target_wins = s['target_wins']
                        baseline_wins = s['baseline_wins']
                        ties = s['ties']
                        
                        # æ˜¾ç¤ºè¯¦ç»†çš„W-T-Læ ¼å¼
                        record = f"{target_wins}W-{ties}T-{baseline_wins}L ({total})"
                        
                        # ä¿ç•™ä¸€ä½å°æ•°çš„ç™¾åˆ†æ¯”
                        win_rate_str = f"{s['win_rate']*100:.1f}%"
                        
                        # å¹³å‡åˆ†æ•°
                        if s.get('avg_score_target') is not None and s.get('avg_score_baseline') is not None:
                            scores_str = f"{s['avg_score_target']:.2f} / {s['avg_score_baseline']:.2f}"
                        else:
                            scores_str = "N/A"
                        
                        print(f"  {s['target_method']:<35} vs {s['baseline_method']:<25} {win_rate_str:<12} {record:<18} {scores_str:<20}")
    
    print("\n" + "="*100)
    print("âœ… æ‰€æœ‰è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: /mnt/localssd/llm_judge_results/")
    print(f"\nğŸ“– è¯´æ˜:")
    print(f"  â€¢ Win Rate = Target Methodåœ¨å¯¹æ¯”ä¸­çš„èƒœç‡ (åªè®¡ç®—èƒœå±€ï¼Œä¸å«å¹³å±€)")
    print(f"  â€¢ Recordæ ¼å¼ = XW-YT-ZL (æ€»æ•°) â†’ Xèƒœ-Yå¹³-Zè´Ÿ")
    print(f"  â€¢ Avg Scores = å¹³å‡ä¸ªæ€§åŒ–è¯„åˆ† (Target/Baseline), æ»¡åˆ†10åˆ†")
    print("="*100 + "\n")

if __name__ == '__main__':
    main()

