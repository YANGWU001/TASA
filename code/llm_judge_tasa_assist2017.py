#!/usr/bin/env python3
"""
LLM as Judge: è¯„ä¼°TASA-llamaç³»åˆ—æ–¹æ³•ï¼ˆä»…assist2017æ•°æ®é›†ï¼‰
"""

import sys
import os

# å¯¼å…¥ä¸»è¯„ä¼°æ¨¡å—çš„å‡½æ•°
sys.path.insert(0, '/mnt/localssd')
from llm_as_judge_personalization import batch_judge, safe_print

def main():
    """ä»…è¯„ä¼°TASA-llamaç³»åˆ—æ–¹æ³•åœ¨assist2017ä¸Š"""
    
    # åªè¯„ä¼°TASA-llamaç›¸å…³æ–¹æ³•
    TARGET_METHODS = [
        'TASA-llama',  # llamaä¸»æ–¹æ³•
        'TASA-woForgetting-llama',  # æ¶ˆèå®éªŒ
        'TASA-woMemory-llama',
        'TASA-woPersona-llama',
    ]
    
    DATASETS = ['assist2017']  # åªè¯„ä¼°assist2017
    
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     ğŸ¯ LLM as Judge: TASA-llama Methods (assist2017 only)                 â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print("ğŸ“‹ é…ç½®:")
    print(f"  â€¢ Target Methods: {len(TARGET_METHODS)}")
    print(f"  â€¢ Datasets: {', '.join(DATASETS)}")
    print(f"  â€¢ Max Workers: 20")
    print()
    print("="*80)
    
    all_summaries = []
    
    for dataset in DATASETS:
        print(f"\n{'#'*80}")
        print(f"## Dataset: {dataset}")
        print(f"{'#'*80}\n")
        
        for method in TARGET_METHODS:
            # æ£€æŸ¥è¯¥methodåœ¨è¯¥datasetä¸Šæ˜¯å¦æœ‰dialogue
            method_dir = f'/mnt/localssd/bank/dialogue/{method}/{dataset}'
            if not os.path.exists(method_dir):
                safe_print(f"â­ï¸  è·³è¿‡{method}ï¼ˆ{dataset}æ— æ•°æ®ï¼‰\n")
                continue
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¯„ä¼°è¿‡
            result_file = f'/mnt/localssd/llm_judge_results/{method}_vs_Vanilla-ICL-llama_{dataset}.json'
            if os.path.exists(result_file):
                safe_print(f"âœ… è·³è¿‡{method}ï¼ˆ{dataset}å·²è¯„ä¼°ï¼‰\n")
                continue
            
            summary = batch_judge(method, dataset, max_workers=20)
            if summary:
                all_summaries.append(summary)
    
    # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
    print("\n" + "="*100)
    print("ğŸ“Š TASA-llamaç³»åˆ—è¯„ä¼°å®Œæˆæ±‡æ€»ï¼ˆassist2017ï¼‰")
    print("="*100)
    
    if all_summaries:
        print(f"\n{'Method':<40} {'Dataset':<15} {'Win Rate':<12} {'Record':<25} {'Avg Scores':<15}")
        print('-'*107)
        for s in all_summaries:
            wr_str = f"{s['win_rate']*100:.1f}%"
            record = f"{s['target_wins']}W-{s['ties']}T-{s['baseline_wins']}L ({s['total_comparisons']})"
            scores = f"{s.get('avg_score_target', 0):.2f}/{s.get('avg_score_baseline', 0):.2f}"
            print(f"{s['target_method']:<40} {s['dataset']:<15} {wr_str:<12} {record:<25} {scores:<15}")
    else:
        print("æ‰€æœ‰æ–¹æ³•å‡å·²è¯„ä¼°æˆ–æ— æ•°æ®ï¼")
    
    print("\n" + "="*100)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: /mnt/localssd/llm_judge_results/")
    print("="*100 + "\n")

if __name__ == '__main__':
    main()

