#!/usr/bin/env python3
"""
è¯„ä¼°æ•°æ®é›†ä¸­çš„æ‰€æœ‰å­¦ç”Ÿ
æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è¿›åº¦æ˜¾ç¤ºã€é”™è¯¯å¤„ç†
"""

import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict
import time
from tqdm import tqdm

from batch_test_students import evaluate_single_student

def get_all_student_ids(dataset: str) -> List[int]:
    """è·å–æ•°æ®é›†ä¸­æ‰€æœ‰å­¦ç”Ÿçš„ID"""
    session_dir = f'/mnt/localssd/bank/session/{dataset}'
    student_files = [f for f in os.listdir(session_dir) if f.endswith('.json')]
    student_ids = [int(f.replace('.json', '')) for f in student_files]
    return sorted(student_ids)

def get_completed_students(method: str, dataset: str) -> List[int]:
    """è·å–å·²å®Œæˆè¯„ä¼°çš„å­¦ç”ŸID"""
    result_dir = f'/mnt/localssd/bank/evaluation_results/{method}/{dataset}'
    if not os.path.exists(result_dir):
        return []
    
    completed = []
    for filename in os.listdir(result_dir):
        if filename.startswith('student_') and filename.endswith('.json'):
            try:
                # è§£æ student_{id}_concept_{cid}.json
                parts = filename.replace('student_', '').replace('.json', '').split('_concept_')
                student_id = int(parts[0])
                completed.append(student_id)
            except:
                pass
    
    return list(set(completed))  # å»é‡

def generate_overall_statistics(method: str, dataset: str):
    """ç”Ÿæˆoverall.jsonç»Ÿè®¡"""
    result_dir = f'/mnt/localssd/bank/evaluation_results/{method}/{dataset}'
    
    # è¯»å–æ‰€æœ‰å­¦ç”Ÿç»“æœ
    results = []
    for filename in os.listdir(result_dir):
        if filename.startswith('student_') and filename.endswith('.json'):
            filepath = os.path.join(result_dir, filename)
            try:
                with open(filepath) as f:
                    result = json.load(f)
                results.append(result)
            except:
                pass
    
    if not results:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœ")
        return
    
    # è®¡ç®—ç»Ÿè®¡
    avg_original = sum(r['original_accuracy'] for r in results) / len(results)
    avg_roleplay = sum(r['roleplay_accuracy'] for r in results) / len(results)
    avg_deviation = sum(abs(r['deviation']) for r in results) / len(results)
    
    # æŒ‰æ°´å¹³åˆ†ç»„
    struggling = [r for r in results if r['original_accuracy'] < 0.4]
    developing = [r for r in results if 0.4 <= r['original_accuracy'] < 0.6]
    competent = [r for r in results if 0.6 <= r['original_accuracy'] < 0.8]
    strong = [r for r in results if r['original_accuracy'] >= 0.8]
    
    overall = {
        "method": method,
        "dataset": dataset,
        "num_students_evaluated": len(results),
        "average_original_accuracy": avg_original,
        "average_roleplay_accuracy": avg_roleplay,
        "average_absolute_deviation": avg_deviation,
        "performance_by_level": {
            "struggling": {
                "range": "<40%",
                "num_students": len(struggling),
                "avg_original_accuracy": sum(r['original_accuracy'] for r in struggling) / len(struggling) if struggling else 0,
                "avg_roleplay_accuracy": sum(r['roleplay_accuracy'] for r in struggling) / len(struggling) if struggling else 0,
                "avg_deviation": sum(abs(r['deviation']) for r in struggling) / len(struggling) if struggling else 0
            },
            "developing": {
                "range": "40-60%",
                "num_students": len(developing),
                "avg_original_accuracy": sum(r['original_accuracy'] for r in developing) / len(developing) if developing else 0,
                "avg_roleplay_accuracy": sum(r['roleplay_accuracy'] for r in developing) / len(developing) if developing else 0,
                "avg_deviation": sum(abs(r['deviation']) for r in developing) / len(developing) if developing else 0
            },
            "competent": {
                "range": "60-80%",
                "num_students": len(competent),
                "avg_original_accuracy": sum(r['original_accuracy'] for r in competent) / len(competent) if competent else 0,
                "avg_roleplay_accuracy": sum(r['roleplay_accuracy'] for r in competent) / len(competent) if competent else 0,
                "avg_deviation": sum(abs(r['deviation']) for r in competent) / len(competent) if competent else 0
            },
            "strong": {
                "range": "â‰¥80%",
                "num_students": len(strong),
                "avg_original_accuracy": sum(r['original_accuracy'] for r in strong) / len(strong) if strong else 0,
                "avg_roleplay_accuracy": sum(r['roleplay_accuracy'] for r in strong) / len(strong) if strong else 0,
                "avg_deviation": sum(abs(r['deviation']) for r in strong) / len(strong) if strong else 0
            }
        }
    }
    
    # ä¿å­˜
    overall_file = f'{result_dir}/overall.json'
    with open(overall_file, 'w') as f:
        json.dump(overall, f, indent=2)
    
    return overall

def evaluate_all_students(
    dataset: str = "assist2017",
    method: str = "pre-test",
    max_workers: int = 20,
    resume: bool = True
):
    """è¯„ä¼°æ‰€æœ‰å­¦ç”Ÿ"""
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ å…¨é‡å­¦ç”Ÿè¯„ä¼°ç³»ç»Ÿ")
    print(f"{'='*80}")
    print(f"Method: {method}")
    print(f"Dataset: {dataset}")
    print(f"Max Workers: {max_workers}")
    print(f"Resume Mode: {resume}")
    print(f"{'='*80}\n")
    
    # è·å–æ‰€æœ‰å­¦ç”Ÿ
    all_students = get_all_student_ids(dataset)
    print(f"ğŸ“Š æ€»å­¦ç”Ÿæ•°: {len(all_students)}")
    
    # æ£€æŸ¥å·²å®Œæˆçš„å­¦ç”Ÿ
    if resume:
        completed = get_completed_students(method, dataset)
        print(f"âœ… å·²å®Œæˆ: {len(completed)}ä¸ªå­¦ç”Ÿ")
        remaining = [sid for sid in all_students if sid not in completed]
        print(f"â³ å¾…è¯„ä¼°: {len(remaining)}ä¸ªå­¦ç”Ÿ")
    else:
        remaining = all_students
        print(f"â³ å¾…è¯„ä¼°: {len(remaining)}ä¸ªå­¦ç”Ÿ")
    
    if not remaining:
        print("\nâœ… æ‰€æœ‰å­¦ç”Ÿéƒ½å·²å®Œæˆè¯„ä¼°ï¼")
        print("\nç”Ÿæˆæœ€ç»ˆç»Ÿè®¡...")
        overall = generate_overall_statistics(method, dataset)
        print_overall_summary(overall)
        return
    
    # å¼€å§‹è¯„ä¼°
    print(f"\nå¼€å§‹è¯„ä¼°... (é¢„è®¡æ—¶é—´: ~{len(remaining) * 30 / max_workers / 60:.1f}åˆ†é’Ÿ)\n")
    
    start_time = time.time()
    success_count = 0
    error_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_student = {
            executor.submit(evaluate_single_student, student_id, dataset, method): student_id
            for student_id in remaining
        }
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(remaining), desc="è¯„ä¼°è¿›åº¦", ncols=100) as pbar:
            for future in as_completed(future_to_student):
                student_id = future_to_student[future]
                try:
                    result = future.result()
                    if result:
                        success_count += 1
                    else:
                        error_count += 1
                except Exception as e:
                    error_count += 1
                    print(f"\nâŒ å­¦ç”Ÿ {student_id} å¤±è´¥: {e}")
                
                pbar.update(1)
                pbar.set_postfix({
                    'Success': success_count,
                    'Error': error_count
                })
                
                # æ¯100ä¸ªå­¦ç”Ÿæ›´æ–°ä¸€æ¬¡overallç»Ÿè®¡
                if (success_count + error_count) % 100 == 0:
                    try:
                        generate_overall_statistics(method, dataset)
                    except:
                        pass
    
    elapsed_time = time.time() - start_time
    
    # æœ€ç»ˆç»Ÿè®¡
    print(f"\n{'='*80}")
    print(f"âœ… è¯„ä¼°å®Œæˆ!")
    print(f"{'='*80}")
    print(f"æ€»è€—æ—¶: {elapsed_time/60:.1f}åˆ†é’Ÿ ({elapsed_time:.1f}ç§’)")
    print(f"æˆåŠŸ: {success_count}ä¸ªå­¦ç”Ÿ")
    print(f"å¤±è´¥: {error_count}ä¸ªå­¦ç”Ÿ")
    print(f"å¹³å‡é€Ÿåº¦: {elapsed_time/len(remaining):.1f}ç§’/å­¦ç”Ÿ")
    print(f"{'='*80}\n")
    
    # ç”Ÿæˆæœ€ç»ˆoverallç»Ÿè®¡
    print("ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡...")
    overall = generate_overall_statistics(method, dataset)
    print_overall_summary(overall)

def print_overall_summary(overall: Dict):
    """æ‰“å°overallç»Ÿè®¡æ‘˜è¦"""
    if not overall:
        return
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š Overallç»Ÿè®¡ - {overall['method']} @ {overall['dataset']}")
    print(f"{'='*80}")
    print(f"è¯„ä¼°å­¦ç”Ÿæ•°: {overall['num_students_evaluated']}")
    print(f"å¹³å‡åŸå§‹å‡†ç¡®ç‡: {overall['average_original_accuracy']*100:.1f}%")
    print(f"å¹³å‡Role-playå‡†ç¡®ç‡: {overall['average_roleplay_accuracy']*100:.1f}%")
    print(f"å¹³å‡ç»å¯¹åå·®: {overall['average_absolute_deviation']*100:.1f}%")
    
    print(f"\næŒ‰æ°´å¹³åˆ†æ:")
    for level, data in overall['performance_by_level'].items():
        if data['num_students'] > 0:
            print(f"\n{level.upper()} ({data['range']}):")
            print(f"  å­¦ç”Ÿæ•°: {data['num_students']}")
            print(f"  å¹³å‡åŸå§‹å‡†ç¡®ç‡: {data['avg_original_accuracy']*100:.1f}%")
            print(f"  å¹³å‡Role-playå‡†ç¡®ç‡: {data['avg_roleplay_accuracy']*100:.1f}%")
            print(f"  å¹³å‡åå·®: {data['avg_deviation']*100:.1f}%")
    
    print(f"\n{'='*80}")
    print(f"ğŸ’¾ å®Œæ•´ç»Ÿè®¡å·²ä¿å­˜è‡³:")
    print(f"   /mnt/localssd/bank/evaluation_results/{overall['method']}/{overall['dataset']}/overall.json")
    print(f"{'='*80}\n")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è¯„ä¼°æ•°æ®é›†ä¸­çš„æ‰€æœ‰å­¦ç”Ÿ')
    parser.add_argument('--dataset', type=str, default='assist2017', help='æ•°æ®é›†åç§°')
    parser.add_argument('--method', type=str, default='pre-test', help='è¯„ä¼°æ–¹æ³•åç§°')
    parser.add_argument('--max-workers', type=int, default=20, help='å¹¶è¡Œçº¿ç¨‹æ•°')
    parser.add_argument('--no-resume', action='store_true', help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°è¯„ä¼°æ‰€æœ‰å­¦ç”Ÿ')
    
    args = parser.parse_args()
    
    evaluate_all_students(
        dataset=args.dataset,
        method=args.method,
        max_workers=args.max_workers,
        resume=not args.no_resume
    )

