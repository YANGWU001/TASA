#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®å¤ nips_task34 æ•°æ®é›†çš„ concept_text
å°†æ•°å­— ID æ›¿æ¢ä¸ºå®é™…çš„ subject åç§°ï¼Œå¹¶é‡æ–°è®¡ç®— embedding
"""

import os
import json
import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
import argparse

try:
    from FlagEmbedding import BGEM3FlagModel
except ImportError:
    from FlagEmbedding import FlagModel as BGEM3FlagModel


def load_subject_mapping():
    """åŠ è½½ SubjectId -> Name çš„æ˜ å°„"""
    metadata_file = '/mnt/localssd/pykt-toolkit/data/nips_task34/metadata/subject_metadata.csv'
    df = pd.read_csv(metadata_file)
    
    # åˆ›å»º SubjectId -> Name çš„æ˜ å°„
    subject_map = {}
    for _, row in df.iterrows():
        subject_id = str(row['SubjectId'])
        name = row['Name']
        subject_map[subject_id] = name
    
    print(f"âœ… åŠ è½½äº† {len(subject_map)} ä¸ª Subject æ˜ å°„")
    print(f"   ç¤ºä¾‹: SubjectId 191 -> '{subject_map.get('191', 'NOT_FOUND')}'")
    
    return subject_map


def replace_concept_text_in_string(text, subject_map):
    """
    åœ¨å­—ç¬¦ä¸²ä¸­æ›¿æ¢ concept_text
    ä¾‹å¦‚: "The student mastered the 191 task." -> "The student mastered the Percentages task."
    """
    result = text
    # æŒ‰ç…§ SubjectId ä»é•¿åˆ°çŸ­æ’åºï¼Œé¿å…æ›¿æ¢å†²çªï¼ˆå¦‚å…ˆæ›¿æ¢1ï¼Œå†æ›¿æ¢191ä¼šæœ‰é—®é¢˜ï¼‰
    for subject_id in sorted(subject_map.keys(), key=lambda x: len(x), reverse=True):
        subject_name = subject_map[subject_id]
        # åªæ›¿æ¢ç‹¬ç«‹çš„æ•°å­—ï¼ˆå‰åæ˜¯ç©ºæ ¼ã€æ ‡ç‚¹æˆ–å­—ç¬¦ä¸²è¾¹ç•Œï¼‰
        import re
        # åŒ¹é…ç‹¬ç«‹çš„æ•°å­—
        pattern = r'\b' + re.escape(subject_id) + r'\b'
        result = re.sub(pattern, subject_name, result)
    
    return result


def process_memory_file(student_file, subject_map, bge_model):
    """å¤„ç†å•ä¸ª memory æ–‡ä»¶"""
    student_id = os.path.basename(student_file).replace('.json', '')
    
    with open(student_file, 'r', encoding='utf-8') as f:
        memories = json.load(f)
    
    if not memories:
        return 0
    
    json_updated = False
    need_recompute_embeddings = False
    
    for idx, memory in enumerate(memories):
        old_concept_text = memory['concept_text']
        old_description = memory['description']
        
        # æ›¿æ¢ concept_textï¼ˆå¦‚æœæ˜¯çº¯æ•°å­—ï¼‰
        if old_concept_text.isdigit():
            new_concept_text = subject_map.get(old_concept_text, old_concept_text)
            if new_concept_text != old_concept_text:
                memory['concept_text'] = new_concept_text
                json_updated = True
                need_recompute_embeddings = True
        
        # æ›¿æ¢ description ä¸­çš„æ•°å­—
        new_description = replace_concept_text_in_string(old_description, subject_map)
        if new_description != old_description:
            memory['description'] = new_description
            json_updated = True
            need_recompute_embeddings = True
    
    # ä¿å­˜æ›´æ–°åçš„ JSON æ–‡ä»¶
    if json_updated:
        with open(student_file, 'w', encoding='utf-8') as f:
            json.dump(memories, f, indent=2, ensure_ascii=False)
    
    # é‡æ–°è®¡ç®—å¹¶ä¿å­˜ embeddings
    if need_recompute_embeddings:
        # æå–æ‰€æœ‰ descriptions å’Œ keywords
        descriptions = [m['description'] for m in memories]
        keywords = [m.get('keywords', m['concept_text']) for m in memories]
        
        # ç”Ÿæˆ embeddings
        desc_embeddings = generate_embeddings_batch(descriptions, bge_model)
        kw_embeddings = generate_embeddings_batch(keywords, bge_model)
        
        # ä¿å­˜åˆ° embeddings/ ç›®å½•
        embeddings_dir = os.path.join(os.path.dirname(os.path.dirname(student_file)), 'embeddings')
        os.makedirs(embeddings_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º npz æ–‡ä»¶ï¼ˆä½¿ç”¨ float16 æ ¼å¼èŠ‚çœç©ºé—´ï¼‰
        np.savez_compressed(
            os.path.join(embeddings_dir, f'{student_id}_description.npz'),
            embeddings=np.array(desc_embeddings, dtype=np.float16)
        )
        np.savez_compressed(
            os.path.join(embeddings_dir, f'{student_id}_keywords.npz'),
            embeddings=np.array(kw_embeddings, dtype=np.float16)
        )
        
        return len(memories)
    
    return 0


def process_persona_file(student_file, subject_map, bge_model):
    """å¤„ç†å•ä¸ª persona æ–‡ä»¶"""
    student_id = os.path.basename(student_file).replace('.json', '')
    
    with open(student_file, 'r', encoding='utf-8') as f:
        personas = json.load(f)
    
    if not personas:
        return 0
    
    json_updated = False
    need_recompute_embeddings = False
    
    for idx, persona in enumerate(personas):
        old_concept_text = persona['concept_text']
        old_description = persona['description']
        
        # æ›¿æ¢ concept_textï¼ˆå¦‚æœæ˜¯çº¯æ•°å­—ï¼‰
        if old_concept_text.isdigit():
            new_concept_text = subject_map.get(old_concept_text, old_concept_text)
            if new_concept_text != old_concept_text:
                persona['concept_text'] = new_concept_text
                # ä¹Ÿæ›´æ–° keywords
                if persona.get('keywords') == old_concept_text:
                    persona['keywords'] = new_concept_text
                json_updated = True
                need_recompute_embeddings = True
        
        # æ›¿æ¢ description ä¸­çš„æ•°å­—
        new_description = replace_concept_text_in_string(old_description, subject_map)
        if new_description != old_description:
            persona['description'] = new_description
            json_updated = True
            need_recompute_embeddings = True
    
    # ä¿å­˜æ›´æ–°åçš„ JSON æ–‡ä»¶
    if json_updated:
        with open(student_file, 'w', encoding='utf-8') as f:
            json.dump(personas, f, indent=2, ensure_ascii=False)
    
    # é‡æ–°è®¡ç®—å¹¶ä¿å­˜ embeddings
    if need_recompute_embeddings:
        # æå–æ‰€æœ‰ descriptions å’Œ keywords
        descriptions = [p['description'] for p in personas]
        keywords = [p.get('keywords', p['concept_text']) for p in personas]
        
        # ç”Ÿæˆ embeddings
        desc_embeddings = generate_embeddings_batch(descriptions, bge_model)
        kw_embeddings = generate_embeddings_batch(keywords, bge_model)
        
        # ä¿å­˜åˆ° embeddings/ ç›®å½•
        embeddings_dir = os.path.join(os.path.dirname(os.path.dirname(student_file)), 'embeddings')
        os.makedirs(embeddings_dir, exist_ok=True)
        
        # ä¿å­˜ä¸º npz æ–‡ä»¶ï¼ˆä½¿ç”¨ float16 æ ¼å¼èŠ‚çœç©ºé—´ï¼‰
        np.savez_compressed(
            os.path.join(embeddings_dir, f'{student_id}_description.npz'),
            embeddings=np.array(desc_embeddings, dtype=np.float16)
        )
        np.savez_compressed(
            os.path.join(embeddings_dir, f'{student_id}_keywords.npz'),
            embeddings=np.array(kw_embeddings, dtype=np.float16)
        )
        
        return len(personas)
    
    return 0


def generate_embeddings_batch(texts, model):
    """æ‰¹é‡ç”Ÿæˆ embeddings"""
    if not texts:
        return []
    
    # ä½¿ç”¨ BGE-M3 æ¨¡å‹çš„ encode æ–¹æ³•
    embeddings = model.encode(texts, batch_size=min(128, len(texts)))
    return embeddings


def load_bge_model():
    """åŠ è½½ BGE æ¨¡å‹"""
    print("ğŸ¤– åˆå§‹åŒ–BGEæ¨¡å‹...")
    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
    print("  âœ… BGEæ¨¡å‹åŠ è½½å®Œæˆ")
    return model


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--type', type=str, choices=['memory', 'persona', 'both'], default='both',
                       help='å¤„ç†ç±»å‹: memory, persona, æˆ– both')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆåªå¤„ç†å‰5ä¸ªå­¦ç”Ÿï¼‰')
    args = parser.parse_args()
    
    print("=" * 100)
    print("ä¿®å¤ NIPS_TASK34 Concept Text")
    print(f"å¤„ç†ç±»å‹: {args.type}")
    if args.test:
        print("âš ï¸  æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰5ä¸ªå­¦ç”Ÿ")
    print("=" * 100)
    print()
    
    # 1. åŠ è½½ Subject æ˜ å°„
    subject_map = load_subject_mapping()
    print()
    
    # 2. åŠ è½½ BGE æ¨¡å‹
    bge_model = load_bge_model()
    print()
    
    # 3. å¤„ç† Memory æ–‡ä»¶
    if args.type in ['memory', 'both']:
        memory_dir = '/mnt/localssd/bank/memory/nips_task34/data'
        memory_files = sorted([f for f in os.listdir(memory_dir) if f.endswith('.json')])
        
        if args.test:
            memory_files = memory_files[:5]
        
        print(f"ğŸ“ å¤„ç† Memory æ–‡ä»¶ (å…±{len(memory_files)}ä¸ª)...")
        total_updated = 0
        for filename in tqdm(memory_files, desc="Memory"):
            filepath = os.path.join(memory_dir, filename)
            updated_count = process_memory_file(filepath, subject_map, bge_model)
            total_updated += updated_count
        
        print(f"  âœ… Memory å¤„ç†å®Œæˆ: æ›´æ–°äº† {total_updated} æ¡è®°å½•çš„ embedding")
        print()
    
    # 4. å¤„ç† Persona æ–‡ä»¶
    if args.type in ['persona', 'both']:
        persona_dir = '/mnt/localssd/bank/persona/nips_task34/data'
        persona_files = sorted([f for f in os.listdir(persona_dir) if f.endswith('.json')])
        
        if args.test:
            persona_files = persona_files[:5]
        
        print(f"ğŸ‘¤ å¤„ç† Persona æ–‡ä»¶ (å…±{len(persona_files)}ä¸ª)...")
        total_updated = 0
        for filename in tqdm(persona_files, desc="Persona"):
            filepath = os.path.join(persona_dir, filename)
            updated_count = process_persona_file(filepath, subject_map, bge_model)
            total_updated += updated_count
        
        print(f"  âœ… Persona å¤„ç†å®Œæˆ: æ›´æ–°äº† {total_updated} æ¡è®°å½•çš„ embedding")
        print()
    
    print("=" * 100)
    print("âœ… å®Œæˆï¼")
    print("=" * 100)


if __name__ == '__main__':
    main()

