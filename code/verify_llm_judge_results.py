#!/usr/bin/env python3
"""
éªŒè¯å·²å®Œæˆçš„LLM judgeç»“æœ
æ£€æŸ¥win ratesã€baselineåŒ¹é…ã€common studentsç­‰
"""

import json
import os
from collections import defaultdict

def verify_results():
    result_dir = '/mnt/localssd/llm_judge_results'
    
    # æŒ‰methodå’Œdatasetåˆ†ç»„
    results_by_method = defaultdict(lambda: defaultdict(dict))
    
    all_files = [f for f in os.listdir(result_dir) if f.endswith('.json')]
    
    print('='*100)
    print('ğŸ” éªŒè¯LLM Judgeç»“æœ')
    print('='*100)
    print(f'æ€»æ–‡ä»¶æ•°: {len(all_files)}\n')
    
    issues = []
    
    for fname in sorted(all_files):
        fpath = os.path.join(result_dir, fname)
        
        try:
            with open(fpath) as f:
                data = json.load(f)
            
            target_method = data.get('target_method', 'unknown')
            baseline_method = data.get('baseline_method', 'unknown')
            dataset = data.get('dataset', 'unknown')
            win_rate = data.get('win_rate', 0)
            tie_rate = data.get('tie_rate', 0)
            
            # å¤„ç†ä¸¤ç§æ ¼å¼
            if 'results' in data:
                # æ–°æ ¼å¼
                common_students = data.get('common_students', 0)
                total_comparisons = data.get('results', {}).get('A_wins', 0) + data.get('results', {}).get('B_wins', 0) + data.get('results', {}).get('ties', 0)
            else:
                # æ—§æ ¼å¼
                common_students = len(data.get('detailed_results', []))
                total_comparisons = data.get('total_comparisons', 0)
                win_rate = win_rate * 100 if win_rate < 1.5 else win_rate  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                tie_rate = tie_rate * 100 if 'ties' in data else 0
            
            # æ£€æŸ¥1: Win rateè¿‡é«˜
            if win_rate > 90:
                issues.append(f"âš ï¸  {fname}: Win rateè¿‡é«˜ ({win_rate:.1f}%)")
            
            # æ£€æŸ¥2: BaselineåŒ¹é…
            if 'llama' in target_method.lower() and 'llama' not in baseline_method.lower():
                issues.append(f"âš ï¸  {fname}: Backboneä¸åŒ¹é… ({target_method} vs {baseline_method})")
            if 'qwen' in target_method.lower() and 'qwen' not in baseline_method.lower():
                issues.append(f"âš ï¸  {fname}: Backboneä¸åŒ¹é… ({target_method} vs {baseline_method})")
            if 'gpt' in target_method.lower() and target_method != 'TASA' and 'gpt' not in baseline_method.lower() and baseline_method != 'Vanilla-ICL':
                issues.append(f"âš ï¸  {fname}: Backboneä¸åŒ¹é… ({target_method} vs {baseline_method})")
            
            # æ£€æŸ¥3: Common studentsæ•°é‡
            if common_students == 0:
                issues.append(f"âš ï¸  {fname}: Common studentsä¸º0")
            
            # æ£€æŸ¥4: Total comparisonsä¸common studentsçš„å…³ç³»
            if total_comparisons < common_students:
                issues.append(f"âš ï¸  {fname}: Total comparisons({total_comparisons}) < common students({common_students})")
            
            # å­˜å‚¨ç»“æœ
            results_by_method[target_method][dataset] = {
                'win_rate': win_rate,
                'tie_rate': tie_rate,
                'common_students': common_students,
                'total_comparisons': total_comparisons,
                'baseline': baseline_method
            }
            
        except Exception as e:
            issues.append(f"âŒ {fname}: è¯»å–å¤±è´¥ - {e}")
    
    # æ‰“å°ç»“æœè¡¨æ ¼
    print('\n' + '='*100)
    print('ğŸ“Š Win Rateæ±‡æ€»ï¼ˆæŒ‰Methodåˆ†ç»„ï¼‰')
    print('='*100)
    print()
    
    for method in sorted(results_by_method.keys()):
        datasets_data = results_by_method[method]
        print(f"\nã€{method}ã€‘")
        print(f"{'Dataset':<15} | {'Baseline':<25} | {'Win%':>6} | {'Tie%':>6} | {'Students':>8} | {'Comparisons':>12}")
        print(f"{'-'*15}-+-{'-'*25}-+-{'-'*6}-+-{'-'*6}-+-{'-'*8}-+-{'-'*12}")
        
        for dataset in sorted(datasets_data.keys()):
            d = datasets_data[dataset]
            print(f"{dataset:<15} | {d['baseline']:<25} | {d['win_rate']:>5.1f}% | {d['tie_rate']:>5.1f}% | {d['common_students']:>8} | {d['total_comparisons']:>12}")
    
    # æ‰“å°é—®é¢˜
    print('\n' + '='*100)
    print('ğŸš¨ å‘ç°çš„é—®é¢˜')
    print('='*100)
    if issues:
        for issue in issues:
            print(issue)
    else:
        print('âœ… æ²¡æœ‰å‘ç°é—®é¢˜ï¼')
    print('='*100)
    
    # æŒ‰backboneåˆ†ç»„çš„ç»Ÿè®¡
    print('\n' + '='*100)
    print('ğŸ“Š æŒ‰Backboneåˆ†ç»„çš„Win Rateç»Ÿè®¡')
    print('='*100)
    
    backbone_stats = defaultdict(lambda: {'methods': [], 'win_rates': []})
    
    for method, datasets_data in results_by_method.items():
        # ç¡®å®šbackbone
        if 'llama' in method.lower():
            backbone = 'Llama'
        elif 'qwen' in method.lower():
            backbone = 'Qwen'
        elif 'gpt' in method.lower() or method == 'TASA':
            backbone = 'GPT-OSS-120b'
        else:
            backbone = 'Unknown'
        
        # è®¡ç®—å¹³å‡win rate
        win_rates = [d['win_rate'] for d in datasets_data.values()]
        if win_rates:
            avg_win_rate = sum(win_rates) / len(win_rates)
            backbone_stats[backbone]['methods'].append(method)
            backbone_stats[backbone]['win_rates'].append(avg_win_rate)
    
    for backbone in sorted(backbone_stats.keys()):
        print(f"\nã€{backbone}ã€‘")
        methods = backbone_stats[backbone]['methods']
        win_rates = backbone_stats[backbone]['win_rates']
        
        for method, win_rate in zip(methods, win_rates):
            print(f"  {method:<30s}: {win_rate:>5.1f}%")

if __name__ == '__main__':
    verify_results()

