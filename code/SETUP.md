# ğŸš€ TASA Environment Setup Guide

å®Œæ•´çš„ç¯å¢ƒé…ç½®æŒ‡å—ï¼Œç”¨äºåœ¨æ–°æœºå™¨ä¸Šéƒ¨ç½²TASAç³»ç»Ÿ

---

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU with 16GB+ VRAM (æ¨è A100/V100/RTX 4090)
- **å†…å­˜**: 32GB+ RAM
- **å­˜å‚¨**: 100GB+ å¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+ æˆ– CentOS 7+)
- **Python**: 3.10 æˆ– 3.11
- **CUDA**: 12.1 æˆ– 12.4 (ä¸PyTorchç‰ˆæœ¬åŒ¹é…)
- **cuDNN**: 8.x+

---

## ğŸ“¦ å®‰è£…æ­¥éª¤

### 1ï¸âƒ£ åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.10 -m venv /opt/venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source /opt/venv/bin/activate

# å‡çº§pip
pip install --upgrade pip setuptools wheel
```

### 2ï¸âƒ£ å®‰è£…PyTorch (å¸¦CUDAæ”¯æŒ)

```bash
# CUDA 12.4ç‰ˆæœ¬ (æ¨è)
pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124

# æˆ–è€… CUDA 12.1ç‰ˆæœ¬
# pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu121
```

éªŒè¯å®‰è£…ï¼š
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}')"
```

### 3ï¸âƒ£ å®‰è£…ä¸»è¦ä¾èµ–

```bash
# å®‰è£…requirements.txtä¸­çš„æ‰€æœ‰åŒ…
pip install -r requirements.txt
```

### 4ï¸âƒ£ å®‰è£…pykt-toolkit (çŸ¥è¯†è¿½è¸ªåº“)

```bash
# å…‹éš†pykt-toolkitä»“åº“
cd /mnt/localssd  # æˆ–è€…ä½ çš„é¡¹ç›®æ ¹ç›®å½•
git clone https://github.com/pykt-team/pykt-toolkit.git
cd pykt-toolkit

# å®‰è£…pykt-toolkit (å¼€å‘æ¨¡å¼)
pip install -e .

# éªŒè¯å®‰è£…
python -c "from pykt.models import LPKT, DKT, AKT, SimpleKT; print('pykt-toolkit installed successfully')"
```

### 5ï¸âƒ£ ä¸‹è½½BGEæ¨¡å‹ (Embeddings & Reranker)

```bash
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p /mnt/localssd/models

# ä¸‹è½½BGE-M3 (Embeddings)
cd /mnt/localssd/models
git clone https://huggingface.co/BAAI/bge-m3

# ä¸‹è½½BGE-Reranker-v2-M3
git clone https://huggingface.co/BAAI/bge-reranker-v2-m3

# æˆ–è€…ä½¿ç”¨Pythonä¸‹è½½
python << EOF
from FlagEmbedding import BGEM3FlagModel, FlagReranker

# è‡ªåŠ¨ä¸‹è½½åˆ°~/.cache/huggingface/
embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)
print("Models downloaded successfully")
EOF
```

### 6ï¸âƒ£ é…ç½®ç¯å¢ƒå˜é‡

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
cat > /mnt/localssd/.env << 'EOF'
# APIé…ç½®
API_KEY=YOUR_API_KEY_HERE

# GPT Proxy (Student/Grader/Rewriterå›ºå®šä½¿ç”¨)
GPT_ENDPOINT=YOUR_ENDPOINT_HERE

# Llama API (å¯ä»¥æ ¹æ®éœ€è¦æ›´æ–°ngroké“¾æ¥)
LLAMA_URL=https://2d96013eaaf0.ngrok-free.app/predict/

# Qwen API (å¯ä»¥æ ¹æ®éœ€è¦æ›´æ–°ngroké“¾æ¥)
QWEN_URL=https://5d80b2bc05ca.ngrok-free.app/predict/

# æ¨¡å‹è·¯å¾„ (å¦‚æœéœ€è¦æœ¬åœ°æ¨¡å‹)
BGE_M3_PATH=/mnt/localssd/models/bge-m3
BGE_RERANKER_PATH=/mnt/localssd/models/bge-reranker-v2-m3
EOF
```

### 7ï¸âƒ£ åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„

```bash
cd /mnt/localssd

# åˆ›å»ºæ•°æ®å’Œç»“æœç›®å½•
mkdir -p bank/{persona,memory,session,dialogue,evaluation_results}
mkdir -p data/{raw,processed}
mkdir -p logs
mkdir -p models

# éªŒè¯ç›®å½•ç»“æ„
tree -L 2 bank/
```

### 8ï¸âƒ£ éªŒè¯å®‰è£…

è¿è¡ŒéªŒè¯è„šæœ¬ï¼š
```bash
python << 'EOF'
import sys
print("=" * 80)
print("ğŸ” ç¯å¢ƒéªŒè¯")
print("=" * 80)

# 1. Pythonç‰ˆæœ¬
print(f"Python: {sys.version.split()[0]}")

# 2. PyTorchå’ŒCUDA
try:
    import torch
    print(f"âœ… PyTorch: {torch.__version__}")
    print(f"âœ… CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA Version: {torch.version.cuda}")
except Exception as e:
    print(f"âŒ PyTorch: {e}")

# 3. Transformers
try:
    import transformers
    print(f"âœ… Transformers: {transformers.__version__}")
except Exception as e:
    print(f"âŒ Transformers: {e}")

# 4. FlagEmbedding
try:
    from FlagEmbedding import BGEM3FlagModel
    print(f"âœ… FlagEmbedding: Available")
except Exception as e:
    print(f"âŒ FlagEmbedding: {e}")

# 5. OpenAI
try:
    import openai
    print(f"âœ… OpenAI: {openai.__version__}")
except Exception as e:
    print(f"âŒ OpenAI: {e}")

# 6. httpx
try:
    import httpx
    print(f"âœ… httpx: {httpx.__version__}")
except Exception as e:
    print(f"âŒ httpx: {e}")

# 7. pykt-toolkit
try:
    from pykt.models import LPKT, DKT, AKT, SimpleKT
    print(f"âœ… pykt-toolkit: Available")
except Exception as e:
    print(f"âŒ pykt-toolkit: {e}")

# 8. NumPy & Pandas
try:
    import numpy as np
    import pandas as pd
    print(f"âœ… NumPy: {np.__version__}")
    print(f"âœ… Pandas: {pd.__version__}")
except Exception as e:
    print(f"âŒ NumPy/Pandas: {e}")

print("=" * 80)
print("âœ… ç¯å¢ƒéªŒè¯å®Œæˆï¼")
print("=" * 80)
EOF
```

---

## ğŸ”§ é…ç½®æ–‡ä»¶è¯´æ˜

### ä¸»è¦é…ç½®æ–‡ä»¶

1. **tasa_config.py** - GPT Backboneé…ç½®
2. **tasa_config_llama.py** - Llama Backboneé…ç½®
3. **tasa_config_qwen.py** - Qwen Backboneé…ç½®
4. **llm_client_unified.py** - ç»Ÿä¸€LLMå®¢æˆ·ç«¯

### éœ€è¦ä¿®æ”¹çš„é…ç½®

åœ¨æ–°æœºå™¨ä¸Šï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼š

1. **API Endpoints**
   - æ›´æ–°`tasa_config_*.py`ä¸­çš„`ENDPOINT`å’Œ`GPT_ENDPOINT`
   - æ›´æ–°`llm_client_unified.py`ä¸­çš„`LLAMA_URL`å’Œ`QWEN_URL`

2. **æ–‡ä»¶è·¯å¾„**
   - å¦‚æœé¡¹ç›®ä¸åœ¨`/mnt/localssd`ï¼Œéœ€è¦æ›´æ–°æ‰€æœ‰é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
   - æœç´¢å¹¶æ›¿æ¢ï¼š`/mnt/localssd` â†’ `ä½ çš„é¡¹ç›®è·¯å¾„`

3. **API Keys**
   - æ›´æ–°`API_KEY`å˜é‡

---

## ğŸ“Š æ•°æ®å‡†å¤‡

### å­¦ç”Ÿé“¶è¡Œæ•°æ®

éœ€è¦å‡†å¤‡ä»¥ä¸‹æ•°æ®æ–‡ä»¶ï¼š

```
bank/
â”œâ”€â”€ persona/           # å­¦ç”Ÿäººè®¾æ•°æ®
â”‚   â”œâ”€â”€ [dataset]/
â”‚   â”‚   â”œâ”€â”€ [student_id].json
â”‚   â”‚   â””â”€â”€ embeddings/[student_id]_description.npz
â”œâ”€â”€ memory/            # å­¦ç”Ÿè®°å¿†æ•°æ®
â”‚   â”œâ”€â”€ [dataset]/
â”‚   â”‚   â”œâ”€â”€ [student_id].json
â”‚   â”‚   â””â”€â”€ embeddings/[student_id]_description.npz
â””â”€â”€ session/           # å­¦ä¹ ä¼šè¯æ•°æ®
    â””â”€â”€ [dataset]/
        â””â”€â”€ [student_id]_learning_history.json
```

### æ•°æ®é›†

æ”¯æŒçš„æ•°æ®é›†ï¼š
- Assist2017
- NIPS Task 3&4
- Algebra2005
- Bridge2006

---

## ğŸš€ è¿è¡Œæµ‹è¯•

### 1. æµ‹è¯•BGE Embeddings

```bash
python << 'EOF'
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
texts = ["Hello world", "Test embedding"]
embeddings = model.encode(texts)

print(f"âœ… BGE-M3 working")
print(f"   Embedding shape: {embeddings['dense_vecs'].shape}")
EOF
```

### 2. æµ‹è¯•LLM APIè°ƒç”¨

```bash
# æµ‹è¯•Llama
curl -X POST https://YOUR_LLAMA_NGROK_URL/predict/ \
  -H "Content-Type: application/json" \
  -d '{"system_prompt": "You are a helpful assistant.", "user_prompt": "Hello!"}'

# æµ‹è¯•Qwen
curl -X POST https://YOUR_QWEN_NGROK_URL/predict/ \
  -H "Content-Type: application/json" \
  -d '{"system_prompt": "You are a helpful assistant.", "user_prompt": "Hello!"}'
```

### 3. è¿è¡Œå•ä¸ªå­¦ç”Ÿè¯„ä¼°

```bash
# æµ‹è¯•Vanilla-ICL baseline
python baseline_evaluation_conservative.py \
  --method Vanilla-ICL \
  --dataset assist2017 \
  --students-file qualified_students_assist2017_sampled10.json \
  --max-workers 2 \
  --backbone-suffix=-llama
```

### 4. è¿è¡Œå®Œæ•´Baselineè¯„ä¼°

```bash
# Llama Backbone
nohup python run_all_baselines_llama.py > logs/baseline_llama.log 2>&1 &

# Qwen Backbone
nohup python run_all_baselines_qwen.py > logs/baseline_qwen.log 2>&1 &

# ç›‘æ§è¿›åº¦
bash check_both_baselines.sh
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: CUDAä¸å¯ç”¨
```bash
# æ£€æŸ¥NVIDIAé©±åŠ¨
nvidia-smi

# é‡æ–°å®‰è£…PyTorch (ç¡®ä¿CUDAç‰ˆæœ¬åŒ¹é…)
pip uninstall torch torchvision torchaudio
pip install torch==2.5.0 --index-url https://download.pytorch.org/whl/cu124
```

### Q2: FlagEmbeddingæ¨¡å‹ä¸‹è½½æ…¢
```bash
# è®¾ç½®HuggingFaceé•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
pip install -U huggingface_hub

# æˆ–è€…æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åæŒ‡å®šæœ¬åœ°è·¯å¾„
```

### Q3: pykt-toolkitå¯¼å…¥é”™è¯¯
```bash
# ç¡®ä¿åœ¨pykt-toolkitç›®å½•ä¸‹å®‰è£…
cd /path/to/pykt-toolkit
pip install -e .

# éªŒè¯PYTHONPATH
echo $PYTHONPATH
```

### Q4: APIè¿æ¥è¶…æ—¶
```bash
# å¢åŠ httpx timeout
# åœ¨llm_client_unified.pyä¸­ä¿®æ”¹ï¼š
# TIMEOUT = 300  # å¢åŠ åˆ°5åˆ†é’Ÿ
```

### Q5: å†…å­˜ä¸è¶³ (OOM)
```bash
# å‡å°‘max_workers
# åœ¨baselineè„šæœ¬ä¸­ä¿®æ”¹ï¼š
# MAX_WORKERS = 5  # ä»10å‡å°‘åˆ°5

# æˆ–è€…ä½¿ç”¨fp16ç²¾åº¦
# use_fp16=True (BGEæ¨¡å‹)
```

---

## ğŸ“š å…¶ä»–èµ„æº

- **PyKT Documentation**: https://pykt-toolkit.readthedocs.io/
- **BGE Models**: https://huggingface.co/BAAI
- **Transformers**: https://huggingface.co/docs/transformers/

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ—¥å¿—æ–‡ä»¶ï¼š`logs/`ç›®å½•
2. GPUçŠ¶æ€ï¼š`nvidia-smi`
3. è¿›ç¨‹çŠ¶æ€ï¼š`ps aux | grep python`
4. ç£ç›˜ç©ºé—´ï¼š`df -h`

---

**æœ€åæ›´æ–°**: 2025-10-22  
**ç‰ˆæœ¬**: 1.0

