#!/usr/bin/env python3
"""
å­¦ç”ŸRole-Playè¯„ä¼°ç³»ç»Ÿ
è®©LLMæ‰®æ¼”å­¦ç”Ÿå›ç­”é—®é¢˜ï¼Œç„¶åç”¨å¦ä¸€ä¸ªLLMæ‰“åˆ†
"""

import json
import os
from pathlib import Path
from openai import OpenAI
from typing import List, Dict, Tuple
from tqdm import tqdm
import time

# å¯¼å…¥é…ç½®
try:
    from roleplay_config import (
        ENDPOINT, API_KEY, STUDENT_MODEL, GRADER_MODEL,
        STUDENT_TEMPERATURE, GRADER_TEMPERATURE, SLEEP_BETWEEN_QUESTIONS
    )
except ImportError:
    print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°roleplay_config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    print("   è¯·å¤åˆ¶roleplay_config.pyå¹¶å¡«å†™ä½ çš„APIå‡­è¯")
    ENDPOINT = "<Insert your endpoint>"
    API_KEY = "<Insert your key>"
    STUDENT_MODEL = "gpt-oss-120b"
    GRADER_MODEL = "gpt-4o-mini"
    STUDENT_TEMPERATURE = 0.7
    GRADER_TEMPERATURE = 0.3
    SLEEP_BETWEEN_QUESTIONS = 0.5

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key="Bearer " + API_KEY,
    base_url=ENDPOINT,
)

def load_concept_questions(filepath: str) -> Dict:
    """åŠ è½½concept questions"""
    with open(filepath) as f:
        return json.load(f)

def load_session(filepath: str) -> Dict:
    """åŠ è½½sessionæ•°æ®"""
    with open(filepath) as f:
        return json.load(f)

def build_student_system_prompt(session: Dict) -> str:
    """æ„å»ºå­¦ç”Ÿçš„system prompt"""
    persona_desc = session['persona']['description']
    concept_text = session['concept_text']
    delta_t_minutes = session.get('delta_t_minutes', 0)
    accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total'] * 100
    total_attempts = session['persona']['stats']['total']
    correct_attempts = session['persona']['stats']['correct']
    
    # æå–è®°å¿†ä¿¡æ¯
    memory = session.get('memory', [])
    recent_performance = []
    if memory:
        for mem in memory[-5:]:  # æœ€è¿‘5æ¬¡
            result = "correct" if mem['response'] == 1 else "incorrect"
            recent_performance.append(f"- {mem['description']} (Result: {result})")
    
    memory_context = "\n".join(recent_performance) if recent_performance else "No recent history available."
    
    # åŠ¨æ€è®¡ç®—æœŸæœ›æ­£ç¡®ç‡ï¼ˆæ›´æ¥è¿‘å†å²å‡†ç¡®ç‡ï¼‰
    expected_correct_out_of_10 = round(accuracy / 10)  # 27% -> 3é¢˜, 65% -> 7é¢˜
    expected_wrong_out_of_10 = 10 - expected_correct_out_of_10
    
    # æ ¹æ®å‡†ç¡®ç‡åŠ¨æ€è°ƒæ•´æè¿°
    if accuracy < 40:
        level = "STRUGGLING"
        confidence = "very low confidence and frequent confusion"
        error_rate = "most of your answers (about 6-7 out of 10)"
    elif accuracy < 60:
        level = "DEVELOPING"
        confidence = "moderate uncertainty and occasional mistakes"
        error_rate = "many of your answers (about 4-5 out of 10)"
    elif accuracy < 80:
        level = "COMPETENT"
        confidence = "reasonable confidence with some gaps"
        error_rate = "some of your answers (about 2-3 out of 10)"
    else:
        level = "STRONG"
        confidence = "high confidence with minor errors"
        error_rate = "a few of your answers (about 1-2 out of 10)"
    
    system_prompt = f"""You are a {level} student with {accuracy:.1f}% accuracy on {concept_text}.

**YOUR ROLE:**
{persona_desc}

**Performance Profile:**
- Historical accuracy: {accuracy:.1f}% ({correct_attempts}/{total_attempts} attempts)
- Time since last attempt: {delta_t_minutes:.1f} minutes
- Expected performance: Get about {expected_correct_out_of_10} out of 10 questions correct

**Recent History:**
{memory_context}

**HOW TO ANSWER QUESTIONS:**

Your answers should reflect your {accuracy:.1f}% accuracy level:

1. **Answer Distribution (out of 10 questions):**
   - Correct: ~{expected_correct_out_of_10} questions (matching your {accuracy:.1f}% rate)
   - Wrong: ~{expected_wrong_out_of_10} questions

2. **Common Mistakes at Your Level:**"""
    
    # æ ¹æ®æ°´å¹³æ·»åŠ å…·ä½“çš„é”™è¯¯æ¨¡å¼
    if accuracy < 40:
        system_prompt += f"""
   - Confuse basic concepts regularly
   - Mix up formulas and apply incorrectly
   - Make frequent calculation errors
   - Give incomplete or wrong explanations
   - Show {confidence}
   
3. **Your Answer Style:**
   Start with uncertainty: "Um...", "I think...", "Maybe...", "I'm not sure..."
   Often give wrong answers due to genuine confusion
   Sometimes second-guess yourself from right to wrong"""
    
    elif accuracy < 60:
        system_prompt += f"""
   - Occasionally confuse similar concepts
   - Sometimes apply formulas incorrectly
   - Make some calculation errors
   - Miss some key details
   - Show {confidence}
   
3. **Your Answer Style:**
   Sometimes show uncertainty: "I think...", "Probably..."
   Get harder questions wrong, easier questions right
   Show partial understanding with gaps"""
    
    elif accuracy < 80:
        system_prompt += f"""
   - Occasionally miss subtle details
   - Sometimes make minor calculation errors
   - Usually understand core concepts
   - May struggle with complex applications
   - Show {confidence}
   
3. **Your Answer Style:**
   Generally confident but acknowledge uncertainty when unsure
   Get most standard questions right
   May struggle with tricky or complex questions"""
    
    else:
        system_prompt += f"""
   - Rarely make mistakes
   - Strong understanding of concepts
   - Occasional minor errors on complex problems
   - Comprehensive explanations
   - Show {confidence}
   
3. **Your Answer Style:**
   Answer confidently and accurately
   Provide clear explanations
   Very rarely make mistakes"""
    
    system_prompt += f"""

**CRITICAL: Maintain ~{accuracy:.1f}% accuracy**
- This means {error_rate} should be WRONG
- Answer naturally based on your knowledge level
- Don't try to be perfect - make realistic mistakes for your level
- Match your historical performance pattern"""
    
    return system_prompt

def get_student_answers(system_prompt: str, questions: List[str], concept: str) -> List[Dict]:
    """è®©LLM role-playå­¦ç”Ÿå›ç­”é—®é¢˜"""
    answers = []
    
    print(f"\nğŸ­ Student role-playing answers for '{concept}'...")
    
    for i, question in enumerate(tqdm(questions, desc="Answering", ncols=100)):
        try:
            response = client.chat.completions.create(
                model=STUDENT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Question: {question}\n\nPlease provide your answer:"}
                ],
                temperature=STUDENT_TEMPERATURE,
            )
            
            # å®‰å…¨åœ°è·å–ç­”æ¡ˆå†…å®¹
            content = response.choices[0].message.content
            if content is None:
                answer = "[Error: API returned empty response]"
                print(f"\nâš ï¸  Warning: Question {i+1} returned None content")
            else:
                answer = content.strip()
            
            answers.append({
                "question_number": i + 1,
                "question": question,
                "student_answer": answer
            })
            
            # é¿å…rate limiting
            time.sleep(SLEEP_BETWEEN_QUESTIONS)
            
        except Exception as e:
            print(f"\nâŒ Error answering question {i+1}: {e}")
            answers.append({
                "question_number": i + 1,
                "question": question,
                "student_answer": "[Error: Could not generate answer]"
            })
    
    return answers

def grade_answers(answers: List[Dict], concept: str) -> Tuple[float, str]:
    """ä½¿ç”¨LLMæ‰¹æ”¹ç­”æ¡ˆå¹¶ç»™å‡ºåˆ†æ•°"""
    print(f"\nğŸ“ Grading answers...")
    
    # æ„å»ºæ‰¹æ”¹prompt
    answers_text = "\n\n".join([
        f"Question {ans['question_number']}: {ans['question']}\nStudent Answer: {ans['student_answer']}"
        for ans in answers
    ])
    
    grading_prompt = f"""You are an expert teacher grading a student's answers on the topic: {concept}

Please evaluate the following {len(answers)} answers and provide:
1. A score for each answer: ONLY 1 (correct) or 0 (incorrect) - NO partial credit (no 0.5)
2. An overall score out of 10 (sum of individual scores, must be a whole number: 0, 1, 2, ..., 10)
3. Brief feedback on the student's understanding

**IMPORTANT GRADING RULES:**
- Each answer is either CORRECT (1 point) or INCORRECT (0 points)
- NO partial scores like 0.5 are allowed
- If an answer is mostly correct but has minor errors, you must decide: is it correct enough? If yes, give 1; if no, give 0
- The total score MUST be a whole number (integer)

**Answers to Grade:**

{answers_text}

**Output Format (JSON):**
{{
    "individual_scores": [score1, score2, ..., score{len(answers)}],
    "total_score": X,
    "feedback": "Brief overall assessment of student's understanding and common mistakes"
}}

Note: individual_scores must contain only 0 or 1, and total_score must be an integer.
Provide ONLY the JSON output, no other text."""
    
    try:
        response = client.chat.completions.create(
            model=GRADER_MODEL,
            messages=[
                {"role": "system", "content": "You are an expert teacher grading student work. Be fair and thorough."},
                {"role": "user", "content": grading_prompt}
            ],
            temperature=GRADER_TEMPERATURE,
            max_tokens=1000
        )
        
        grading_result = response.choices[0].message.content.strip()
        
        # å°è¯•è§£æJSON
        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if "```json" in grading_result:
            grading_result = grading_result.split("```json")[1].split("```")[0].strip()
        elif "```" in grading_result:
            grading_result = grading_result.split("```")[1].split("```")[0].strip()
        
        grading_data = json.loads(grading_result)
        
        total_score = grading_data.get('total_score', 0.0)
        feedback = grading_data.get('feedback', '')
        individual_scores = grading_data.get('individual_scores', [])
        
        return total_score, feedback, individual_scores
        
    except Exception as e:
        print(f"\nâŒ Error grading answers: {e}")
        return 0.0, "Error during grading", []

def evaluate_session(session_file: str, concept_questions: Dict, output_dir: str):
    """è¯„ä¼°å•ä¸ªsession"""
    # åŠ è½½session
    session = load_session(session_file)
    
    student_id = session['student_id']
    concept_id = str(session['concept_id'])
    concept_text = session['concept_text']
    
    print(f"\n{'='*80}")
    print(f"è¯„ä¼°å­¦ç”Ÿ: {student_id} | Concept: {concept_text}")
    print(f"{'='*80}")
    
    # è·å–å¯¹åº”çš„é—®é¢˜
    if concept_id not in concept_questions:
        print(f"âš ï¸  Warning: No questions found for concept_id {concept_id}")
        return None
    
    questions_data = concept_questions[concept_id]
    questions = questions_data['questions']
    
    print(f"ğŸ“š Concept: {questions_data['concept_description']}")
    print(f"ğŸ“Š Student Performance: {session['persona']['stats']['correct']}/{session['persona']['stats']['total']} correct")
    
    # æ„å»ºstudent prompt
    system_prompt = build_student_system_prompt(session)
    
    # è·å–å­¦ç”Ÿç­”æ¡ˆ
    student_answers = get_student_answers(system_prompt, questions, concept_text)
    
    # æ‰¹æ”¹ç­”æ¡ˆ
    total_score, feedback, individual_scores = grade_answers(student_answers, concept_text)
    
    # å‡†å¤‡ç»“æœ
    result = {
        "student_id": student_id,
        "concept_id": concept_id,
        "concept_text": concept_text,
        "concept_description": questions_data['concept_description'],
        "original_accuracy": session['persona']['stats']['correct'] / session['persona']['stats']['total'],
        "roleplay_score": total_score,
        "individual_scores": individual_scores,
        "feedback": feedback,
        "answers": student_answers,
        "session_info": {
            "delta_t_minutes": session.get('delta_t_minutes', 0),
            "num_attempts": session.get('num_attempts', 0),
            "last_response": session.get('last_response', None)
        }
    }
    
    # ä¿å­˜ç»“æœ
    output_file = Path(output_dir) / f"student_{student_id}_concept_{concept_id}.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
    print(f"   åŸå§‹å‡†ç¡®ç‡: {result['original_accuracy']*100:.1f}%")
    print(f"   Role-playå¾—åˆ†: {total_score}/10 ({total_score*10:.1f}%)")
    print(f"   ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    print(f"\nğŸ’¬ åé¦ˆ: {feedback}\n")
    
    return result

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("å­¦ç”ŸRole-Playè¯„ä¼°ç³»ç»Ÿ")
    print("="*80)
    
    # åŠ è½½concept questions
    concept_questions_file = '/mnt/localssd/bank/test_data/assist2017/concept_questions.json'
    print(f"\nğŸ“– åŠ è½½é¢˜åº“: {concept_questions_file}")
    concept_questions = load_concept_questions(concept_questions_file)
    print(f"   âœ… åŠ è½½äº† {len(concept_questions)} ä¸ªconceptsçš„é¢˜ç›®")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = '/mnt/localssd/bank/evaluation_results/assist2017'
    
    # ç¤ºä¾‹ï¼šè¯„ä¼°å•ä¸ªsession
    session_file = '/mnt/localssd/bank/session/assist2017/1.json'
    
    print(f"\nğŸ¯ å¼€å§‹è¯„ä¼°...")
    result = evaluate_session(session_file, concept_questions, output_dir)
    
    if result:
        print(f"\n{'='*80}")
        print("è¯„ä¼°å®Œæˆ!")
        print(f"{'='*80}")

if __name__ == '__main__':
    main()

