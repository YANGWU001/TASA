#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åˆ›å»ºå­¦ç”ŸPersonaå’ŒMemory Bank - æœ€ç»ˆç‰ˆ
- Temperature = 1.0
- æ¯ä¸ªå­¦ç”Ÿå•ç‹¬æ–‡ä»¶
- ä½¿ç”¨çœŸå®conceptæ–‡æœ¬æè¿°
"""

import os
import json
import pandas as pd
import numpy as np
from collections import defaultdict
from tqdm import tqdm
try:
    from FlagEmbedding import BGEM3FlagModel
except ImportError:
    # Fallback for older FlagEmbedding versions (Python 3.7 compatibility)
    from FlagEmbedding import FlagModel as BGEM3FlagModel

import openai
import time

# LLMé…ç½®
ENDPOINT = ""  # Your API endpoint
KEY = ""  # Your API key
MODEL = "gpt-4o"
TEMPERATURE = 1.0  # ä¿®æ”¹ä¸º1.0

# é…ç½®æ—§ç‰ˆopenai
openai.api_key = f"Bearer {KEY}"
openai.api_base = ENDPOINT

# å¹¶è¡Œå¤„ç†é…ç½®
MAX_WORKERS = 30  # æŒ‰å­¦ç”Ÿå¹¶è¡Œçš„è¿›ç¨‹æ•°ï¼ˆæ¯ä¸ªè¿›ç¨‹å¤„ç†å®Œæ•´å­¦ç”Ÿæµç¨‹ï¼šLLM + BGE + ä¿å­˜ï¼‰

# æ•°æ®é›†é…ç½® - åŒ…å«train_validå’Œtest
DATASETS = {
    'assist2017': {
        'data_paths': [
            '/mnt/localssd/pykt-toolkit/data/assist2017/train_valid_sequences.csv',
            '/mnt/localssd/pykt-toolkit/data/assist2017/test_sequences.csv'
        ],
        'keyid2idx_path': '/mnt/localssd/pykt-toolkit/data/assist2017/keyid2idx.json',
    },
    'nips_task34': {
        'data_paths': [
            '/mnt/localssd/pykt-toolkit/data/nips_task34/train_valid_sequences.csv',
            '/mnt/localssd/pykt-toolkit/data/nips_task34/test_sequences.csv'
        ],
        'keyid2idx_path': '/mnt/localssd/pykt-toolkit/data/nips_task34/keyid2idx.json',
    },
    'algebra2005': {
        'data_paths': [
            '/mnt/localssd/pykt-toolkit/data/algebra2005/train_valid_sequences.csv',
            '/mnt/localssd/pykt-toolkit/data/algebra2005/test_sequences.csv'
        ],
        'keyid2idx_path': '/mnt/localssd/pykt-toolkit/data/algebra2005/keyid2idx.json',
    },
    'bridge2006': {
        'data_paths': [
            '/mnt/localssd/pykt-toolkit/data/bridge2algebra2006/train_valid_sequences.csv',
            '/mnt/localssd/pykt-toolkit/data/bridge2algebra2006/test_sequences.csv'
        ],
        'keyid2idx_path': '/mnt/localssd/pykt-toolkit/data/bridge2algebra2006/keyid2idx.json',
    }
}

# å…¨å±€å˜é‡
bge_model = None
llm_client = None

def init_models():
    """åˆå§‹åŒ–æ¨¡å‹"""
    global bge_model, llm_client
    
    if bge_model is None:
        print("åŠ è½½BGE-M3æ¨¡å‹...")
        # æ—§ç‰ˆFlagModel (v1.1.6) ä¸æ¥å—device/deviceså‚æ•°ï¼Œè‡ªåŠ¨ä½¿ç”¨CUDA
        bge_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
        print("BGE-M3åŠ è½½å®Œæˆ")
    
    if llm_client is None:
        print("åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        # æ—§ç‰ˆopenaiä¸éœ€è¦åˆ›å»ºclientï¼Œç›´æ¥ä½¿ç”¨æ¨¡å—çº§API
        llm_client = True  # æ ‡è®°å·²åˆå§‹åŒ–
        print("LLMå®¢æˆ·ç«¯å°±ç»ª")
    
    return bge_model, llm_client

def load_concept_mapping(dataset_name, keyid2idx_path):
    """åŠ è½½concept IDåˆ°æ–‡æœ¬çš„æ˜ å°„"""
    try:
        with open(keyid2idx_path, 'r') as f:
            keyid2idx = json.load(f)
        
        # åˆ›å»ºä»idxåˆ°conceptæ–‡æœ¬çš„åå‘æ˜ å°„
        if 'concepts' in keyid2idx:
            idx2concept = {v: k for k, v in keyid2idx['concepts'].items()}
            
            # å¯¹äº nips_task34ï¼Œconcept æ˜¯æ•°å­— IDï¼Œéœ€è¦æ˜ å°„åˆ°å®é™…çš„ subject åç§°
            if dataset_name == 'nips_task34':
                metadata_file = '/mnt/localssd/pykt-toolkit/data/nips_task34/metadata/subject_metadata.csv'
                if os.path.exists(metadata_file):
                    import pandas as pd
                    df_subject = pd.read_csv(metadata_file)
                    subject_map = {}
                    for _, row in df_subject.iterrows():
                        subject_id = str(row['SubjectId'])
                        name = row['Name']
                        subject_map[subject_id] = name
                    
                    # å°† idx2concept ä¸­çš„æ•°å­— ID æ›¿æ¢ä¸ºå®é™…åç§°
                    new_idx2concept = {}
                    for idx, concept_id in idx2concept.items():
                        if concept_id in subject_map:
                            new_idx2concept[idx] = subject_map[concept_id]
                        else:
                            new_idx2concept[idx] = concept_id
                    idx2concept = new_idx2concept
                    
                    print(f"  {dataset_name}: ğŸ“‹ åŠ è½½äº† {len(subject_map)} ä¸ª Subject æ˜ å°„")
            
            print(f"  {dataset_name}: åŠ è½½äº† {len(idx2concept)} ä¸ªconceptæ˜ å°„")
            return idx2concept
        else:
            print(f"  {dataset_name}: æœªæ‰¾åˆ°conceptæ˜ å°„")
            return {}
    except Exception as e:
        print(f"  {dataset_name}: åŠ è½½æ˜ å°„å¤±è´¥ - {e}")
        return {}

def get_concept_text(concept_id, idx2concept):
    """è·å–conceptçš„æ–‡æœ¬æè¿°"""
    if concept_id in idx2concept:
        return idx2concept[concept_id]
    else:
        return f"Concept {concept_id}"

def parse_csv_field(field_str):
    """è§£æCSVå­—æ®µ"""
    if pd.isna(field_str) or field_str == 'NA' or field_str == '':
        return []
    
    try:
        values = str(field_str).strip().split(',')
        result = []
        for v in values:
            v = v.strip()
            if v and v != '-1' and v != 'NA':
                try:
                    result.append(int(v))
                except ValueError:
                    result.append(v)
        return result
    except:
        return []

def extract_student_data(row, dataset_name, idx2concept):
    """æå–å­¦ç”Ÿæ•°æ®ï¼Œæ’é™¤æœ€åä¸€æ¬¡ç­”é¢˜"""
    uid = row['uid']
    
    questions = parse_csv_field(row['questions'])
    concepts = parse_csv_field(row['concepts'])
    responses = parse_csv_field(row['responses'])
    timestamps = parse_csv_field(row['timestamps'])
    
    # æ„å»ºäº¤äº’
    interactions = []
    for i in range(min(len(questions), len(concepts), len(responses), len(timestamps))):
        interactions.append({
            'question_id': questions[i],
            'concept_id': concepts[i],
            'concept_text': get_concept_text(concepts[i], idx2concept),
            'response': responses[i],
            'timestamp': timestamps[i],
            'index': i
        })
    
    # æŒ‰conceptåˆ†ç»„
    concept_groups = defaultdict(list)
    for inter in interactions:
        concept_groups[inter['concept_id']].append(inter)
    
    # åˆ†ç¦»å†å²å’Œæœ€åä¸€æ¬¡
    history = []
    last_interactions = {}
    
    for cid, inters in concept_groups.items():
        concept_text = inters[0]['concept_text']
        if len(inters) > 1:
            history.extend(inters[:-1])
            last_interactions[str(cid)] = {
                'concept_id': cid,
                'concept_text': concept_text,
                'question_id': inters[-1]['question_id'],
                'response': inters[-1]['response'],
                'timestamp': inters[-1]['timestamp']
            }
        elif len(inters) == 1:
            # åªæœ‰ä¸€æ¬¡ï¼Œä¿å­˜ä¸ºlastä½†ä¸ç”¨äºpersona/memory
            last_interactions[str(cid)] = {
                'concept_id': cid,
                'concept_text': concept_text,
                'question_id': inters[0]['question_id'],
                'response': inters[0]['response'],
                'timestamp': inters[0]['timestamp']
            }
    
    return {
        'uid': uid,
        'history': history,
        'last_interactions': last_interactions
    }

def generate_persona_llm(uid, history, dataset_name):
    """ä½¿ç”¨LLMç”Ÿæˆpersona"""
    # æŒ‰conceptç»Ÿè®¡
    stats = defaultdict(lambda: {'correct': 0, 'total': 0, 'concept_text': ''})
    for inter in history:
        cid = inter['concept_id']
        stats[cid]['total'] += 1
        stats[cid]['concept_text'] = inter['concept_text']
        if inter['response'] == 1:
            stats[cid]['correct'] += 1
    
    if not stats:
        return []
    
    # æ„å»ºpromptï¼ˆä½¿ç”¨çœŸå®conceptæ–‡æœ¬ï¼‰
    system_prompt = """You are an educational analyst. Summarize each concept's mastery level in 2 sentences based on performance data. Be concise and specific."""
    
    user_prompt = f"Student {uid} ({dataset_name}):\n\n"
    for cid, s in list(stats.items())[:20]:  # é™åˆ¶20ä¸ªconcept
        acc = s['correct'] / s['total'] if s['total'] > 0 else 0
        user_prompt += f"{s['concept_text']}: {s['correct']}/{s['total']} ({acc:.0%})\n"
    
    user_prompt += "\nFor each concept above, provide: 1) Overall mastery (excellent/good/struggling), 2) One insight. Format: 'Concept name: [2 sentences]'"
    
    try:
        response = openai.ChatCompletion.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=TEMPERATURE,  # ä½¿ç”¨1.0
            max_tokens=800
        )
        
        content = response['choices'][0]['message']['content']
        
        # æ£€æŸ¥contentæ˜¯å¦ä¸ºç©º
        if not content or content.strip() == "":
            raise ValueError(f"LLMè¿”å›ç©ºå“åº”ã€‚Responseå¯¹è±¡: {response}")
        
        # è§£æå“åº”ï¼Œåˆ›å»ºpersona
        personas = []
        for cid, s in stats.items():
            acc = s['correct'] / s['total'] if s['total'] > 0 else 0
            if acc >= 0.8:
                level = "excellent mastery"
            elif acc >= 0.6:
                level = "good understanding"
            else:
                level = "needs improvement"
            
            concept_text = s['concept_text']
            desc = f"Student shows {level} of '{concept_text}' with {acc:.0%} accuracy over {s['total']} attempts."
            
            personas.append({
                'concept_id': int(cid),
                'concept_text': concept_text,
                'description': desc,
                'keywords': concept_text,
                'stats': {
                    'correct': s['correct'],
                    'total': s['total']
                }
            })
        
        return personas
    
    except Exception as e:
        print(f"  LLMç”Ÿæˆpersonaå¤±è´¥: {e}")
        return []

def generate_memory_llm(uid, history, dataset_name):
    """ä½¿ç”¨LLMç”Ÿæˆæ›´è‡ªç„¶çš„memoryæè¿°"""
    memories = []
    
    # é™åˆ¶æ•°é‡ï¼Œåˆ†æ‰¹å¤„ç†
    max_memories = 50
    sample_history = history[:max_memories]
    
    if not sample_history:
        return []
    
    # æ„å»ºpromptï¼Œè®©LLMç”Ÿæˆè‡ªç„¶çš„äº‹ä»¶æè¿°
    system_prompt = """You are creating natural event descriptions for a student's learning journey. 

For each learning event, write a brief, natural description (one sentence) that varies in style. Include:
- What concept was practiced
- Whether they succeeded or struggled
- Use varied phrasing (e.g., "tackled", "worked on", "attempted", "solved", "struggled with", "mastered")
- Be concise but natural

IMPORTANT: Always use "The student" as the subject. Do NOT use pronouns like "They", "He", "She", or "Their".

Examples:
- "The student tackled an equation-solving problem."
- "The student demonstrated understanding of proportion."
- "The student struggled with geometry basics."

Vary your language - don't repeat the same patterns."""

    # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ¬¡10ä¸ªäº‹ä»¶ï¼‰
    batch_size = 10
    
    # ä¸åŒçš„é£æ ¼æç¤ºï¼Œä¸ºæ¯æ‰¹éšæœºé€‰æ‹©
    style_hints = [
        "Use active voice and action verbs like 'tackled', 'mastered', 'struggled with'.",
        "Focus on the learning process: 'attempted', 'worked through', 'practiced'.",
        "Emphasize outcomes: 'succeeded in', 'got right', 'missed', 'nailed'.",
        "Use casual academic tone: 'answered correctly', 'made an error on', 'solved'.",
        "Be descriptive: 'demonstrated understanding', 'showed proficiency', 'had difficulty'.",
        "Mix metaphors: 'cracked', 'fumbled', 'aced', 'stumbled on'."
    ]
    
    for batch_idx, batch_start in enumerate(range(0, len(sample_history), batch_size)):
        batch = sample_history[batch_start:batch_start + batch_size]
        
        # ä¸ºè¿™æ‰¹é€‰æ‹©ä¸€ä¸ªé£æ ¼ï¼ˆä½¿ç”¨æ‰¹æ¬¡ç´¢å¼•å’Œå­¦ç”ŸIDæ¥"éšæœº"é€‰æ‹©ï¼‰
        style_idx = (batch_idx + hash(uid)) % len(style_hints)
        current_style = style_hints[style_idx]
        
        user_prompt = f"Student {uid} ({dataset_name}) learning events:\n\n"
        for i, inter in enumerate(batch, 1):
            concept_text = inter['concept_text']
            result = "correctly" if inter['response'] == 1 else "incorrectly"
            user_prompt += f"{i}. Concept: '{concept_text}', Result: {result}\n"
        
        user_prompt += f"""\nFor each event above, write ONE natural sentence describing what happened. 
{current_style}
Return JSON format:
{{
  "memories": [
    {{"index": 1, "description": "<natural sentence>"}},
    {{"index": 2, "description": "<natural sentence>"}},
    ...
  ]
}}"""
        
        try:
            response = openai.ChatCompletion.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,  # ä½¿ç”¨0.7ä»¥è·å¾—æ›´è‡ªç„¶çš„å˜åŒ–
                max_tokens=500
            )
            
            content = response['choices'][0]['message']['content']
            
            # æ£€æŸ¥contentæ˜¯å¦ä¸ºç©º
            if not content or content.strip() == "":
                raise ValueError(f"LLMè¿”å›ç©ºå“åº”ã€‚Responseå¯¹è±¡: {response}")
            
            # å»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°ï¼ˆGPT-4oåœ¨ä½¿ç”¨json_objectæ ¼å¼æ—¶ä¼šæ·»åŠ ï¼‰
            content = content.strip()
            if content.startswith("```json"):
                content = content[7:]  # ç§»é™¤ ```json
            if content.startswith("```"):
                content = content[3:]  # ç§»é™¤ ```
            if content.endswith("```"):
                content = content[:-3]  # ç§»é™¤ç»“å°¾çš„ ```
            content = content.strip()
            
            result = json.loads(content)
            
            # åŒ¹é…LLMç”Ÿæˆçš„æè¿°åˆ°åŸå§‹äº¤äº’
            if 'memories' in result:
                for mem in result['memories']:
                    idx = mem.get('index', 1) - 1
                    if idx < len(batch):
                        inter = batch[idx]
                        memories.append({
                            'concept_id': int(inter['concept_id']),
                            'concept_text': inter['concept_text'],
                            'description': mem.get('description', ''),
                            'keywords': inter['concept_text'],
                            'question_id': inter['question_id'],
                            'response': inter['response'],
                            'timestamp': inter['timestamp']
                        })
        
        except Exception as e:
            print(f"  LLMç”Ÿæˆmemoryå¤±è´¥ï¼Œä½¿ç”¨åå¤‡æ–¹æ¡ˆ: {e}")
            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨å˜åŒ–çš„æ¨¡æ¿
            templates_correct = [
                "Successfully solved a {} problem.",
                "Correctly answered a question on {}.",
                "Demonstrated understanding of {} by answering correctly.",
                "Tackled a {} question and got it right.",
                "Showed mastery of {} in this attempt."
            ]
            templates_incorrect = [
                "Struggled with a {} question.",
                "Made an error on a {} problem.",
                "Found {} challenging in this attempt.",
                "Attempted {} but answered incorrectly.",
                "Had difficulty with a {} question."
            ]
            
            for inter in batch:
                concept_text = inter['concept_text']
                if inter['response'] == 1:
                    templates = templates_correct
                else:
                    templates = templates_incorrect
                
                # ä½¿ç”¨äº¤äº’ç´¢å¼•æ¥"éšæœº"é€‰æ‹©æ¨¡æ¿
                template_idx = (inter['question_id'] + inter['concept_id']) % len(templates)
                desc = templates[template_idx].format(concept_text)
                
                memories.append({
                    'concept_id': int(inter['concept_id']),
                    'concept_text': concept_text,
                    'description': desc,
                    'keywords': concept_text,
                    'question_id': inter['question_id'],
                    'response': inter['response'],
                    'timestamp': inter['timestamp']
                })
    
    return memories

def generate_embeddings_batch(texts, model):
    """æ‰¹é‡ç”Ÿæˆembeddings"""
    if not texts:
        return []
    
    try:
        # æ—§ç‰ˆFlagModel (v1.1.6) çš„encode()æ–¹æ³•ä¸æ¥å—return_denseç­‰å‚æ•°
        # ç›´æ¥è°ƒç”¨encodeä¼šè¿”å›dense embeddings
        result = model.encode(texts, batch_size=min(32, len(texts)))
        return result
    except Exception as e:
        print(f"  Embeddingç”Ÿæˆå¤±è´¥: {e}")
        return None

def save_student_files(uid, personas, memories, last_interactions, dataset_name, bge_model):
    """ä¿å­˜å•ä¸ªå­¦ç”Ÿçš„æ‰€æœ‰æ–‡ä»¶"""
    base_dir = f"/mnt/localssd/bank"
    
    # Personaæ•°æ®æ–‡ä»¶
    persona_data_file = f"{base_dir}/persona/{dataset_name}/data/{uid}.json"
    os.makedirs(os.path.dirname(persona_data_file), exist_ok=True)
    with open(persona_data_file, 'w') as f:
        json.dump(personas, f, indent=2)
    
    # Persona embeddings - åˆ†åˆ«ä¿å­˜descriptionå’Œkeywords
    if personas:
        desc_texts = [p['description'] for p in personas]
        key_texts = [p['keywords'] for p in personas]
        
        desc_embs = generate_embeddings_batch(desc_texts, bge_model)
        key_embs = generate_embeddings_batch(key_texts, bge_model)
        
        emb_dir = f"{base_dir}/persona/{dataset_name}/embeddings"
        os.makedirs(emb_dir, exist_ok=True)
        
        if desc_embs is not None:
            desc_emb_file = f"{emb_dir}/{uid}_description.npz"
            np.savez_compressed(desc_emb_file, embeddings=desc_embs)
        
        if key_embs is not None:
            key_emb_file = f"{emb_dir}/{uid}_keywords.npz"
            np.savez_compressed(key_emb_file, embeddings=key_embs)
    
    # Memoryæ•°æ®æ–‡ä»¶
    memory_data_file = f"{base_dir}/memory/{dataset_name}/data/{uid}.json"
    os.makedirs(os.path.dirname(memory_data_file), exist_ok=True)
    with open(memory_data_file, 'w') as f:
        json.dump(memories, f, indent=2)
    
    # Memory embeddings - åˆ†åˆ«ä¿å­˜descriptionå’Œkeywords
    if memories:
        desc_texts = [m['description'] for m in memories]
        key_texts = [m['keywords'] for m in memories]
        
        desc_embs = generate_embeddings_batch(desc_texts, bge_model)
        key_embs = generate_embeddings_batch(key_texts, bge_model)
        
        emb_dir = f"{base_dir}/memory/{dataset_name}/embeddings"
        os.makedirs(emb_dir, exist_ok=True)
        
        if desc_embs is not None:
            desc_emb_file = f"{emb_dir}/{uid}_description.npz"
            np.savez_compressed(desc_emb_file, embeddings=desc_embs)
        
        if key_embs is not None:
            key_emb_file = f"{emb_dir}/{uid}_keywords.npz"
            np.savez_compressed(key_emb_file, embeddings=key_embs)
    
    # Last interactions
    last_file = f"{base_dir}/persona/{dataset_name}/last_interactions/{uid}.json"
    os.makedirs(os.path.dirname(last_file), exist_ok=True)
    with open(last_file, 'w') as f:
        json.dump(last_interactions, f, indent=2)

# å…¨å±€BGEæ¨¡å‹ï¼ˆæ¯ä¸ªworkerè¿›ç¨‹lazyåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
_worker_bge_model = None

def get_worker_bge_model():
    """è·å–å½“å‰workerè¿›ç¨‹çš„BGEæ¨¡å‹ï¼ˆlazy initializationï¼‰"""
    global _worker_bge_model
    if _worker_bge_model is None:
        print(f"  [Worker {os.getpid()}] åˆå§‹åŒ–BGEæ¨¡å‹...")
        _worker_bge_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
    return _worker_bge_model

def process_student_complete(row, dataset_name, idx2concept):
    """å¤„ç†å•ä¸ªå­¦ç”Ÿçš„å®Œæ•´æµç¨‹ï¼šLLMç”Ÿæˆ + BGE embedding + ä¿å­˜"""
    try:
        # æå–æ•°æ®
        data = extract_student_data(row, dataset_name, idx2concept)
        uid = str(data['uid'])
        
        if len(data['history']) == 0:
            # è·å–BGEæ¨¡å‹ï¼ˆåªç”¨äºä¿å­˜ç©ºæ–‡ä»¶ï¼‰
            bge_model = get_worker_bge_model()
            save_student_files(uid, [], [], data['last_interactions'], dataset_name, bge_model)
            return {'uid': uid, 'status': 'skipped', 'reason': 'no_history'}
        
        # 1. ç”Ÿæˆpersonaï¼ˆLLMè°ƒç”¨ï¼‰
        personas = generate_persona_llm(uid, data['history'], dataset_name)
        
        # 2. ç”Ÿæˆmemoryï¼ˆLLMè°ƒç”¨ï¼‰
        memories = generate_memory_llm(uid, data['history'], dataset_name)
        
        # 3. è·å–BGEæ¨¡å‹å¹¶ç”Ÿæˆembeddings + ä¿å­˜
        bge_model = get_worker_bge_model()
        save_student_files(uid, personas, memories, data['last_interactions'], dataset_name, bge_model)
        
        return {'uid': uid, 'status': 'success'}
    
    except Exception as e:
        print(f"  å¤„ç†å­¦ç”Ÿå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {'uid': row.get('uid', 'unknown'), 'status': 'error', 'error': str(e)}

def process_dataset(dataset_name, config, max_students=None):
    """å¤„ç†å•ä¸ªæ•°æ®é›†ï¼ˆåŒ…å«train_validå’Œtestï¼‰"""
    print(f"\n{'='*60}")
    print(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
    print(f"{'='*60}\n")
    
    data_paths = config['data_paths']
    keyid2idx_path = config['keyid2idx_path']
    
    # åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼ˆtrain_validå’Œtestï¼‰
    all_dfs = []
    for data_path in data_paths:
        if os.path.exists(data_path):
            print(f"åŠ è½½æ•°æ®: {data_path}")
            df_part = pd.read_csv(data_path)
            all_dfs.append(df_part)
            print(f"  å­¦ç”Ÿæ•°: {len(df_part)}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    if not all_dfs:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    df = pd.concat(all_dfs, ignore_index=True)
    print(f"\næ€»è®°å½•æ•°ï¼ˆtrain_valid + testï¼‰: {len(df)}")
    
    # ç»Ÿè®¡å”¯ä¸€å­¦ç”Ÿæ•°
    unique_students = df['uid'].nunique()
    print(f"å”¯ä¸€å­¦ç”Ÿæ•°: {unique_students}")
    
    # å¦‚æœåŒä¸€ä¸ªå­¦ç”Ÿæœ‰å¤šæ¡è®°å½•ï¼Œéœ€è¦åˆå¹¶
    if len(df) > unique_students:
        print(f"æ£€æµ‹åˆ°åŒä¸€å­¦ç”Ÿæœ‰å¤šæ¡è®°å½•ï¼Œæ­£åœ¨åˆå¹¶... ({len(df)}æ¡è®°å½• â†’ {unique_students}ä¸ªå­¦ç”Ÿ)")
        
        # æŒ‰å­¦ç”ŸIDåˆ†ç»„å¹¶åˆå¹¶æ•°æ®
        merged_data = []
        for uid, group in df.groupby('uid'):
            # åˆå¹¶concepts, responses, timestampsç­‰å­—æ®µ
            merged_row = {}
            merged_row['uid'] = uid
            merged_row['fold'] = group.iloc[0]['fold']  # ä½¿ç”¨ç¬¬ä¸€æ¡çš„fold
            
            # åˆå¹¶æ‰€æœ‰åºåˆ—å­—æ®µ
            for col in ['questions', 'concepts', 'responses', 'timestamps', 'usetimes', 'selectmasks', 'is_repeat']:
                if col in group.columns:
                    # åˆå¹¶æ‰€æœ‰è¡Œçš„è¯¥å­—æ®µï¼ˆå»é™¤-1çš„å ä½ç¬¦ï¼‰
                    all_vals = []
                    for val in group[col]:
                        if pd.notna(val) and val != '' and str(val) != '-1':
                            vals = [v.strip() for v in str(val).split(',') if v.strip() != '-1' and v.strip() != '']
                            all_vals.extend(vals)
                    merged_row[col] = ','.join(all_vals) if all_vals else ''
            
            merged_data.append(merged_row)
        
        df = pd.DataFrame(merged_data)
        print(f"åˆå¹¶å®Œæˆ: {len(df)}ä¸ªå­¦ç”Ÿ")
    
    if max_students:
        df = df.head(max_students)
        print(f"æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰{max_students}ä¸ªå­¦ç”Ÿ")
    
    # åŠ è½½conceptæ˜ å°„
    idx2concept = load_concept_mapping(dataset_name, keyid2idx_path)
    
    # æŒ‰å­¦ç”Ÿå¹¶è¡Œå¤„ç†ï¼ˆæ¯ä¸ªè¿›ç¨‹å¤„ç†å®Œæ•´æµç¨‹ï¼šLLM + BGE + ä¿å­˜ï¼‰
    print(f"\nå¼€å§‹å¹¶è¡Œå¤„ç†å­¦ç”Ÿï¼ˆ{MAX_WORKERS}ä¸ªè¿›ç¨‹ï¼‰...")
    print(f"  - æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹å¤„ç†ï¼šLLMç”Ÿæˆ â†’ BGE embedding â†’ ä¿å­˜æ–‡ä»¶")
    print(f"  - BGEæ¨¡å‹ï¼šæ¯ä¸ªworkerè¿›ç¨‹åˆå§‹åŒ–ä¸€æ¬¡ï¼Œç„¶åå¤ç”¨")
    
    from multiprocessing import Pool
    from functools import partial
    
    process_func = partial(process_student_complete, 
                          dataset_name=dataset_name, 
                          idx2concept=idx2concept)
    
    results = []
    with Pool(processes=MAX_WORKERS) as pool:
        for result in tqdm(pool.imap_unordered(process_func, [row for _, row in df.iterrows()]), 
                          total=len(df), 
                          desc=f"{dataset_name}"):
            results.append(result)
    
    # ç»Ÿè®¡
    success = sum(1 for r in results if r['status'] == 'success')
    skipped = sum(1 for r in results if r['status'] == 'skipped')
    errors = sum(1 for r in results if r['status'] == 'error')
    
    print(f"\nâœ… {dataset_name} å®Œæˆ: æˆåŠŸ{success}, è·³è¿‡{skipped}, é”™è¯¯{errors}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("  åˆ›å»ºå­¦ç”ŸPersonaå’ŒMemory Bank - æœ€ç»ˆç‰ˆ")
    print("="*60)
    print(f"\né…ç½®:")
    print(f"  - Temperature: {TEMPERATURE}")
    print(f"  - å­˜å‚¨: æ¯ä¸ªå­¦ç”Ÿå•ç‹¬æ–‡ä»¶")
    print(f"  - Concept: ä½¿ç”¨çœŸå®æ–‡æœ¬æè¿°")
    
    # æµ‹è¯•æ¨¡å¼
    TEST_MODE = False
    max_students = None if TEST_MODE else None
    
    if TEST_MODE:
        print(f"\nâš ï¸  æµ‹è¯•æ¨¡å¼ï¼šæ¯ä¸ªæ•°æ®é›†å¤„ç†{max_students}ä¸ªå­¦ç”Ÿ\n")
    
    for dataset_name, config in DATASETS.items():
        try:
            process_dataset(dataset_name, config, max_students)
        except Exception as e:
            print(f"\nâŒ {dataset_name}å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*60)
    print("  âœ… æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")
    print("="*60)

if __name__ == '__main__':
    main()

