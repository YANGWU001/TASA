#!/usr/bin/env python
"""
æµ‹è¯•å…¶ä»–KTæ¨¡å‹ï¼ˆsimpleKT, DKT, AKTï¼‰ç”¨äºForgetting Scoreè®¡ç®—
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import random
from datetime import timedelta

# è®¾ç½®éšæœºç§å­
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# å¯¼å…¥æ¨¡å‹åŠ è½½å‡½æ•°
from pykt.models.init_model import init_model

# æ•°æ®é›†é…ç½®
DATASETS = {
    'assist2017': {
        'name': 'ASSISTments2017',
        'data_path': '/mnt/localssd/pykt-toolkit/data/assist2017',
        'tau_days': 3.21,
        'models': {
            'simplekt': '/mnt/localssd/pykt-toolkit/examples/saved_model/assist2017_simplekt_qid_saved_model_42_0_0.1_256_256_2_4_0.5_0.5_0.5_50_256_256_4_2_0.0001_1_0',
            'dkt': '/mnt/localssd/pykt-toolkit/examples/saved_model/assist2017_dkt_qid_saved_model_42_0_0.2_200_0.001_1_0',
            'akt': '/mnt/localssd/pykt-toolkit/examples/saved_model/assist2017_akt_qid_saved_model_3407_0_0.2_256_512_8_4_0.0001_1_0',
        }
    },
}

def load_model(model_name, model_path, num_q, num_c, device):
    """åŠ è½½æŒ‡å®šçš„KTæ¨¡å‹"""
    config_path = os.path.join(model_path, 'config.json')
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„checkpointæ–‡ä»¶å
    checkpoint_names = ['qid_model.ckpt', 'model.ckpt', 'best_model.ckpt']
    checkpoint_path = None
    for name in checkpoint_names:
        path = os.path.join(model_path, name)
        if os.path.exists(path):
            checkpoint_path = path
            break
    
    if not checkpoint_path:
        print(f"  âŒ æœªæ‰¾åˆ°checkpointæ–‡ä»¶")
        return None
    
    # è¯»å–é…ç½®
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    model_config = config.get('model_config', {})
    params = config.get('params', {})
    data_config = {
        'num_q': num_q,
        'num_c': num_c,
        'emb_path': '',
    }
    
    try:
        # å®šä¹‰è®­ç»ƒç›¸å…³çš„å‚æ•°ï¼ˆä¸æ˜¯æ¨¡å‹æ¶æ„å‚æ•°ï¼‰
        training_params = {
            'learning_rate', 'use_wandb', 'add_uuid', 'batch_size', 
            'num_epochs', 'optimizer', 'seq_len', 'emb_type', 'emb_path',
            'dataset_name', 'model_name', 'save_dir', 'seed', 'fold'
        }
        
        # åªä¿ç•™æ¨¡å‹æ¶æ„å‚æ•°
        clean_model_config = {k: v for k, v in model_config.items() 
                              if k not in training_params}
        
        print(f"  æ¨¡å‹å‚æ•°: {list(clean_model_config.keys())}")
        
        # ä½¿ç”¨init_modelå‡½æ•°
        model = init_model(model_name, clean_model_config, data_config, emb_type='qid')
        
        if model is None:
            print(f"  âŒ init_modelè¿”å›None")
            return None
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint, strict=False)
        model.to(device)
        model.eval()
        
        print(f"  âœ… æˆåŠŸåŠ è½½ {model_name.upper()} æ¨¡å‹")
        return model
        
    except Exception as e:
        print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def predict_with_model(model, model_name, device, questions, concepts, responses, num_q, num_c):
    """ä½¿ç”¨KTæ¨¡å‹é¢„æµ‹"""
    if len(questions) < 2:
        return None
    
    try:
        # å‡†å¤‡è¾“å…¥ï¼ˆä¸åŒ…æ‹¬æœ€åä¸€æ¬¡ï¼‰
        q_seq = questions[:-1]
        c_seq = concepts[:-1]
        r_seq = responses[:-1]
        
        # æ£€æŸ¥IDèŒƒå›´
        if max(q_seq) >= num_q or max(c_seq) >= num_c:
            return None
        
        # è½¬æ¢ä¸ºtensor
        q_tensor = torch.LongTensor([q_seq]).to(device)
        c_tensor = torch.LongTensor([c_seq]).to(device)
        r_tensor = torch.LongTensor([r_seq]).to(device)
        
        # æ„å»ºshiftedç‰ˆæœ¬
        qshft = torch.cat([torch.zeros(1, 1, dtype=torch.long).to(device), q_tensor[:, :-1]], dim=1)
        cshft = torch.cat([torch.zeros(1, 1, dtype=torch.long).to(device), c_tensor[:, :-1]], dim=1)
        rshft = torch.cat([torch.zeros(1, 1, dtype=torch.long).to(device), r_tensor[:, :-1]], dim=1)
        
        # æ‹¼æ¥
        cq = torch.cat([q_tensor[:, 0:1], qshft], dim=1)
        cc = torch.cat([c_tensor[:, 0:1], cshft], dim=1)
        cr = torch.cat([r_tensor[:, 0:1], rshft], dim=1)
        
        with torch.no_grad():
            if model_name == 'simplekt':
                # SimpleKTä½¿ç”¨dcurå­—å…¸
                dcur = {
                    'qseqs': q_tensor,
                    'cseqs': c_tensor,
                    'rseqs': r_tensor,
                    'shft_qseqs': qshft,
                    'shft_cseqs': cshft,
                    'shft_rseqs': rshft,
                }
                y = model(dcur)
                y = y[:, 1:]  # è·³è¿‡ç¬¬ä¸€ä¸ª
                
            elif model_name == 'dkt':
                # DKT: y = model(cc, cr, cq)
                y = model(cc.long(), cr.long(), cq.long())
                y = y[:, 1:]
                
            elif model_name == 'akt':
                # AKT: y, reg_loss = model(cc, cr, cq)
                y, reg_loss = model(cc.long(), cr.long(), cq.long())
                y = y[:, 1:]
            
            # è·å–æœ€åä¸€ä¸ªé¢„æµ‹
            pred_prob = torch.sigmoid(y[0, -1]).item()
            return pred_prob
            
    except Exception as e:
        # print(f"    âš ï¸ {model_name} é¢„æµ‹å¤±è´¥: {e}")
        return None

def test_single_student(dataset_key, model_name, student_id=None):
    """æµ‹è¯•å•ä¸ªå­¦ç”Ÿ"""
    dataset_info = DATASETS[dataset_key]
    
    print(f"\n{'='*100}")
    print(f"æ•°æ®é›†: {dataset_info['name']} | æ¨¡å‹: {model_name.upper()}")
    print(f"{'='*100}")
    
    # åŠ è½½æ•°æ®
    data_file = os.path.join(dataset_info['data_path'], 'train_valid_sequences.csv')
    df = pd.read_csv(data_file)
    
    # è·å–num_qå’Œnum_c
    all_questions = []
    all_concepts = []
    for _, row in df.iterrows():
        questions = list(map(int, str(row['questions']).split(',')))
        concepts = list(map(int, str(row['concepts']).split(',')))
        all_questions.extend(questions)
        all_concepts.extend(concepts)
    
    num_q = max(all_questions) + 1
    num_c = max(all_concepts) + 1
    
    print(f"æ•°æ®ç»Ÿè®¡: num_q={num_q}, num_c={num_c}, å­¦ç”Ÿæ•°={len(df)}")
    
    # åŠ è½½æ¨¡å‹
    device = torch.device('cpu')
    model_path = dataset_info['models'][model_name]
    model = load_model(model_name, model_path, num_q, num_c, device)
    
    if model is None:
        return
    
    # é€‰æ‹©ä¸€ä¸ªå­¦ç”Ÿ
    if student_id is None:
        # é€‰æ‹©æœ‰å¤šä¸ªconceptsçš„å­¦ç”Ÿ
        valid_students = []
        for idx, row in df.iterrows():
            concepts = list(map(int, str(row['concepts']).split(',')))
            if len(set(concepts)) >= 5:  # è‡³å°‘5ä¸ªä¸åŒçš„concepts
                valid_students.append(idx)
        
        if not valid_students:
            print("  âŒ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„å­¦ç”Ÿ")
            return
        
        student_idx = random.choice(valid_students[:50])
    else:
        student_idx = df[df['uid'] == student_id].index[0]
    
    row = df.iloc[student_idx]
    uid = row['uid']
    
    print(f"\næµ‹è¯•å­¦ç”Ÿ: {uid}")
    print(f"{'-'*100}")
    
    # è§£ææ•°æ®
    questions = list(map(int, str(row['questions']).split(',')))
    concepts = list(map(int, str(row['concepts']).split(',')))
    responses = list(map(int, str(row['responses']).split(',')))
    timestamps = list(map(int, str(row['timestamps']).split(',')))
    
    # æŒ‰conceptåˆ†ç»„
    concept_data = {}
    for i in range(len(concepts)):
        c = concepts[i]
        if c not in concept_data:
            concept_data[c] = {
                'questions': [],
                'responses': [],
                'timestamps': [],
                'indices': []
            }
        concept_data[c]['questions'].append(questions[i])
        concept_data[c]['responses'].append(responses[i])
        concept_data[c]['timestamps'].append(timestamps[i])
        concept_data[c]['indices'].append(i)
    
    # è®¡ç®—æ¯ä¸ªconceptçš„FS
    tau_minutes = dataset_info['tau_days'] * 24 * 60
    
    results = []
    success_count = 0
    fallback_count = 0
    
    for concept, data in concept_data.items():
        if len(data['indices']) < 2:
            continue
        
        # è·å–æœ€åä¸¤æ¬¡çš„ç´¢å¼•
        last_idx = data['indices'][-1]
        second_last_idx = data['indices'][-2]
        
        # æ—¶é—´é—´éš”
        delta_t = (data['timestamps'][-1] - data['timestamps'][-2]) / (1000 * 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿ
        
        # ä½¿ç”¨æ¨¡å‹é¢„æµ‹s_t,cï¼ˆä½¿ç”¨åˆ°å€’æ•°ç¬¬äºŒæ¬¡çš„æ‰€æœ‰æ•°æ®ï¼‰
        seq_end_idx = second_last_idx + 1
        q_seq = questions[:seq_end_idx]
        c_seq = concepts[:seq_end_idx]
        r_seq = responses[:seq_end_idx]
        
        s_tc = predict_with_model(model, model_name, device, q_seq, c_seq, r_seq, num_q, num_c)
        
        if s_tc is None:
            # Fallbackåˆ°å†å²å‡†ç¡®ç‡
            s_tc = sum(data['responses'][:-1]) / len(data['responses'][:-1]) if len(data['responses']) > 1 else 0.5
            method = 'ğŸ“Šå†å²'
            fallback_count += 1
        else:
            method = f'ğŸ¤–{model_name}'
            success_count += 1
        
        # è®¡ç®—FS
        time_factor = delta_t / (delta_t + tau_minutes)
        fs = (1 - s_tc) * time_factor
        
        results.append({
            'concept': concept,
            'count': len(data['indices']),
            's_tc': s_tc,
            'method': method,
            'delta_t': delta_t,
            'time_factor': time_factor,
            'fs': fs,
            'last_correct': data['responses'][-1] == 1,
        })
    
    # æ’åºå¹¶æ˜¾ç¤º
    results.sort(key=lambda x: x['fs'], reverse=True)
    
    print(f"\nForgetting Scoreç»Ÿè®¡:")
    print(f"  æ¨¡å‹é¢„æµ‹æˆåŠŸ: {success_count}ä¸ª ({success_count*100/(success_count+fallback_count):.1f}%)")
    print(f"  å›é€€åˆ°å†å²: {fallback_count}ä¸ª ({fallback_count*100/(success_count+fallback_count):.1f}%)")
    
    if results:
        fs_values = [r['fs'] for r in results]
        print(f"  å¹³å‡FS: {np.mean(fs_values):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(fs_values):.4f}")
        print(f"  èŒƒå›´: [{np.min(fs_values):.4f}, {np.max(fs_values):.4f}]")
    
    print(f"\nå‰10ä¸ªæœ€éœ€è¦å¤ä¹ çš„Concepts:")
    print(f"  {'Concept':<10} {'æ¬¡æ•°':<8} {'é¢„æµ‹æ¦‚ç‡':<12} {'æ–¹æ³•':<15} {'é—´éš”':<15} {'FS':<10} {'æœ€å':<8}")
    print(f"  {'-'*90}")
    
    for r in results[:10]:
        delta_str = format_time_interval(r['delta_t'])
        last_str = 'âœ…' if r['last_correct'] else 'âŒ'
        print(f"  {r['concept']:<10} {r['count']:<8} {r['s_tc']*100:<11.1f}% {r['method']:<15} "
              f"{delta_str:<15} {r['fs']:<10.4f} {last_str:<8}")

def format_time_interval(minutes):
    """æ ¼å¼åŒ–æ—¶é—´é—´éš”"""
    if minutes < 1:
        return f"{minutes*60:.0f}s"
    elif minutes < 60:
        return f"{minutes:.1f}m"
    elif minutes < 1440:
        return f"{minutes/60:.1f}h"
    else:
        return f"{minutes/1440:.1f}d"

if __name__ == '__main__':
    print("="*100)
    print("æµ‹è¯•å…¶ä»–KTæ¨¡å‹ï¼ˆsimpleKT, DKT, AKTï¼‰ç”¨äºForgetting Scoreé¢„æµ‹")
    print("="*100)
    
    # æµ‹è¯•æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹
    for model_name in ['simplekt', 'dkt', 'akt']:
        test_single_student('assist2017', model_name)
    
    print(f"\n{'='*100}")
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*100}")

