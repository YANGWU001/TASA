"""
ä¿å®ˆç‰ˆæœ¬çš„Baselineè¯„ä¼°
- è¿è¡Œ2æ¬¡post-testï¼Œä½¿ç”¨å¹³å‡åˆ†ï¼ˆä¸æ˜¯æœ€é«˜åˆ†ï¼‰
- åŒ…å«è´Ÿæ•°learning gainï¼ˆä¸æ’é™¤ï¼‰
- æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œ
"""

import os
import sys
import json
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import threading

# å¯¼å…¥baselineæ¨¡å—
from baseline_vanilla_icl import VanillaICLTutor
from baseline_mathchat import MathChatTutor
from baseline_tutorllm import TutorLLM
from baseline_pssmv import PSSMV

# å¯¼å…¥è¯„ä¼°æ¨¡å—
from student_roleplay_evaluation import build_student_system_prompt, load_session, grade_answers
from openai import OpenAI

# æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©é…ç½®æ–‡ä»¶
_config_module = os.environ.get('TASA_CONFIG', 'tasa_config')
if _config_module == 'tasa_config_llama':
    from tasa_config_llama import ENDPOINT, GPT_ENDPOINT, API_KEY, STUDENT_MODEL, STUDENT_TEMPERATURE, TUTOR_MODEL
elif _config_module == 'tasa_config_qwen':
    from tasa_config_qwen import ENDPOINT, GPT_ENDPOINT, API_KEY, STUDENT_MODEL, STUDENT_TEMPERATURE, TUTOR_MODEL
elif _config_module == 'tasa_config_gpt':
    from tasa_config_gpt import ENDPOINT, API_KEY, STUDENT_MODEL, STUDENT_TEMPERATURE, TUTOR_MODEL
    GPT_ENDPOINT = ENDPOINT
else:
    from tasa_config import ENDPOINT, API_KEY, STUDENT_MODEL, STUDENT_TEMPERATURE, TUTOR_MODEL
    GPT_ENDPOINT = ENDPOINT

def get_backbone_suffix():
    """æ ¹æ®TUTOR_MODELç¡®å®šbackboneåç¼€"""
    if 'llama' in TUTOR_MODEL.lower():
        return '-llama'
    elif 'qwen' in TUTOR_MODEL.lower():
        return '-qwen'
    else:
        return ''  # GPTé»˜è®¤æ— åç¼€

# å…¨å±€é”å’Œçº¿ç¨‹æœ¬åœ°å­˜å‚¨
print_lock = Lock()
model_init_lock = Lock()
thread_local = threading.local()

def safe_print(msg):
    """çº¿ç¨‹å®‰å…¨çš„æ‰“å°"""
    with print_lock:
        print(msg)

def get_tutor(method):
    """è·å–çº¿ç¨‹æœ¬åœ°çš„tutorå®ä¾‹"""
    attr_name = f'tutor_{method}'
    if not hasattr(thread_local, attr_name):
        with model_init_lock:
            safe_print(f"   [Thread-{threading.current_thread().ident}] åˆå§‹åŒ–{method} Tutor...")
            if method == 'Vanilla-ICL':
                setattr(thread_local, attr_name, VanillaICLTutor())
            elif method == 'MathChat':
                setattr(thread_local, attr_name, MathChatTutor())
            elif method == 'TutorLLM':
                setattr(thread_local, attr_name, TutorLLM())
            elif method == 'PSS-MV':
                setattr(thread_local, attr_name, PSSMV())
    return getattr(thread_local, attr_name)

def get_client():
    """è·å–çº¿ç¨‹æœ¬åœ°çš„OpenAI client (ç”¨äºStudent roleplayï¼Œå›ºå®šä½¿ç”¨GPT)"""
    if not hasattr(thread_local, 'client'):
        with model_init_lock:
            safe_print(f"   [Thread-{threading.current_thread().ident}] åˆå§‹åŒ–OpenAI Client (GPT)...")
            safe_print(f"   [Thread-{threading.current_thread().ident}] GPT_ENDPOINT={GPT_ENDPOINT}")
            safe_print(f"   [Thread-{threading.current_thread().ident}] STUDENT_MODEL={STUDENT_MODEL}")
            thread_local.client = OpenAI(api_key=API_KEY, base_url=GPT_ENDPOINT)
    return thread_local.client

def conduct_post_test_single_run(student_id: int, dataset: str, method: str, 
                                  dialogue: list, concept_text: str, 
                                  concept_id: str, student_prompt: str) -> float:
    """
    è¿›è¡Œå•æ¬¡post-testè¯„ä¼°
    
    Returns:
        post_test_accuracy: float
    """
    # è·å–client
    client = get_client()
    
    # æå–å­¦åˆ°çš„çŸ¥è¯†ï¼ˆå‰3ä¸ªtutorå›å¤ï¼‰
    tutor_explanations = []
    for msg in dialogue:
        if msg['role'] == 'assistant' and msg['round'] > 1 and msg['round'] <= 4:
            content = msg['content']
            explanation = content[:300] if len(content) > 300 else content
            tutor_explanations.append(explanation)
    
    learned_knowledge = "\n\n".join([f"- {exp}" for exp in tutor_explanations[:3]])
    
    # Post-test prompt (å¹³è¡¡ç‰ˆæœ¬ï¼šæœ‰è¿›æ­¥ä½†æœªå®Œå…¨æŒæ¡)
    enhanced_prompt = f"""{student_prompt}

[IMPORTANT UPDATE: You Have Just Had a Tutoring Session]

You have just completed a tutoring session on {concept_text}. The tutor covered:

{learned_knowledge}

**Your understanding has IMPROVED, but is NOT complete.** The tutoring session was helpful:
- You learned some of the key concepts
- You can now solve SOME problems better than before
- However, you still struggle with certain aspects
- Your mastery is MODERATE, not full

When answering the following questions:
- Apply what you learned from the tutoring session
- You should perform SOMEWHAT better than before
- You may get some questions right that you couldn't before
- But you will still make mistakes on harder questions
- Show moderate improvement with continued uncertainty on complex problems"""
    
    # åŠ è½½æµ‹è¯•é¢˜ç›®
    questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
    with open(questions_file) as f:
        all_questions = json.load(f)
    
    questions = all_questions[concept_id]['questions']
    
    # è®©å­¦ç”Ÿå›ç­”é—®é¢˜
    answers = []
    for i, question in enumerate(questions):
        answer_prompt = f"Question: {question}\n\nProvide your answer:"
        
        try:
            response = client.chat.completions.create(
                model=STUDENT_MODEL,
                messages=[
                    {"role": "system", "content": enhanced_prompt},
                    {"role": "user", "content": answer_prompt}
                ],
                temperature=STUDENT_TEMPERATURE,
                max_tokens=800
            )
            
            if response.choices[0].message.content:
                answer = response.choices[0].message.content.strip()
            else:
                answer = "[No response]"
            
            answers.append({
                'question_number': i + 1,
                'question': question,
                'student_answer': answer
            })
        except Exception as e:
            answers.append({
                'question_number': i + 1,
                'question': question,
                'student_answer': "[Error]"
            })
    
    # è¯„åˆ†
    total_score, feedback, individual_scores = grade_answers(answers, concept_text)
    post_test_accuracy = total_score / len(questions)
    
    return post_test_accuracy

def evaluate_single_student_conservative(student_id: int, dataset: str, method: str) -> dict:
    """
    è¯„ä¼°å•ä¸ªå­¦ç”Ÿï¼ˆä¿å®ˆç‰ˆæœ¬ï¼š2æ¬¡post-testå–å¹³å‡ï¼‰
    """
    try:
        safe_print(f"\n{'='*80}")
        safe_print(f"ğŸ“Š è¯„ä¼°å­¦ç”Ÿ {student_id} - {method} - {dataset}")
        safe_print(f"{'='*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        concept_text = session['concept_text']
        concept_id = str(session['concept_id'])
        student_prompt = build_student_system_prompt(session)
        
        # æ£€æŸ¥dialogueæ˜¯å¦å­˜åœ¨ï¼ˆæ ¹æ®backboneç¡®å®šè·¯å¾„ï¼‰
        backbone_suffix = get_backbone_suffix()
        dialogue_file = f'/mnt/localssd/bank/dialogue/{method}{backbone_suffix}/{dataset}/{student_id}-{concept_text}.json'
        
        if not os.path.exists(dialogue_file):
            safe_print(f"   ğŸ“ ç”Ÿæˆdialogue...")
            tutor = get_tutor(method)
            success = tutor.conduct_tutoring_session(student_id, dataset, concept_text, student_prompt)
            
            if not success:
                safe_print(f"   âŒ Dialogueç”Ÿæˆå¤±è´¥")
                return None
        else:
            safe_print(f"   âœ… Dialogueå·²å­˜åœ¨")
        
        # åŠ è½½dialogue
        with open(dialogue_file) as f:
            dialogue_data = json.load(f)
        
        dialogue = dialogue_data['dialogue']
        
        # åŠ è½½pre-testç»“æœ
        pretest_file = f"/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json"
        with open(pretest_file) as f:
            pretest_data = json.load(f)
        
        pre_test_accuracy = pretest_data['roleplay_accuracy']
        original_accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total']
        
        # è¿›è¡Œ2æ¬¡post-test
        safe_print(f"   ğŸ“Š è¿›è¡ŒPost-testè¯„ä¼° (2æ¬¡)")
        
        post_test_results = []
        for run_id in range(1, 3):
            safe_print(f"   Run {run_id}/2")
            post_acc = conduct_post_test_single_run(
                student_id, dataset, method, dialogue, 
                concept_text, concept_id, student_prompt
            )
            post_test_results.append(post_acc)
            safe_print(f"   Run {run_id} å‡†ç¡®ç‡: {post_acc*100:.1f}%")
        
        # è®¡ç®—å¹³å‡post-testå‡†ç¡®ç‡
        avg_post_test_accuracy = np.mean(post_test_results)
        
        # è®¡ç®—æœ€ä½post-testå‡†ç¡®ç‡ï¼ˆæœ€ä¿å®ˆç­–ç•¥ï¼‰
        min_post_test_accuracy = min(post_test_results)
        
        # è®¡ç®—æœ€é«˜post-testå‡†ç¡®ç‡ï¼ˆæœ€æ¿€è¿›ç­–ç•¥ï¼‰
        max_post_test_accuracy = max(post_test_results)
        
        # è®¡ç®—learning gainï¼ˆåŒ…å«è´Ÿæ•°ï¼‰- åŸºäºå¹³å‡åˆ†
        if pre_test_accuracy >= 1.0:
            learning_gain_avg = 0.0
        else:
            learning_gain_avg = (avg_post_test_accuracy - pre_test_accuracy) / (1.0 - pre_test_accuracy)
        
        # è®¡ç®—learning gainï¼ˆåŒ…å«è´Ÿæ•°ï¼‰- åŸºäºæœ€ä½åˆ†
        if pre_test_accuracy >= 1.0:
            learning_gain_min = 0.0
        else:
            learning_gain_min = (min_post_test_accuracy - pre_test_accuracy) / (1.0 - pre_test_accuracy)
        
        # è®¡ç®—learning gainï¼ˆåŒ…å«è´Ÿæ•°ï¼‰- åŸºäºæœ€é«˜åˆ†
        if pre_test_accuracy >= 1.0:
            learning_gain_max = 0.0
        else:
            learning_gain_max = (max_post_test_accuracy - pre_test_accuracy) / (1.0 - pre_test_accuracy)
        
        improvement_avg = avg_post_test_accuracy - pre_test_accuracy
        improvement_min = min_post_test_accuracy - pre_test_accuracy
        improvement_max = max_post_test_accuracy - pre_test_accuracy
        
        safe_print(f"   âœ… æœ€é«˜Post-test: {max_post_test_accuracy*100:.1f}% (Gain={learning_gain_max*100:.1f}%)")
        safe_print(f"   âœ… å¹³å‡Post-test: {avg_post_test_accuracy*100:.1f}% (Gain={learning_gain_avg*100:.1f}%)")
        safe_print(f"   âœ… æœ€ä½Post-test: {min_post_test_accuracy*100:.1f}% (Gain={learning_gain_min*100:.1f}%)")
        
        # ç»„åˆç»“æœ
        full_result = {
            'student_id': student_id,
            'dataset': dataset,
            'concept_text': concept_text,
            'concept_id': concept_id,
            'method': method,
            'original_accuracy': original_accuracy,
            'pre_test_accuracy': pre_test_accuracy,
            'post_test_run1': post_test_results[0],
            'post_test_run2': post_test_results[1],
            'max_post_test_accuracy': max_post_test_accuracy,
            'avg_post_test_accuracy': avg_post_test_accuracy,
            'min_post_test_accuracy': min_post_test_accuracy,
            'learning_gain_max': learning_gain_max,
            'learning_gain_avg': learning_gain_avg,
            'learning_gain_min': learning_gain_min,
            'improvement_max': improvement_max,
            'improvement_avg': improvement_avg,
            'improvement_min': improvement_min
        }
        
        # ä¿å­˜ç»“æœï¼ˆæ”¯æŒbackboneåç¼€ï¼‰
        backbone_suffix = globals().get('BACKBONE_SUFFIX', '')
        result_dir = f'/mnt/localssd/bank/evaluation_results/{method}-conservative{backbone_suffix}/{dataset}'
        os.makedirs(result_dir, exist_ok=True)
        
        result_file = f'{result_dir}/student_{student_id}.json'
        with open(result_file, 'w') as f:
            json.dump(full_result, f, indent=2)
        
        safe_print(f"   âœ… å­¦ç”Ÿ{student_id}è¯„ä¼°å®Œæˆ")
        
        return full_result
        
    except Exception as e:
        safe_print(f"   âŒ å­¦ç”Ÿ{student_id}è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_batch_evaluation(student_ids, dataset, method, max_workers=20):
    """æ‰¹é‡è¯„ä¼°"""
    print("="*80)
    print(f"ğŸš€ {method} æ‰¹é‡è¯„ä¼° (ä¿å®ˆç‰ˆæœ¬)")
    print("="*80)
    print(f"   æ•°æ®é›†: {dataset}")
    print(f"   å­¦ç”Ÿæ•°: {len(student_ids)}")
    print(f"   å¹¶è¡Œåº¦: {max_workers} workers")
    print(f"   ç­–ç•¥: 2æ¬¡post-testå–å¹³å‡ï¼ŒåŒ…å«è´Ÿæ•°gain")
    print("="*80)
    
    start_time = time.time()
    
    all_results = []
    completed = 0
    failed = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_student = {
            executor.submit(evaluate_single_student_conservative, student_id, dataset, method): student_id
            for student_id in student_ids
        }
        
        for future in as_completed(future_to_student):
            student_id = future_to_student[future]
            
            try:
                result = future.result()
                if result:
                    all_results.append(result)
                    completed += 1
                else:
                    failed += 1
                
                total_processed = completed + failed
                progress = total_processed / len(student_ids) * 100
                
                with print_lock:
                    print(f"\nğŸ“ˆ è¿›åº¦: {total_processed}/{len(student_ids)} ({progress:.1f}%) | æˆåŠŸ: {completed} | å¤±è´¥: {failed}")
                
            except Exception as e:
                failed += 1
                with print_lock:
                    print(f"\nâŒ å­¦ç”Ÿ{student_id}å¤„ç†å¼‚å¸¸: {e}")
    
    # ç”Ÿæˆoverallç»Ÿè®¡
    if all_results:
        generate_overall_stats(all_results, dataset, method)
    
    elapsed = time.time() - start_time
    print(f"\nâœ… æ‰¹é‡è¯„ä¼°å®Œæˆï¼")
    print(f"   æ€»ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
    print(f"   æˆåŠŸ: {completed}/{len(student_ids)}")
    
    return all_results

def generate_overall_stats(results, dataset, method):
    """ç”Ÿæˆoverallç»Ÿè®¡ï¼ˆåŒ…å«æ‰€æœ‰learning gainï¼Œä¸æ’é™¤è´Ÿæ•°ï¼‰"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç”Ÿæˆæ•´ä½“ç»Ÿè®¡")
    print(f"{'='*80}")
    
    # è®¡ç®—ç»Ÿè®¡ï¼ˆåŒ…å«æ‰€æœ‰gainï¼Œä¸æ’é™¤è´Ÿæ•°ï¼‰
    # åŸºäºæœ€é«˜åˆ†çš„ç»Ÿè®¡
    all_learning_gains_max = [r['learning_gain_max'] for r in results]
    
    avg_gain_max = np.mean(all_learning_gains_max)
    std_gain_max = np.std(all_learning_gains_max, ddof=1) if len(all_learning_gains_max) > 1 else 0
    median_gain_max = np.median(all_learning_gains_max)
    
    negative_count_max = len([g for g in all_learning_gains_max if g < 0])
    positive_count_max = len([g for g in all_learning_gains_max if g >= 0])
    
    # åŸºäºå¹³å‡åˆ†çš„ç»Ÿè®¡
    all_learning_gains_avg = [r['learning_gain_avg'] for r in results]
    
    avg_gain_avg = np.mean(all_learning_gains_avg)
    std_gain_avg = np.std(all_learning_gains_avg, ddof=1) if len(all_learning_gains_avg) > 1 else 0
    median_gain_avg = np.median(all_learning_gains_avg)
    
    negative_count_avg = len([g for g in all_learning_gains_avg if g < 0])
    positive_count_avg = len([g for g in all_learning_gains_avg if g >= 0])
    
    # åŸºäºæœ€ä½åˆ†çš„ç»Ÿè®¡
    all_learning_gains_min = [r['learning_gain_min'] for r in results]
    
    avg_gain_min = np.mean(all_learning_gains_min)
    std_gain_min = np.std(all_learning_gains_min, ddof=1) if len(all_learning_gains_min) > 1 else 0
    median_gain_min = np.median(all_learning_gains_min)
    
    negative_count_min = len([g for g in all_learning_gains_min if g < 0])
    positive_count_min = len([g for g in all_learning_gains_min if g >= 0])
    
    overall_stats = {
        "dataset": dataset,
        "method": f"{method}-conservative",
        "num_students": len(results),
        "strategy_max": {
            "name": "æœ€é«˜åˆ†ç­–ç•¥ (2æ¬¡å–æœ€é«˜)",
            "avg_learning_gain": avg_gain_max,
            "std_learning_gain": std_gain_max,
            "median_learning_gain": median_gain_max,
            "min_gain": min(all_learning_gains_max),
            "max_gain": max(all_learning_gains_max),
            "num_positive_gain": positive_count_max,
            "num_negative_gain": negative_count_max
        },
        "strategy_avg": {
            "name": "å¹³å‡åˆ†ç­–ç•¥ (2æ¬¡å–å¹³å‡)",
            "avg_learning_gain": avg_gain_avg,
            "std_learning_gain": std_gain_avg,
            "median_learning_gain": median_gain_avg,
            "min_gain": min(all_learning_gains_avg),
            "max_gain": max(all_learning_gains_avg),
            "num_positive_gain": positive_count_avg,
            "num_negative_gain": negative_count_avg
        },
        "strategy_min": {
            "name": "æœ€ä½åˆ†ç­–ç•¥ (2æ¬¡å–æœ€ä½)",
            "avg_learning_gain": avg_gain_min,
            "std_learning_gain": std_gain_min,
            "median_learning_gain": median_gain_min,
            "min_gain": min(all_learning_gains_min),
            "max_gain": max(all_learning_gains_min),
            "num_positive_gain": positive_count_min,
            "num_negative_gain": negative_count_min
        },
        "note": "åŒ…å«ä¸‰ç§ç­–ç•¥ï¼š1) 2æ¬¡æœ€é«˜åˆ† 2) 2æ¬¡å¹³å‡åˆ† 3) 2æ¬¡æœ€ä½åˆ†ï¼Œå‡åŒ…å«æ‰€æœ‰learning gainï¼ˆå«è´Ÿæ•°ï¼‰",
        "students": results
    }
    
    # ä¿å­˜ï¼ˆæ”¯æŒbackboneåç¼€ï¼‰
    backbone_suffix = globals().get('BACKBONE_SUFFIX', '')
    output_dir = f"/mnt/localssd/bank/evaluation_results/{method}-conservative{backbone_suffix}/{dataset}"
    os.makedirs(output_dir, exist_ok=True)
    output_file = f"{output_dir}/overall.json"
    
    with open(output_file, 'w') as f:
        json.dump(overall_stats, f, indent=2)
    
    print(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡ (åŒ…å«æ‰€æœ‰æ•°æ®):")
    print(f"   å­¦ç”Ÿæ•°: {len(results)}")
    print(f"\n   ğŸ”¹ ç­–ç•¥1: æœ€é«˜åˆ† (2æ¬¡å–æœ€é«˜) - æœ€æ¿€è¿›")
    print(f"      å¹³å‡Learning Gain: {avg_gain_max*100:.1f}% Â± {std_gain_max*100:.1f}%")
    print(f"      ä¸­ä½æ•°: {median_gain_max*100:.1f}%")
    print(f"      èŒƒå›´: [{min(all_learning_gains_max)*100:.1f}%, {max(all_learning_gains_max)*100:.1f}%]")
    print(f"      æ­£å¢é•¿: {positive_count_max}ä¸ª | è´Ÿå¢é•¿: {negative_count_max}ä¸ª")
    
    print(f"\n   ğŸ”¹ ç­–ç•¥2: å¹³å‡åˆ† (2æ¬¡å–å¹³å‡)")
    print(f"      å¹³å‡Learning Gain: {avg_gain_avg*100:.1f}% Â± {std_gain_avg*100:.1f}%")
    print(f"      ä¸­ä½æ•°: {median_gain_avg*100:.1f}%")
    print(f"      èŒƒå›´: [{min(all_learning_gains_avg)*100:.1f}%, {max(all_learning_gains_avg)*100:.1f}%]")
    print(f"      æ­£å¢é•¿: {positive_count_avg}ä¸ª | è´Ÿå¢é•¿: {negative_count_avg}ä¸ª")
    
    print(f"\n   ğŸ”¹ ç­–ç•¥3: æœ€ä½åˆ† (2æ¬¡å–æœ€ä½) - æœ€ä¿å®ˆ")
    print(f"      å¹³å‡Learning Gain: {avg_gain_min*100:.1f}% Â± {std_gain_min*100:.1f}%")
    print(f"      ä¸­ä½æ•°: {median_gain_min*100:.1f}%")
    print(f"      èŒƒå›´: [{min(all_learning_gains_min)*100:.1f}%, {max(all_learning_gains_min)*100:.1f}%]")
    print(f"      æ­£å¢é•¿: {positive_count_min}ä¸ª | è´Ÿå¢é•¿: {negative_count_min}ä¸ª")
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
    
    return overall_stats

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿å®ˆç‰ˆæœ¬Baselineè¯„ä¼°')
    parser.add_argument('--method', type=str, required=True,
                       choices=['Vanilla-ICL', 'MathChat', 'TutorLLM', 'PSS-MV'],
                       help='Baselineæ–¹æ³•')
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†åç§°')
    parser.add_argument('--max-workers', type=int, default=20,
                       help='å¹¶è¡Œåº¦')
    parser.add_argument('--students-file', type=str, default=None,
                       help='å­¦ç”Ÿæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨20to60æ–‡ä»¶ï¼‰')
    parser.add_argument('--backbone-suffix', type=str, default='',
                       help='Backboneåç¼€ï¼ˆå¦‚-llama, -qwenï¼‰ï¼Œç”¨äºåŒºåˆ†ä¸åŒLLMçš„ç»“æœ')
    
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€å˜é‡ä¾›evaluate_single_student_conservativeä½¿ç”¨
    globals()['BACKBONE_SUFFIX'] = args.backbone_suffix
    
    # è¯»å–ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ
    if args.students_file:
        student_file = args.students_file
    else:
        student_file = f'/mnt/localssd/qualified_students_{args.dataset}_20to60.json'
    
    if not os.path.exists(student_file):
        print(f"âŒ å­¦ç”Ÿåˆ—è¡¨ä¸å­˜åœ¨: {student_file}")
        sys.exit(1)
    
    with open(student_file) as f:
        data = json.load(f)
    
    # å…¼å®¹ä¸åŒæ ¼å¼çš„å­¦ç”Ÿæ–‡ä»¶
    if 'sampled_students' in data:
        student_ids = data['sampled_students']
    elif 'students' in data:
        if isinstance(data['students'][0], dict):
            student_ids = [s['student_id'] for s in data['students']]
        else:
            student_ids = data['students']
    else:
        print(f"âŒ æ— æ³•è¯»å–å­¦ç”ŸID from {student_file}")
        sys.exit(1)
    
    print(f"å°†è¯„ä¼° {len(student_ids)} ä¸ªå­¦ç”Ÿ")
    
    # è¿è¡Œè¯„ä¼°
    results = run_batch_evaluation(student_ids, args.dataset, args.method, args.max_workers)
    
    print(f"\nâœ… {args.method}-conservative è¯„ä¼°å®Œæˆï¼")

