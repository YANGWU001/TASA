#!/usr/bin/env python3
"""
ç”ŸæˆåŸºäºå†å²accuracyçš„Forgetting Score
è¿™æ˜¯æœ€æ ‡å‡†çš„æ–¹æ³•ï¼Œæ‰€æœ‰å­—æ®µéƒ½åŸºäºç›¸åŒçš„åŸå§‹æ•°æ®
"""

import os
import json
import pandas as pd
import numpy as np
from collections import defaultdict
import argparse

def load_sequences(dataset_name):
    """åŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆtrain/valid/testï¼‰"""
    data_dir = f'/mnt/localssd/pykt-toolkit/data/{dataset_name}'
    
    all_data = []
    
    # åŠ è½½train+valid
    train_valid_file = os.path.join(data_dir, 'train_valid_sequences.csv')
    if os.path.exists(train_valid_file):
        df_tv = pd.read_csv(train_valid_file)
        df_tv['split'] = 'train_valid'
        all_data.append(df_tv)
        print(f"  âœ… Train+Valid: {len(df_tv)} sequences")
    
    # åŠ è½½test
    test_file = os.path.join(data_dir, 'test_sequences.csv')
    if os.path.exists(test_file):
        df_test = pd.read_csv(test_file)
        df_test['split'] = 'test'
        all_data.append(df_test)
        print(f"  âœ… Test: {len(df_test)} sequences")
    
    if not all_data:
        raise FileNotFoundError(f"No data files found for {dataset_name}")
    
    df_all = pd.concat(all_data, ignore_index=True)
    print(f"  âœ… Total: {len(df_all)} sequences, {df_all['uid'].nunique()} unique students")
    
    return df_all

def parse_sequence_data(df_all, dataset_name):
    """è§£æåºåˆ—æ•°æ®ï¼Œæå–æ¯ä¸ªå­¦ç”Ÿçš„æ¯ä¸ªconceptçš„äº¤äº’å†å²"""
    print(f"\nğŸ“Š è§£æåºåˆ—æ•°æ®...")
    
    # å­¦ç”Ÿ-conceptçº§åˆ«çš„ç»Ÿè®¡
    student_concept_data = defaultdict(lambda: {
        'interactions': [],  # [(timestamp, response)]
        'concept_id': None,
        'concept_text': None
    })
    
    # åŠ è½½conceptæ˜ å°„
    concept_map = {}
    concepts_file = f'/mnt/localssd/pykt-toolkit/data/{dataset_name}/concepts.txt'
    if os.path.exists(concepts_file):
        with open(concepts_file) as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    concept_map[int(parts[0])] = parts[1]
    
    total_interactions = 0
    
    for idx, row in df_all.iterrows():
        uid = str(row['uid'])
        
        # è§£æconcepts, responses, timestamps
        concepts = [int(c) for c in str(row['concepts']).split(',')]
        responses = [int(r) for r in str(row['responses']).split(',')]
        
        # è§£ætimestamps
        if 'timestamps' in row and pd.notna(row['timestamps']):
            timestamps = [int(t) for t in str(row['timestamps']).split(',')]
        else:
            # å¦‚æœæ²¡æœ‰timestampsï¼Œä½¿ç”¨åºåˆ—ç´¢å¼•
            timestamps = list(range(len(concepts)))
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(concepts), len(responses), len(timestamps))
        concepts = concepts[:min_len]
        responses = responses[:min_len]
        timestamps = timestamps[:min_len]
        
        # è®°å½•æ¯ä¸ªconceptçš„äº¤äº’
        for c, r, t in zip(concepts, responses, timestamps):
            if c == -1:  # è·³è¿‡æ— æ•ˆconcept
                continue
            
            key = (uid, c)
            student_concept_data[key]['interactions'].append((t, r))
            student_concept_data[key]['concept_id'] = c
            
            # è®¾ç½®concept_text
            if c in concept_map:
                student_concept_data[key]['concept_text'] = concept_map[c]
            else:
                student_concept_data[key]['concept_text'] = f'concept_{c}'
            
            total_interactions += 1
    
    print(f"  âœ… Parsed {total_interactions} interactions")
    print(f"  âœ… {len(student_concept_data)} student-concept pairs")
    
    return student_concept_data

def calculate_forgetting_scores(student_concept_data, tau_days):
    """è®¡ç®—åŸºäºå†å²accuracyçš„forgetting score"""
    print(f"\nğŸ“ˆ è®¡ç®—Forgetting Scores (Ï„={tau_days} days)...")
    
    tau_minutes = tau_days * 24 * 60
    results = []
    
    for (uid, concept_id), data in student_concept_data.items():
        interactions = sorted(data['interactions'])  # æŒ‰æ—¶é—´æ’åº
        
        if len(interactions) < 2:
            # è‡³å°‘éœ€è¦2æ¬¡äº¤äº’æ‰èƒ½è®¡ç®—forgetting score
            continue
        
        # è®¡ç®—å†å²accuracy (ä½¿ç”¨å€’æ•°ç¬¬äºŒæ¬¡ä¹‹å‰çš„æ‰€æœ‰äº¤äº’)
        historical_responses = [r for t, r in interactions[:-1]]
        s_tc = sum(historical_responses) / len(historical_responses)
        
        # æœ€åä¸€æ¬¡äº¤äº’ä¿¡æ¯
        last_time, last_response = interactions[-1]
        second_last_time, _ = interactions[-2]
        
        # è®¡ç®—æ—¶é—´å·®ï¼ˆè½¬æ¢ä¸ºåˆ†é’Ÿï¼‰
        if isinstance(last_time, (int, float)) and last_time > 1e9:  # æ¯«ç§’çº§æ—¶é—´æˆ³
            delta_t = (last_time - second_last_time) / 1000 / 60  # ms -> minutes
        else:  # åºåˆ—ç´¢å¼•æˆ–ç§’çº§
            delta_t = abs(last_time - second_last_time)
            if delta_t < 1000:  # å¯èƒ½æ˜¯åºåˆ—ç´¢å¼•
                delta_t = delta_t * 60  # å‡è®¾æ¯ä¸ªstepæ˜¯1åˆ†é’Ÿ
        
        # è®¡ç®—forgetting score
        time_factor = delta_t / (delta_t + tau_minutes)
        fs = (1 - s_tc) * time_factor
        
        results.append({
            'uid': uid,
            'concept_id': concept_id,
            'concept_text': data['concept_text'],
            's_tc': s_tc,
            'fs': fs,
            'delta_t': delta_t,
            'tau': tau_minutes,
            'last_response': last_response,
            'num_attempts': len(interactions)
        })
    
    print(f"  âœ… Calculated {len(results)} forgetting scores")
    
    return results

def assign_levels(results):
    """æ ¹æ®FSåˆ†å¸ƒåˆ†é…level"""
    if not results:
        return results
    
    fs_values = [r['fs'] for r in results]
    q33 = np.percentile(fs_values, 33)
    q67 = np.percentile(fs_values, 67)
    
    for r in results:
        if r['fs'] < q33:
            r['level'] = 'low'
        elif r['fs'] < q67:
            r['level'] = 'medium'
        else:
            r['level'] = 'high'
    
    return results

def save_to_bank(results, dataset_name):
    """ä¿å­˜åˆ°bankæ ¼å¼"""
    print(f"\nğŸ’¾ ä¿å­˜åˆ°Bank...")
    
    # è½¬æ¢ä¸ºbankæ ¼å¼
    bank_data = defaultdict(dict)
    
    for r in results:
        uid = r['uid']
        concept_text = r['concept_text']
        
        bank_data[uid][concept_text] = {
            's_tc': float(r['s_tc']),
            'fs': float(r['fs']),
            'delta_t': float(r['delta_t']),
            'tau': float(r['tau']),
            'level': r['level'],
            'last_response': int(r['last_response']),
            'num_attempts': int(r['num_attempts'])
        }
    
    # ä¿å­˜
    output_dir = f'/mnt/localssd/bank/forgetting/{dataset_name}'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'history.json')
    
    with open(output_file, 'w') as f:
        json.dump(bank_data, f, indent=2)
    
    print(f"  âœ… Saved: {output_file}")
    print(f"  ğŸ“Š Students: {len(bank_data)}")
    print(f"  ğŸ“Š Total entries: {sum(len(concepts) for concepts in bank_data.values())}")
    print(f"  ğŸ“Š Avg concepts/student: {sum(len(concepts) for concepts in bank_data.values()) / len(bank_data):.1f}")

def main():
    parser = argparse.ArgumentParser(description='Generate history-based forgetting scores')
    parser.add_argument('--dataset', type=str, required=True, 
                       help='Dataset name (assist2017, nips_task34, algebra2005, bridge2algebra2006)')
    parser.add_argument('--tau', type=float, default=None,
                       help='Tau value in days (if not specified, use dataset default)')
    
    args = parser.parse_args()
    
    # Taué»˜è®¤å€¼
    tau_values = {
        'assist2017': 2.93,
        'nips_task34': 2.93,
        'algebra2005': 0.70,
        'bridge2algebra2006': 0.70,
    }
    tau = args.tau if args.tau is not None else tau_values.get(args.dataset, 2.93)
    
    print("="*100)
    print(f"ğŸ“Š ç”ŸæˆåŸºäºå†å²accuracyçš„Forgetting Score")
    print(f"   Dataset: {args.dataset}")
    print(f"   Tau: {tau} days ({tau * 24 * 60:.1f} minutes)")
    print("="*100)
    
    # 1. åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®...")
    df_all = load_sequences(args.dataset)
    
    # 2. è§£æåºåˆ—æ•°æ®
    student_concept_data = parse_sequence_data(df_all, args.dataset)
    
    # 3. è®¡ç®—forgetting scores
    results = calculate_forgetting_scores(student_concept_data, tau)
    
    # 4. åˆ†é…levels
    results = assign_levels(results)
    
    # 5. ä¿å­˜
    save_to_bank(results, args.dataset)
    
    print("\n" + "="*100)
    print("âœ… å®Œæˆï¼")
    print("="*100)

if __name__ == '__main__':
    main()

