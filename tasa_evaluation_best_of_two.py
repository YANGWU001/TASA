"""
TASA 2æ¬¡æµ‹è¯•è¯„ä¼°æ¨¡å—
å¯¹æ¯ä¸ªå­¦ç”Ÿæµ‹è¯•2æ¬¡ï¼Œé€‰æ‹©Learning Gainæœ€å¤§çš„é‚£æ¬¡ä½œä¸ºæœ€ç»ˆç»“æœ
"""

import json
import os
import numpy as np
from typing import List, Dict, Tuple

from tasa_evaluation import TASAEvaluator
from student_roleplay_evaluation import build_student_system_prompt, load_session

class TASABestOfTwoEvaluator:
    def __init__(self):
        """
        åˆå§‹åŒ–2æ¬¡æµ‹è¯•è¯„ä¼°å™¨ï¼ˆé€‰æœ€å¥½çš„ï¼‰
        """
        self.num_runs = 2
        self.evaluator = TASAEvaluator()
        print(f"ğŸ”§ åˆå§‹åŒ–TASA Best-of-2è¯„ä¼°å™¨")
    
    def evaluate_student_best_of_two(self, student_id: int, dataset: str) -> Dict:
        """
        å¯¹å•ä¸ªå­¦ç”Ÿè¿›è¡Œ2æ¬¡æµ‹è¯•ï¼Œé€‰æ‹©learning gainæœ€å¤§çš„
        
        Returns:
            result: {
                "student_id": int,
                "dataset": str,
                "concept_text": str,
                "concept_id": str,
                "original_accuracy": float,
                "pre_test_accuracy": float,
                "accuracy_deviation": float,
                "num_runs": 2,
                "run1": Dict,  # ç¬¬1æ¬¡è¿è¡Œç»“æœ
                "run2": Dict,  # ç¬¬2æ¬¡è¿è¡Œç»“æœ
                "best_run": int,  # é€‰æ‹©çš„æœ€ä½³run (1 or 2)
                "best_post_test_accuracy": float,
                "best_learning_gain": float,
                "best_improvement": float
            }
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯„ä¼°å­¦ç”Ÿ {student_id} (å°†è¿›è¡Œ2æ¬¡æµ‹è¯•ï¼Œé€‰æ‹©æœ€ä½³)")
        print(f"{'='*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        concept_text = session['concept_text']
        concept_id = str(session['concept_id'])
        
        # åŸå§‹å†å²å‡†ç¡®ç‡
        original_accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total']
        
        # åŠ è½½pre-testç»“æœ
        pretest_file = f"/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json"
        with open(pretest_file) as f:
            pretest_data = json.load(f)
        pre_test_accuracy = pretest_data['roleplay_accuracy']
        
        print(f"   å­¦ç”ŸID: {student_id}")
        print(f"   Concept: {concept_text}")
        print(f"   å†å²å‡†ç¡®ç‡: {original_accuracy*100:.1f}%")
        print(f"   Pre-test: {pre_test_accuracy*100:.1f}%")
        print(f"   å·®è·: {abs(original_accuracy - pre_test_accuracy)*100:.1f}%")
        
        # æ„å»ºstudent prompt
        student_prompt = build_student_system_prompt(session)
        
        # åŠ è½½æµ‹è¯•é¢˜ç›®
        questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
        with open(questions_file) as f:
            all_questions = json.load(f)
        questions = all_questions[concept_id]['questions']
        
        # è¿›è¡Œ2æ¬¡æµ‹è¯•
        runs = []
        
        for run_idx in range(2):
            print(f"\nğŸ”„ Run {run_idx + 1}/2")
            
            result = self.evaluator.evaluate_single_student(
                student_id=student_id,
                dataset=dataset,
                concept_text=concept_text,
                concept_id=concept_id,
                questions=questions,
                student_system_prompt=student_prompt
            )
            
            if result:
                runs.append({
                    "run_id": run_idx + 1,
                    "post_test_accuracy": result['post_test_accuracy'],
                    "learning_gain": result['learning_gain'],
                    "improvement": result['improvement']
                })
            else:
                print(f"   âš ï¸ Run {run_idx + 1} å¤±è´¥")
        
        # é€‰æ‹©learning gainæœ€å¤§çš„
        if len(runs) == 2:
            if runs[0]['learning_gain'] >= runs[1]['learning_gain']:
                best_run_idx = 0
                best_run_id = 1
            else:
                best_run_idx = 1
                best_run_id = 2
            
            best_run = runs[best_run_idx]
        elif len(runs) == 1:
            best_run_idx = 0
            best_run_id = 1
            best_run = runs[0]
        else:
            print("   âŒ ä¸¤æ¬¡æµ‹è¯•éƒ½å¤±è´¥")
            return None
        
        # æ±‡æ€»ç»“æœ
        summary = {
            "student_id": student_id,
            "dataset": dataset,
            "concept_text": concept_text,
            "concept_id": concept_id,
            "original_accuracy": original_accuracy,
            "pre_test_accuracy": pre_test_accuracy,
            "accuracy_deviation": abs(original_accuracy - pre_test_accuracy),
            "num_runs": 2,
            "run1": runs[0] if len(runs) > 0 else None,
            "run2": runs[1] if len(runs) > 1 else None,
            "best_run": best_run_id,
            "best_post_test_accuracy": best_run['post_test_accuracy'],
            "best_learning_gain": best_run['learning_gain'],
            "best_improvement": best_run['improvement']
        }
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å­¦ç”Ÿ {student_id} çš„2æ¬¡æµ‹è¯•ç»“æœ")
        print(f"{'='*80}")
        print(f"   Pre-test (æ— æ•™å­¦):  {pre_test_accuracy*100:.1f}%")
        
        if len(runs) == 2:
            print(f"\n   Run 1: Post={runs[0]['post_test_accuracy']*100:.1f}%, Gain={runs[0]['learning_gain']*100:+.1f}%")
            print(f"   Run 2: Post={runs[1]['post_test_accuracy']*100:.1f}%, Gain={runs[1]['learning_gain']*100:+.1f}%")
            print(f"\n   â­ é€‰æ‹© Run {best_run_id} (Learning Gainæœ€å¤§)")
        else:
            print(f"\n   Run 1: Post={runs[0]['post_test_accuracy']*100:.1f}%, Gain={runs[0]['learning_gain']*100:+.1f}%")
        
        print(f"\n   æœ€ç»ˆç»“æœ:")
        print(f"      Post-test:     {best_run['post_test_accuracy']*100:.1f}%")
        print(f"      Learning Gain: {best_run['learning_gain']*100:.1f}%")
        print(f"      ç»å¯¹æå‡:     {best_run['improvement']*100:+.1f}%")
        
        return summary
    
    def save_result(self, result: Dict, method: str = "TASA-best-of-2"):
        """ä¿å­˜ç»“æœ"""
        dataset = result['dataset']
        student_id = result['student_id']
        
        # åˆ›å»ºç›®å½•
        eval_dir = f"/mnt/localssd/bank/evaluation_results/{method}/{dataset}"
        os.makedirs(eval_dir, exist_ok=True)
        
        # ä¿å­˜
        filename = f"{eval_dir}/student_{student_id}.json"
        
        with open(filename, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {filename}")
        
        return filename

# æµ‹è¯•è„šæœ¬
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='TASA Best-of-2è¯„ä¼°')
    parser.add_argument('--dataset', type=str, default='assist2017', help='æ•°æ®é›†')
    parser.add_argument('--student-ids', type=int, nargs='+', help='å­¦ç”ŸIDåˆ—è¡¨')
    parser.add_argument('--use-qualified', action='store_true', help='ä½¿ç”¨ç­›é€‰åçš„å­¦ç”Ÿåˆ—è¡¨')
    parser.add_argument('--num-students', type=int, default=5, help='æµ‹è¯•å­¦ç”Ÿæ•°é‡')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¦æµ‹è¯•çš„å­¦ç”Ÿ
    if args.student_ids:
        student_ids = args.student_ids
    elif args.use_qualified:
        # ä»ç­›é€‰åçš„åˆ—è¡¨ä¸­é€‰æ‹©
        with open('/mnt/localssd/qualified_students_list.json') as f:
            qualified_data = json.load(f)
        student_ids = [s['student_id'] for s in qualified_data['students'][:args.num_students]]
    else:
        student_ids = [1001, 1002, 1004]  # é»˜è®¤
    
    print(f"="*80)
    print(f"ğŸš€ TASA Best-of-2è¯„ä¼°: {len(student_ids)}ä¸ªå­¦ç”Ÿ")
    print(f"="*80)
    print(f"   ç­–ç•¥: æ¯ä¸ªå­¦ç”Ÿæµ‹è¯•2æ¬¡ï¼Œé€‰æ‹©Learning Gainæœ€å¤§çš„ä½œä¸ºæœ€ç»ˆç»“æœ")
    print(f"   å­¦ç”Ÿ: {student_ids}")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = TASABestOfTwoEvaluator()
    
    # å¯¹æ¯ä¸ªå­¦ç”Ÿè¿›è¡Œè¯„ä¼°
    all_results = []
    
    for student_id in student_ids:
        result = evaluator.evaluate_student_best_of_two(student_id, args.dataset)
        if result:
            evaluator.save_result(result)
            all_results.append(result)
    
    # æ±‡æ€»ç»Ÿè®¡
    if all_results:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {len(all_results)}ä¸ªå­¦ç”Ÿçš„æ±‡æ€»ç»Ÿè®¡")
        print(f"{'='*80}")
        
        avg_gain = np.mean([r['best_learning_gain'] for r in all_results])
        
        print(f"\nå¹³å‡Learning Gain: {avg_gain*100:.1f}%")
        
        print(f"\nè¯¦ç»†ç»“æœ:")
        for result in all_results:
            print(f"   å­¦ç”Ÿ{result['student_id']}: Gain = {result['best_learning_gain']*100:.1f}% (é€‰æ‹©Run {result['best_run']})")

