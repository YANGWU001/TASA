#!/usr/bin/env python
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„KTæ¨¡å‹æ‰¹é‡é¢„æµ‹å¹¶è®¡ç®—Forgetting Score

æ€è·¯ï¼š
1. ä½¿ç”¨PyKTçš„DataLoaderåŠ è½½test setï¼ˆå·²ç»å¤„ç†å¥½IDæ˜ å°„ï¼‰
2. æ¨¡å‹å¯¹æ•´ä¸ªtest setè¿›è¡Œé¢„æµ‹
3. æå–æ¯ä¸ªå­¦ç”Ÿæ¯ä¸ªconceptçš„é¢„æµ‹æ¦‚ç‡
4. è®¡ç®—forgetting scores
"""

import os
import sys
import json
import torch
import pandas as pd
import numpy as np
from collections import defaultdict
from datetime import datetime

# æ·»åŠ PyKTè·¯å¾„
sys.path.insert(0, '/mnt/localssd/pykt-toolkit')
from pykt.models.init_model import load_model
from pykt.datasets.lpkt_dataloader import KTQueDataset
from torch.utils.data import DataLoader

def load_trained_model(dataset, model_name='lpkt', device='cpu'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    base_dir = '/mnt/localssd/pykt-toolkit/examples/saved_model'
    
    # æŸ¥æ‰¾æ¨¡å‹ç›®å½•
    for dirname in os.listdir(base_dir):
        if dirname.startswith(f"{dataset}_{model_name}_"):
            model_dir = os.path.join(base_dir, dirname)
            config_path = os.path.join(model_dir, 'config.json')
            
            if not os.path.exists(config_path):
                continue
            
            # è¯»å–é…ç½®
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            # ä½¿ç”¨PyKTçš„æ ‡å‡†åŠ è½½æ–¹å¼
            model = load_model(
                model_name=model_name,
                model_config=config['model_config'],
                data_config=config['data_config'],
                emb_type=config.get('params', {}).get('emb_type', 'qid'),
                ckpt_path=model_dir
            )
            
            model = model.to(device)
            model.eval()
            
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_name.upper()} on {dataset.upper()}")
            print(f"   num_q={config['data_config']['num_q']}, num_c={config['data_config']['num_c']}")
            
            return model, config
    
    print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹: {dataset}_{model_name}")
    return None, None

def create_dataloader(dataset, config, batch_size=64):
    """åˆ›å»ºDataLoaderï¼ˆä½¿ç”¨PyKTçš„æ ‡å‡†æ–¹å¼ï¼‰"""
    data_config = config['data_config']
    
    # ä½¿ç”¨test set
    test_file = os.path.join(data_config['dpath'], 'test_sequences.csv')
    
    if not os.path.exists(test_file):
        print(f"âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return None
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {test_file}")
    
    # ä½¿ç”¨PyKTçš„Dataset
    test_dataset = KTQueDataset(
        test_file,
        input_type=data_config.get('input_type', ['questions']),
        folds=[-1],  # test set
        qtest=False,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0
    )
    
    print(f"âœ… DataLoaderåˆ›å»ºæˆåŠŸï¼Œbatchæ•°: {len(test_loader)}")
    
    return test_loader

def predict_all_students(model, model_name, test_loader, device='cpu'):
    """
    å¯¹æ‰€æœ‰å­¦ç”Ÿè¿›è¡Œé¢„æµ‹
    
    è¿”å›ï¼šæ¯ä¸ªå­¦ç”Ÿæ¯ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹æ¦‚ç‡
    """
    print(f"\nğŸ”® å¼€å§‹é¢„æµ‹...")
    
    all_predictions = []
    
    model.eval()
    with torch.no_grad():
        for batch_idx, dcur in enumerate(test_loader):
            if batch_idx % 10 == 0:
                print(f"   å¤„ç†æ‰¹æ¬¡ {batch_idx+1}/{len(test_loader)}...")
            
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            for key in dcur:
                if isinstance(dcur[key], torch.Tensor):
                    dcur[key] = dcur[key].to(device)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨forward
            try:
                if model_name == 'lpkt':
                    # LPKTçš„è¾“å…¥æ ¼å¼
                    q = dcur["qseqs"]
                    c = dcur["cseqs"]
                    r = dcur["rseqs"]
                    qshft = dcur["shft_qseqs"]
                    cshft = dcur["shft_cseqs"]
                    rshft = dcur["shft_rseqs"]
                    
                    cq = torch.cat([q[:, 0:1], qshft], dim=1)
                    cr = torch.cat([r[:, 0:1], rshft], dim=1)
                    cit = torch.cat([dcur["itseqs"][:, 0:1], dcur["shft_itseqs"]], dim=1)
                    
                    y = model(cq.long(), cr.long(), cit.long())
                    
                elif model_name == 'dkt':
                    # DKTçš„è¾“å…¥æ ¼å¼
                    q = dcur["qseqs"]
                    c = dcur["cseqs"]
                    r = dcur["rseqs"]
                    qshft = dcur["shft_qseqs"]
                    cshft = dcur["shft_cseqs"]
                    rshft = dcur["shft_rseqs"]
                    
                    cq = torch.cat([q[:, 0:1], qshft], dim=1)
                    cc = torch.cat([c[:, 0:1], cshft], dim=1)
                    cr = torch.cat([r[:, 0:1], rshft], dim=1)
                    
                    y = model(cc.long(), cr.long(), cq.long())
                    
                elif model_name == 'akt':
                    # AKTçš„è¾“å…¥æ ¼å¼
                    q = dcur["qseqs"]
                    c = dcur["cseqs"]
                    r = dcur["rseqs"]
                    qshft = dcur["shft_qseqs"]
                    cshft = dcur["shft_cseqs"]
                    rshft = dcur["shft_rseqs"]
                    
                    cq = torch.cat([q[:, 0:1], qshft], dim=1)
                    cc = torch.cat([c[:, 0:1], cshft], dim=1)
                    cr = torch.cat([r[:, 0:1], rshft], dim=1)
                    
                    y, _ = model(cc.long(), cr.long(), cq.long())
                    
                elif model_name == 'simplekt':
                    # simpleKTçš„è¾“å…¥æ ¼å¼
                    y = model(dcur)
                    
                else:
                    print(f"âš ï¸  ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
                    continue
                
                # è·³è¿‡ç¬¬ä¸€ä¸ªé¢„æµ‹ï¼ˆshiftçš„ç»“æœï¼‰
                y = y[:, 1:]
                
                # è½¬æ¢ä¸ºæ¦‚ç‡
                probs = torch.sigmoid(y)
                
                # ä¿å­˜ç»“æœï¼ˆç§»åˆ°CPUï¼‰
                batch_result = {
                    'probs': probs.cpu(),
                    'concepts': dcur["cseqs"].cpu(),
                    'responses': dcur["rseqs"].cpu(),
                    'questions': dcur["qseqs"].cpu(),
                    'masks': dcur["masks"].cpu(),
                    'uids': dcur.get("uid", None),
                }
                
                all_predictions.append(batch_result)
                
            except Exception as e:
                print(f"   âš ï¸  æ‰¹æ¬¡ {batch_idx} é¢„æµ‹å¤±è´¥: {e}")
                continue
    
    print(f"âœ… é¢„æµ‹å®Œæˆï¼")
    
    return all_predictions

def extract_concept_predictions(all_predictions):
    """
    ä»æ‰¹é‡é¢„æµ‹ç»“æœä¸­æå–æ¯ä¸ªå­¦ç”Ÿæ¯ä¸ªconceptçš„é¢„æµ‹
    
    è¿”å›ï¼š{student_id: {concept_id: [predictions]}}
    """
    print(f"\nğŸ“Š æå–conceptçº§åˆ«çš„é¢„æµ‹...")
    
    student_concept_preds = defaultdict(lambda: defaultdict(list))
    
    total_samples = 0
    for batch_result in all_predictions:
        probs = batch_result['probs']
        concepts = batch_result['concepts']
        responses = batch_result['responses']
        masks = batch_result['masks']
        
        batch_size, seq_len = probs.shape
        
        for i in range(batch_size):
            student_id = total_samples + i
            
            for j in range(seq_len):
                if masks[i, j] == 0:  # padding
                    continue
                
                concept = int(concepts[i, j])
                pred_prob = float(probs[i, j])
                response = int(responses[i, j])
                
                student_concept_preds[student_id][concept].append({
                    'pred_prob': pred_prob,
                    'response': response,
                    'position': j,
                })
        
        total_samples += batch_size
    
    print(f"âœ… æå–å®Œæˆ: {len(student_concept_preds)} ä¸ªå­¦ç”Ÿ")
    
    return student_concept_preds

def calculate_forgetting_scores(student_concept_preds, tau_days=3.0):
    """
    è®¡ç®—æ¯ä¸ªå­¦ç”Ÿæ¯ä¸ªconceptçš„forgetting score
    
    ä½¿ç”¨æœ€åä¸€æ¬¡çš„é¢„æµ‹æ¦‚ç‡ä½œä¸ºs_t,c
    """
    print(f"\nğŸ“ˆ è®¡ç®—Forgetting Scores (Ï„={tau_days} å¤©)...")
    
    tau_minutes = tau_days * 24 * 60
    
    results = []
    
    for student_id, concepts in student_concept_preds.items():
        for concept_id, predictions in concepts.items():
            if len(predictions) < 2:
                continue
            
            # ä½¿ç”¨å€’æ•°ç¬¬äºŒæ¬¡çš„é¢„æµ‹ä½œä¸ºs_t,cï¼ˆé¢„æµ‹æœ€åä¸€æ¬¡çš„è¡¨ç°ï¼‰
            s_tc = predictions[-2]['pred_prob']
            
            # ä½¿ç”¨ä½ç½®å·®ä½œä¸ºæ—¶é—´å·®çš„ä»£ç†
            # å®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸå®æ—¶é—´æˆ³
            delta_steps = predictions[-1]['position'] - predictions[-2]['position']
            
            # å‡è®¾æ¯æ­¥å¹³å‡é—´éš”1å¤©ï¼ˆå¯ä»¥æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼‰
            delta_t = delta_steps * 24 * 60  # è½¬æ¢ä¸ºåˆ†é’Ÿ
            
            # è®¡ç®—FS
            time_factor = delta_t / (delta_t + tau_minutes)
            fs = (1 - s_tc) * time_factor
            
            # è®°å½•ç»“æœ
            results.append({
                'student_id': student_id,
                'concept_id': concept_id,
                'num_attempts': len(predictions),
                's_tc_model': s_tc,
                's_tc_historical': np.mean([p['response'] for p in predictions[:-1]]),
                'delta_steps': delta_steps,
                'fs_model': fs,
                'last_response': predictions[-1]['response'],
                'predicted_correct': 1 if s_tc >= 0.5 else 0,
            })
    
    print(f"âœ… è®¡ç®—å®Œæˆ: {len(results)} ä¸ª (å­¦ç”Ÿ, concept) å¯¹")
    
    return pd.DataFrame(results)

def analyze_results(df):
    """åˆ†æç»“æœ"""
    print(f"\n{'='*100}")
    print(f"ğŸ“Š ç»“æœåˆ†æ")
    print(f"{'='*100}\n")
    
    print(f"åŸºæœ¬ç»Ÿè®¡:")
    print(f"  å­¦ç”Ÿæ•°: {df['student_id'].nunique()}")
    print(f"  Conceptæ•°: {df['concept_id'].nunique()}")
    print(f"  æ€»è®°å½•æ•°: {len(df)}")
    
    print(f"\næ¨¡å‹é¢„æµ‹ vs å†å²å‡†ç¡®ç‡:")
    print(f"  æ¨¡å‹s_tcå¹³å‡: {df['s_tc_model'].mean():.4f}")
    print(f"  å†å²s_tcå¹³å‡: {df['s_tc_historical'].mean():.4f}")
    print(f"  ç›¸å…³ç³»æ•°: {df['s_tc_model'].corr(df['s_tc_historical']):.4f}")
    
    print(f"\nForgetting Scoreåˆ†å¸ƒ:")
    print(df['fs_model'].describe())
    
    # æŒ‰FSåˆ†ç»„åˆ†æé¢„æµ‹å‡†ç¡®æ€§
    df['fs_group'] = pd.cut(df['fs_model'], bins=[0, 0.1, 0.3, 0.5, 1.0], 
                             labels=['Low', 'Medium', 'High', 'Very High'])
    
    print(f"\næŒ‰Forgetting Scoreåˆ†ç»„çš„ç­”é”™ç‡:")
    for group in ['Low', 'Medium', 'High', 'Very High']:
        group_df = df[df['fs_group'] == group]
        if len(group_df) > 0:
            error_rate = 1 - group_df['last_response'].mean()
            print(f"  {group}: {error_rate:.1%} ({len(group_df)} samples)")
    
    # æ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§
    print(f"\næ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§:")
    accuracy = (df['predicted_correct'] == df['last_response']).mean()
    print(f"  å‡†ç¡®ç‡: {accuracy:.1%}")
    
    # å¯¹æ¯”é«˜FS vs ä½FS
    high_fs = df[df['fs_model'] >= 0.3]
    low_fs = df[df['fs_model'] < 0.1]
    
    if len(high_fs) > 0 and len(low_fs) > 0:
        print(f"\né«˜FS (â‰¥0.3) vs ä½FS (<0.1):")
        print(f"  é«˜FSç­”é”™ç‡: {(1 - high_fs['last_response'].mean()):.1%}")
        print(f"  ä½FSç­”é”™ç‡: {(1 - low_fs['last_response'].mean()):.1%}")
        print(f"  å·®å¼‚: {(1 - high_fs['last_response'].mean()) - (1 - low_fs['last_response'].mean()):.1%}")

def main():
    print("="*100)
    print("ğŸš€ ä½¿ç”¨KTæ¨¡å‹æ‰¹é‡é¢„æµ‹å¹¶è®¡ç®—Forgetting Score")
    print("="*100)
    
    # é…ç½®
    dataset = 'assist2017'
    model_name = 'lpkt'
    device = 'cpu'
    tau_days = 3.21  # ä»ä¹‹å‰çš„åˆ†æå¾—å‡º
    
    print(f"\né…ç½®:")
    print(f"  æ•°æ®é›†: {dataset.upper()}")
    print(f"  æ¨¡å‹: {model_name.upper()}")
    print(f"  Ï„: {tau_days} å¤©")
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"\n{'='*100}")
    print(f"ç¬¬1æ­¥: åŠ è½½æ¨¡å‹")
    print(f"{'='*100}")
    
    model, config = load_trained_model(dataset, model_name, device)
    
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        return
    
    # 2. åˆ›å»ºDataLoader
    print(f"\n{'='*100}")
    print(f"ç¬¬2æ­¥: åˆ›å»ºDataLoader")
    print(f"{'='*100}")
    
    test_loader = create_dataloader(dataset, config, batch_size=64)
    
    if test_loader is None:
        print("âŒ DataLoaderåˆ›å»ºå¤±è´¥ï¼Œé€€å‡º")
        return
    
    # 3. æ‰¹é‡é¢„æµ‹
    print(f"\n{'='*100}")
    print(f"ç¬¬3æ­¥: æ‰¹é‡é¢„æµ‹")
    print(f"{'='*100}")
    
    all_predictions = predict_all_students(model, model_name, test_loader, device)
    
    # 4. æå–conceptçº§åˆ«çš„é¢„æµ‹
    print(f"\n{'='*100}")
    print(f"ç¬¬4æ­¥: æå–concepté¢„æµ‹")
    print(f"{'='*100}")
    
    student_concept_preds = extract_concept_predictions(all_predictions)
    
    # 5. è®¡ç®—Forgetting Scores
    print(f"\n{'='*100}")
    print(f"ç¬¬5æ­¥: è®¡ç®—Forgetting Scores")
    print(f"{'='*100}")
    
    results_df = calculate_forgetting_scores(student_concept_preds, tau_days)
    
    # 6. åˆ†æç»“æœ
    analyze_results(results_df)
    
    # 7. ä¿å­˜ç»“æœ
    output_file = f'/mnt/localssd/fs_results_{dataset}_{model_name}.csv'
    results_df.to_csv(output_file, index=False)
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")
    
    print(f"\n{'='*100}")
    print(f"âœ… å®Œæˆï¼")
    print(f"{'='*100}")

if __name__ == '__main__':
    main()

