#!/usr/bin/env python3
"""
å¤šçº¿ç¨‹æ‰¹é‡æµ‹è¯•å­¦ç”ŸRole-Playç³»ç»Ÿ
åœ¨5ä¸ªä¸åŒå‡†ç¡®ç‡çš„å­¦ç”Ÿèº«ä¸Šæµ‹è¯•çµæ´»promptçš„æ•ˆæœ
"""

import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, List
import time

from student_roleplay_evaluation import (
    load_session,
    load_concept_questions,
    build_student_system_prompt,
    get_student_answers,
    grade_answers
)

def evaluate_single_student(student_id: int, dataset: str = "assist2017", method: str = "pre-test") -> Dict:
    """è¯„ä¼°å•ä¸ªå­¦ç”Ÿ"""
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ¯ å¼€å§‹è¯„ä¼°å­¦ç”Ÿ: {student_id} (Dataset: {dataset})")
        print(f"{'='*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        concept_text = session['concept_text']
        concept_id = str(session['concept_id'])
        accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total'] * 100
        
        print(f"ğŸ“š Concept: {concept_text} (ID: {concept_id})")
        print(f"ğŸ“Š Historical Accuracy: {accuracy:.1f}%")
        
        # åŠ è½½é¢˜ç›®
        questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
        questions = load_concept_questions(questions_file)
        
        # å¯¹äºassist2017ï¼Œä½¿ç”¨concept_idæŸ¥æ‰¾é¢˜ç›®
        concept_data = questions.get(concept_id, {})
        if not concept_data or 'questions' not in concept_data:
            print(f"âŒ æœªæ‰¾åˆ°concept ID '{concept_id}' ({concept_text}) çš„é¢˜ç›®")
            return None
        
        concept_questions = concept_data['questions']
        print(f"âœ… æ‰¾åˆ° {len(concept_questions)} é“é¢˜ç›®")
        
        # æ„å»ºprompt
        system_prompt = build_student_system_prompt(session)
        
        # å­¦ç”Ÿå›ç­”
        print(f"ğŸ­ Student role-playing...")
        start_time = time.time()
        answers = get_student_answers(system_prompt, concept_questions, concept_text)
        answer_time = time.time() - start_time
        
        # æ‰¹æ”¹
        print(f"ğŸ“ Grading...")
        total_score, feedback, individual_scores = grade_answers(answers, concept_text)
        total_time = time.time() - start_time
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆæ³¨æ„ï¼šåªæœ‰2é“é¢˜ï¼Œæ‰€ä»¥æ»¡åˆ†æ˜¯2è€Œä¸æ˜¯10ï¼‰
        max_score = len(answers)
        roleplay_accuracy = (total_score / max_score) * 100
        
        print(f"\nâœ… è¯„ä¼°å®Œæˆ!")
        print(f"   åŸå§‹å‡†ç¡®ç‡: {accuracy:.1f}%")
        print(f"   Role-playå‡†ç¡®ç‡: {roleplay_accuracy:.1f}%")
        print(f"   åå·®: {roleplay_accuracy - accuracy:+.1f}%")
        print(f"   ç”¨æ—¶: {total_time:.1f}s")
        
        # ä¿å­˜ç»“æœ
        result = {
            "student_id": str(student_id),
            "dataset": dataset,
            "method": method,
            "concept_text": concept_text,
            "concept_id": concept_id,
            "original_accuracy": accuracy / 100,
            "roleplay_accuracy": roleplay_accuracy / 100,
            "deviation": (roleplay_accuracy - accuracy) / 100,
            "roleplay_score": total_score,
            "max_score": max_score,
            "individual_scores": individual_scores,
            "feedback": feedback,
            "answers": answers,
            "session_info": {
                "delta_t_minutes": session.get('delta_t_minutes', 0),
                "num_attempts": session['persona']['stats']['total'],
                "last_response": session.get('memory', [{}])[-1].get('response') if session.get('memory') else None
            },
            "timing": {
                "answer_time": answer_time,
                "total_time": total_time
            }
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆæ–°çš„ç›®å½•ç»“æ„ï¼šmethod/dataset/ï¼‰
        output_dir = f'/mnt/localssd/bank/evaluation_results/{method}/{dataset}'
        os.makedirs(output_dir, exist_ok=True)
        output_file = f'{output_dir}/student_{student_id}_concept_{session["concept_id"]}.json'
        
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        
        return result
        
    except Exception as e:
        print(f"âŒ å­¦ç”Ÿ {student_id} è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def batch_evaluate_students(student_ids: List[int], dataset: str = "assist2017", method: str = "pre-test", max_workers: int = 5):
    """æ‰¹é‡è¯„ä¼°å¤šä¸ªå­¦ç”Ÿï¼ˆå¤šçº¿ç¨‹ï¼‰"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ æ‰¹é‡è¯„ä¼°ç³»ç»Ÿ - æµ‹è¯•çµæ´»Prompt")
    print(f"{'='*80}")
    print(f"Method: {method}")
    print(f"Dataset: {dataset}")
    print(f"Students: {student_ids}")
    print(f"Max Workers: {max_workers}")
    print(f"{'='*80}\n")
    
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¯„ä¼°
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_student = {
            executor.submit(evaluate_single_student, student_id, dataset, method): student_id 
            for student_id in student_ids
        }
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_student):
            student_id = future_to_student[future]
            try:
                result = future.result()
                if result:
                    results.append(result)
            except Exception as e:
                print(f"âŒ å­¦ç”Ÿ {student_id} æ‰§è¡Œå¤±è´¥: {e}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\n\n{'='*80}")
    print(f"ğŸ“Š æ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*80}\n")
    
    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„è¯„ä¼°ç»“æœ")
        return
    
    print(f"æˆåŠŸè¯„ä¼°: {len(results)}/{len(student_ids)} ä¸ªå­¦ç”Ÿ\n")
    
    print(f"{'å­¦ç”ŸID':<10} {'åŸå§‹å‡†ç¡®ç‡':<12} {'Role-Play':<12} {'åå·®':<10} {'å¾—åˆ†':<10} {'Concept'}")
    print(f"{'-'*80}")
    
    total_deviation = 0
    for r in sorted(results, key=lambda x: x['original_accuracy']):
        original = r['original_accuracy'] * 100
        roleplay = r['roleplay_accuracy'] * 100
        deviation = r['deviation'] * 100
        score = r['roleplay_score']
        max_score = r['max_score']
        concept = r['concept_text'][:20]
        
        total_deviation += abs(deviation)
        
        print(f"{r['student_id']:<10} {original:>6.1f}%{'':<5} {roleplay:>6.1f}%{'':<5} {deviation:>+6.1f}%{'':<3} {score}/{max_score}{'':<7} {concept}")
    
    avg_deviation = total_deviation / len(results)
    print(f"\nå¹³å‡ç»å¯¹åå·®: {avg_deviation:.1f}%")
    
    # å…ˆå®šä¹‰æ°´å¹³åˆ†ç»„ï¼ˆç”¨äºåç»­ç»Ÿè®¡ï¼‰
    struggling = [r for r in results if r['original_accuracy'] < 0.4]
    developing = [r for r in results if 0.4 <= r['original_accuracy'] < 0.6]
    competent = [r for r in results if 0.6 <= r['original_accuracy'] < 0.8]
    strong = [r for r in results if r['original_accuracy'] >= 0.8]
    
    # è®¡ç®—overallç»Ÿè®¡
    avg_original = sum(r['original_accuracy'] for r in results) / len(results)
    avg_roleplay = sum(r['roleplay_accuracy'] for r in results) / len(results)
    
    # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆbatch_summary.json - æœ¬æ¬¡æ‰¹é‡æµ‹è¯•çš„è¯¦ç»†ä¿¡æ¯ï¼‰
    summary = {
        "method": method,
        "dataset": dataset,
        "num_students": len(results),
        "average_deviation": avg_deviation,
        "average_original_accuracy": avg_original,
        "average_roleplay_accuracy": avg_roleplay,
        "results": [
            {
                "student_id": r['student_id'],
                "concept": r['concept_text'],
                "original_accuracy": r['original_accuracy'],
                "roleplay_accuracy": r['roleplay_accuracy'],
                "deviation": r['deviation']
            }
            for r in results
        ]
    }
    
    summary_file = f'/mnt/localssd/bank/evaluation_results/{method}/{dataset}/batch_summary.json'
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_file}")
    
    # ç”Ÿæˆoverall.jsonï¼ˆè¯¥methodä¸‹è¯¥datasetçš„æ•´ä½“ç»Ÿè®¡ï¼‰
    overall = {
        "method": method,
        "dataset": dataset,
        "num_students_evaluated": len(results),
        "average_original_accuracy": avg_original,
        "average_roleplay_accuracy": avg_roleplay,
        "average_absolute_deviation": avg_deviation / 100,
        "performance_by_level": {
            "struggling": {
                "range": "<40%",
                "num_students": len(struggling),
                "avg_deviation": sum(abs(r['deviation']) for r in struggling) / len(struggling) if struggling else 0
            },
            "developing": {
                "range": "40-60%",
                "num_students": len(developing),
                "avg_deviation": sum(abs(r['deviation']) for r in developing) / len(developing) if developing else 0
            },
            "competent": {
                "range": "60-80%",
                "num_students": len(competent),
                "avg_deviation": sum(abs(r['deviation']) for r in competent) / len(competent) if competent else 0
            },
            "strong": {
                "range": "â‰¥80%",
                "num_students": len(strong),
                "avg_deviation": sum(abs(r['deviation']) for r in strong) / len(strong) if strong else 0
            }
        }
    }
    
    overall_file = f'/mnt/localssd/bank/evaluation_results/{method}/{dataset}/overall.json'
    with open(overall_file, 'w') as f:
        json.dump(overall, f, indent=2)
    
    print(f"ğŸ’¾ Overallç»Ÿè®¡å·²ä¿å­˜è‡³: {overall_file}")
    
    # åˆ†æä¸åŒæ°´å¹³å­¦ç”Ÿçš„è¡¨ç°
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ æŒ‰æ°´å¹³åˆ†æ")
    print(f"{'='*80}\n")
    
    for level_name, level_results in [
        ("STRUGGLING (<40%)", struggling),
        ("DEVELOPING (40-60%)", developing),
        ("COMPETENT (60-80%)", competent),
        ("STRONG (â‰¥80%)", strong)
    ]:
        if level_results:
            avg_dev = sum(abs(r['deviation']) for r in level_results) / len(level_results) * 100
            print(f"{level_name}: {len(level_results)}ä¸ªå­¦ç”Ÿ, å¹³å‡åå·® {avg_dev:.1f}%")

if __name__ == "__main__":
    # æµ‹è¯•5ä¸ªä¸åŒå‡†ç¡®ç‡çš„å­¦ç”Ÿï¼Œè¦†ç›–ä»0%åˆ°85.7%
    student_ids = [
        1264,  # 0.0% - STRUGGLING
        793,   # 35.3% - DEVELOPING  
        565,   # 55.6% - COMPETENT
        398,   # 70.0% - STRONG
        1355   # 85.7% - EXPERT
    ]
    
    print(f"\næµ‹è¯•å­¦ç”Ÿåˆ†å¸ƒ:")
    print(f"  1264: 0.0% (STRUGGLING)")
    print(f"  793: 35.3% (DEVELOPING)")
    print(f"  565: 55.6% (COMPETENT)")
    print(f"  398: 70.0% (STRONG)")
    print(f"  1355: 85.7% (EXPERT)")
    print()
    
    batch_evaluate_students(
        student_ids=student_ids,
        dataset="assist2017",
        method="pre-test",
        max_workers=5
    )

