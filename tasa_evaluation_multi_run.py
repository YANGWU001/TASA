"""
TASAå¤šæ¬¡æµ‹è¯•è¯„ä¼°æ¨¡å—
æ”¯æŒæ¯ä¸ªå­¦ç”Ÿæµ‹è¯•å¤šæ¬¡ï¼Œè®¡ç®—å¹³å‡Learning Gainå’Œæ ‡å‡†å·®
"""

import json
import os
import numpy as np
from typing import List, Dict, Tuple
from tqdm import tqdm

from tasa_evaluation import TASAEvaluator
from student_roleplay_evaluation import build_student_system_prompt, load_session

class TASAMultiRunEvaluator:
    def __init__(self, num_runs: int = 3):
        """
        åˆå§‹åŒ–å¤šæ¬¡æµ‹è¯•è¯„ä¼°å™¨
        
        Args:
            num_runs: æ¯ä¸ªå­¦ç”Ÿæµ‹è¯•çš„æ¬¡æ•°
        """
        self.num_runs = num_runs
        self.evaluator = TASAEvaluator()
        print(f"ğŸ”§ åˆå§‹åŒ–TASAå¤šæ¬¡æµ‹è¯•è¯„ä¼°å™¨ (æ¯ä¸ªå­¦ç”Ÿæµ‹è¯•{num_runs}æ¬¡)")
    
    def evaluate_student_multi_runs(self, student_id: int, dataset: str) -> Dict:
        """
        å¯¹å•ä¸ªå­¦ç”Ÿè¿›è¡Œå¤šæ¬¡æµ‹è¯•
        
        Returns:
            result: {
                "student_id": int,
                "dataset": str,
                "concept_text": str,
                "concept_id": str,
                "original_accuracy": float,  # å†å²å‡†ç¡®ç‡
                "pre_test_accuracy": float,  # Pre-test roleplayå‡†ç¡®ç‡
                "num_runs": int,
                "runs": List[Dict],  # æ¯æ¬¡è¿è¡Œçš„è¯¦ç»†ç»“æœ
                "avg_post_test_accuracy": float,  # å¹³å‡post-testå‡†ç¡®ç‡
                "std_post_test_accuracy": float,  # post-testå‡†ç¡®ç‡æ ‡å‡†å·®
                "avg_learning_gain": float,  # å¹³å‡learning gain
                "std_learning_gain": float,  # learning gainæ ‡å‡†å·®
                "avg_improvement": float,  # å¹³å‡ç»å¯¹æå‡
                "std_improvement": float   # ç»å¯¹æå‡æ ‡å‡†å·®
            }
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š è¯„ä¼°å­¦ç”Ÿ {student_id} (å°†è¿›è¡Œ{self.num_runs}æ¬¡æµ‹è¯•)")
        print(f"{'='*80}")
        
        # åŠ è½½session
        session_file = f'/mnt/localssd/bank/session/{dataset}/{student_id}.json'
        session = load_session(session_file)
        
        concept_text = session['concept_text']
        concept_id = str(session['concept_id'])
        
        # åŸå§‹å†å²å‡†ç¡®ç‡
        original_accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total']
        
        # åŠ è½½pre-testç»“æœ
        pretest_file = f"/mnt/localssd/bank/evaluation_results/pre-test/{dataset}/student_{student_id}_concept_{concept_id}.json"
        with open(pretest_file) as f:
            pretest_data = json.load(f)
        pre_test_accuracy = pretest_data['roleplay_accuracy']
        
        print(f"   å­¦ç”ŸID: {student_id}")
        print(f"   Concept: {concept_text}")
        print(f"   å†å²å‡†ç¡®ç‡: {original_accuracy*100:.1f}%")
        print(f"   Pre-testå‡†ç¡®ç‡: {pre_test_accuracy*100:.1f}%")
        print(f"   å·®è·: {abs(original_accuracy - pre_test_accuracy)*100:.1f}%")
        
        # æ„å»ºstudent prompt
        student_prompt = build_student_system_prompt(session)
        
        # åŠ è½½æµ‹è¯•é¢˜ç›®
        questions_file = f'/mnt/localssd/bank/test_data/{dataset}/concept_questions.json'
        with open(questions_file) as f:
            all_questions = json.load(f)
        questions = all_questions[concept_id]['questions']
        
        # è¿›è¡Œå¤šæ¬¡æµ‹è¯•
        runs = []
        post_test_accuracies = []
        learning_gains = []
        improvements = []
        
        for run_idx in range(self.num_runs):
            print(f"\nğŸ”„ Run {run_idx + 1}/{self.num_runs}")
            
            result = self.evaluator.evaluate_single_student(
                student_id=student_id,
                dataset=dataset,
                concept_text=concept_text,
                concept_id=concept_id,
                questions=questions,
                student_system_prompt=student_prompt
            )
            
            if result:
                runs.append({
                    "run_id": run_idx + 1,
                    "post_test_accuracy": result['post_test_accuracy'],
                    "learning_gain": result['learning_gain'],
                    "improvement": result['improvement']
                })
                
                post_test_accuracies.append(result['post_test_accuracy'])
                learning_gains.append(result['learning_gain'])
                improvements.append(result['improvement'])
            else:
                print(f"   âš ï¸ Run {run_idx + 1} å¤±è´¥")
        
        # è®¡ç®—ç»Ÿè®¡é‡
        avg_post_test = np.mean(post_test_accuracies)
        std_post_test = np.std(post_test_accuracies, ddof=1) if len(post_test_accuracies) > 1 else 0
        
        avg_learning_gain = np.mean(learning_gains)
        std_learning_gain = np.std(learning_gains, ddof=1) if len(learning_gains) > 1 else 0
        
        avg_improvement = np.mean(improvements)
        std_improvement = np.std(improvements, ddof=1) if len(improvements) > 1 else 0
        
        # æ±‡æ€»ç»“æœ
        summary = {
            "student_id": student_id,
            "dataset": dataset,
            "concept_text": concept_text,
            "concept_id": concept_id,
            "original_accuracy": original_accuracy,
            "pre_test_accuracy": pre_test_accuracy,
            "accuracy_deviation": abs(original_accuracy - pre_test_accuracy),
            "num_runs": self.num_runs,
            "runs": runs,
            "avg_post_test_accuracy": avg_post_test,
            "std_post_test_accuracy": std_post_test,
            "avg_learning_gain": avg_learning_gain,
            "std_learning_gain": std_learning_gain,
            "avg_improvement": avg_improvement,
            "std_improvement": std_improvement
        }
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        print(f"\n{'='*80}")
        print(f"ğŸ“Š å­¦ç”Ÿ {student_id} çš„{self.num_runs}æ¬¡æµ‹è¯•ç»Ÿè®¡ç»“æœ")
        print(f"{'='*80}")
        print(f"   Pre-test (æ— æ•™å­¦):  {pre_test_accuracy*100:.1f}%")
        print(f"   Post-test å¹³å‡:     {avg_post_test*100:.1f}% Â± {std_post_test*100:.1f}%")
        print(f"   Learning Gain:      {avg_learning_gain*100:.1f}% Â± {std_learning_gain*100:.1f}%")
        print(f"   ç»å¯¹æå‡:           {avg_improvement*100:+.1f}% Â± {std_improvement*100:.1f}%")
        
        print(f"\n   æ¯æ¬¡è¿è¡Œè¯¦æƒ…:")
        for run in runs:
            print(f"      Run {run['run_id']}: Post={run['post_test_accuracy']*100:.1f}%, "
                  f"Gain={run['learning_gain']*100:.1f}%")
        
        return summary
    
    def save_multi_run_result(self, result: Dict, method: str = "TASA-multi"):
        """ä¿å­˜å¤šæ¬¡æµ‹è¯•ç»“æœ"""
        dataset = result['dataset']
        student_id = result['student_id']
        
        # åˆ›å»ºç›®å½•
        eval_dir = f"/mnt/localssd/bank/evaluation_results/{method}/{dataset}"
        os.makedirs(eval_dir, exist_ok=True)
        
        # ä¿å­˜
        filename = f"{eval_dir}/student_{student_id}_multi_run.json"
        
        with open(filename, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"\nğŸ’¾ å¤šæ¬¡æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {filename}")
        
        return filename

def find_qualified_students(dataset: str = "assist2017", max_deviation: float = 0.1, num_students: int = 3) -> List[int]:
    """
    æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿï¼ˆå†å²å‡†ç¡®ç‡å’Œpre-testç»“æœå·®è·ä¸è¶…è¿‡thresholdï¼‰
    
    Args:
        dataset: æ•°æ®é›†åç§°
        max_deviation: æœ€å¤§å…è®¸åå·®ï¼ˆé»˜è®¤0.1å³10%ï¼‰
        num_students: éœ€è¦æ‰¾åˆ°çš„å­¦ç”Ÿæ•°é‡
    
    Returns:
        qualified_student_ids: ç¬¦åˆæ¡ä»¶çš„å­¦ç”ŸIDåˆ—è¡¨
    """
    print(f"\nğŸ” æŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ...")
    print(f"   æ¡ä»¶: å†å²å‡†ç¡®ç‡ vs Pre-testå‡†ç¡®ç‡å·®è· â‰¤ {max_deviation*100:.0f}%")
    
    # è¯»å–sessionç›®å½•è·å–æ‰€æœ‰å­¦ç”Ÿ
    session_dir = f"/mnt/localssd/bank/session/{dataset}"
    pretest_dir = f"/mnt/localssd/bank/evaluation_results/pre-test/{dataset}"
    
    qualified = []
    
    # éå†sessionæ–‡ä»¶
    session_files = sorted([f for f in os.listdir(session_dir) if f.endswith('.json')])
    
    for session_file in session_files:
        student_id = int(session_file.replace('.json', ''))
        
        try:
            # åŠ è½½session
            with open(f"{session_dir}/{session_file}") as f:
                session = json.load(f)
            
            concept_id = str(session['concept_id'])
            original_accuracy = session['persona']['stats']['correct'] / session['persona']['stats']['total']
            
            # åŠ è½½pre-testç»“æœ
            pretest_file = f"{pretest_dir}/student_{student_id}_concept_{concept_id}.json"
            if not os.path.exists(pretest_file):
                continue
            
            with open(pretest_file) as f:
                pretest_data = json.load(f)
            
            pre_test_accuracy = pretest_data['roleplay_accuracy']
            
            # æ£€æŸ¥åå·®
            deviation = abs(original_accuracy - pre_test_accuracy)
            
            if deviation <= max_deviation:
                qualified.append({
                    'student_id': student_id,
                    'concept_text': session['concept_text'],
                    'original_accuracy': original_accuracy,
                    'pre_test_accuracy': pre_test_accuracy,
                    'deviation': deviation
                })
                
                if len(qualified) >= num_students:
                    break
        
        except Exception as e:
            continue
    
    print(f"\nâœ… æ‰¾åˆ° {len(qualified)} ä¸ªç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ:\n")
    for i, student in enumerate(qualified, 1):
        print(f"   {i}. å­¦ç”Ÿ{student['student_id']} - {student['concept_text']}")
        print(f"      å†å²å‡†ç¡®ç‡: {student['original_accuracy']*100:.1f}%")
        print(f"      Pre-test:   {student['pre_test_accuracy']*100:.1f}%")
        print(f"      å·®è·:       {student['deviation']*100:.1f}%")
    
    return [s['student_id'] for s in qualified]

# æµ‹è¯•è„šæœ¬
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='TASAå¤šæ¬¡æµ‹è¯•è¯„ä¼°')
    parser.add_argument('--dataset', type=str, default='assist2017', help='æ•°æ®é›†')
    parser.add_argument('--num-runs', type=int, default=3, help='æ¯ä¸ªå­¦ç”Ÿæµ‹è¯•æ¬¡æ•°')
    parser.add_argument('--max-deviation', type=float, default=0.1, help='æœ€å¤§å…è®¸åå·®(0.1=10%)')
    parser.add_argument('--num-students', type=int, default=3, help='æµ‹è¯•å­¦ç”Ÿæ•°é‡')
    
    args = parser.parse_args()
    
    # æŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ
    qualified_students = find_qualified_students(
        dataset=args.dataset,
        max_deviation=args.max_deviation,
        num_students=args.num_students
    )
    
    if not qualified_students:
        print("\nâŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å­¦ç”Ÿ")
        exit(1)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = TASAMultiRunEvaluator(num_runs=args.num_runs)
    
    # å¯¹æ¯ä¸ªå­¦ç”Ÿè¿›è¡Œå¤šæ¬¡æµ‹è¯•
    all_results = []
    
    for student_id in qualified_students:
        result = evaluator.evaluate_student_multi_runs(student_id, args.dataset)
        evaluator.save_multi_run_result(result)
        all_results.append(result)
    
    # æ±‡æ€»æ‰€æœ‰å­¦ç”Ÿçš„ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {len(all_results)}ä¸ªå­¦ç”Ÿçš„æ±‡æ€»ç»Ÿè®¡")
    print(f"{'='*80}")
    
    for result in all_results:
        print(f"\nå­¦ç”Ÿ{result['student_id']} - {result['concept_text']}:")
        print(f"  Learning Gain: {result['avg_learning_gain']*100:.1f}% Â± {result['std_learning_gain']*100:.1f}%")

